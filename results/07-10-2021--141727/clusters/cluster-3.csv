text,title,id,project_number,terms,administration,organization,mechanism,year,cost,funding,score
"Synthetic Biology: At the Crossroads of Genetic Engineering and Human Therapeutics Abstract Support is requested for a Keystone Symposia conference entitled Synthetic Biology: At the Crossroads of Genetic Engineering and Human Therapeutics, organized by Drs. Jose M. Lora and Timothy K. Lu. The conference will be held in Breckenridge, Colorado from March 29- April 1, 2019. Synthetic Biology tools and principles have matured tremendously over the last decade and have reached extraordinary levels of sophistication, both in eukaryotic and prokaryotic systems. Synthetic biology as a therapeutic modality is starting to enter multiple clinical studies and has the potential to have a significant impact on medicine across a wide range of diseases (e.g., metabolic, immune-mediated, cancer, and neurologic diseases). This Keystone Symposia conference will delve into the field of synthetic biology with a special emphasis on its applications to medicine. While there are conferences that capture synthetic biology in only a few talks mixed in among other various topics, there is a paucity of conferences focused on synthetic biology as drugs to treat disease. However, due to the rapid pace of fundamental scientific advances along with an expanding number of biotechnology companies and emerging clinical studies with synthetic biology at their core, this conference will be highly relevant for a wide audience of scientists both from academia and industry. In addition, other meetings in this field have a highly technology-driven focus on synthetic biology techniques with relatively little attention given to biological and medical context. Ultimately, this Keystone Symposia conference should inspire researchers from diverse backgrounds to discuss synthetic biology via many new angles. PROJECT NARRATIVE Over the past two decades, tremendous advances have been made in the use of biological parts to engineer systems that can effectively direct living cells for a vast variety of purposes (a.k.a. synthetic biology). Synthetic biology is being used to construct more effective therapies in diseases such as cancer, but there are remaining obstacles to the clinical translation of these therapies. This Keystone Symposia conference will delve into the field of synthetic biology with a special emphasis on its applications to medicine.",Synthetic Biology: At the Crossroads of Genetic Engineering and Human Therapeutics,9913772,R13EB029305,"['Academia', 'Address', 'Area', 'Attention', 'Biological', 'Biomedical Research', 'Biotechnology', 'Cells', 'Clinical Research', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collaborations', 'Colorado', 'Computers', 'Disease', 'Educational workshop', 'Engineering', 'Future', 'Genetic Engineering', 'Genetic Screening', 'Human', 'Immune', 'Industrialization', 'Industry', 'Knowledge', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Mediating', 'Medical', 'Medicine', 'Metabolic', 'Methodology', 'Modality', 'Neurologic', 'Outcome', 'Participant', 'Pharmaceutical Preparations', 'Postdoctoral Fellow', 'Preventive', 'Process', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Scientific Advances and Accomplishments', 'Scientist', 'System', 'Techniques', 'Technology', 'Therapeutic', 'Work', 'clinical application', 'clinical practice', 'clinical translation', 'combinatorial', 'design', 'effective therapy', 'graduate student', 'meetings', 'nervous system disorder', 'next generation', 'novel diagnostics', 'posters', 'symposium', 'synthetic biology', 'targeted treatment', 'tool']",NIBIB,KEYSTONE SYMPOSIA,R13,2020,10000,559385,-0.00939984385560589
"ADAR-editing landscape dysregulation in neuropsychiatric disorders Project Abstract:  Adenosine deaminase acting on RNA (ADAR) editing plays a major role in shaping transcriptome diversity by creating variant isoforms that enable fine-tuning of calcium-mediated excitatory and other signaling needed for brain development, neural plasticity and mood regulation. The spatio-temporal ADAR editing landscapes are tightly regulated by controlling ADAR expression levels to preserve preferential binding and editing. Previous studies have shown that activation of the interferon pathways of the innate immune system – such as those seen in viral infections - leads to increased expression of ADAR1p150, which ultimately results in changes to ADAR editing patterns. Furthermore, common side effects to innate immune activation by interferon alpha therapies include increased risk of depression and suicide. The changes in spatio-temporal regulation of editing patterns can lead to a wide spectrum of neurological symptoms, including neuropsychiatric disorders (e.g., decreased ADAR editing in the serotonin receptor subunit2C in the prefrontal cortex observed in individuals who commit suicide). Yet, our understanding of ADAR editing landscapes remain cursory. Advances in high throughput RNA-seq enable more accurate variant calling from the sequencing reads, providing a way to map ADAR editing patterns in the transcriptome. However, there are no computational pipelines focused on ADAR editing that are easy to use, are reproducible and can handle large scale analysis. I have recently built a pipeline to handle meta-analysis of RNA-seq data that incorporates variant calling steps, but further work is needed to validate this tool to assure accuracy and reproducibility of results. It can then be used to map the spatio-temporal variation of ADAR editing landscapes. The proposed project will study ADAR editing landscapes in the following ways: (i) new computational pipelines will be benchmarked to use variant calling with RNA-seq datasets using simulated reads, (ii) ADAR editing landscape diversity in the publicly available human samples will be mapped; the computational predictions and hypotheses generated from the pipeline will be validated using (iii) measuring calcium flux in cells with known differential ADAR editing landscapes caused by PolyI:C (viral infection mimic) treatment. The proposed work will yield a validated pipeline capable of mapping ADAR editing landscapes with machine learning algorithms. Defining ADAR editing landscapes is paramount to biomarker discovery and can influence precision medicine applications in diagnosis and treatment of neuropsychiatric disorders. This project will allow for me to gain the knowledge base necessary to become an independent researcher with a unique skill set of both computational and benchwork methods to advance the field of neuroscience. Project Narrative Inferring ADAR editing landscapes and their link with ion homeostasis and excitatory signaling in the brain is important for understanding, diagnosing or staging neurological and neuropsychiatric disorders, including major depressive disorder and suicide. This proposed project will develop and validate computational tools to use RNA-seq from publicly available datasets for ADAR editing inferences and to delineate patterns of editing changes in cells experiencing viral infections. Overall, this project will give me the training to build my unique skill set of both computational and experimental methods that will enable me as an independent researcher to bridge the gap between bioinformatics and experimental researchers and translate my findings into precision medicine.",ADAR-editing landscape dysregulation in neuropsychiatric disorders,9992699,F31MH123131,"['Accounting', 'Anxiety', 'Benchmarking', 'Binding', 'Bioinformatics', 'Brain', 'Calcium', 'Cardiovascular Diseases', 'Cause of Death', 'Cell Culture Techniques', 'Cells', 'Complex', 'Computer Analysis', 'Custom', 'Data', 'Data Set', 'Databases', 'Depression and Suicide', 'Development', 'Diagnosis', 'Economic Burden', 'Etiology', 'Frequencies', 'Fura-2', 'Genes', 'Genetic Transcription', 'Genotype-Tissue Expression Project', 'Glutamate Receptor', 'High-Throughput RNA Sequencing', 'Homeostasis', 'Human', 'Image', 'Immune response', 'Immune system', 'Immunohistochemistry', 'Individual', 'Innate Immune System', 'Interferon Activation', 'Interferon-alpha', 'Ions', 'Lead', 'Link', 'Major Depressive Disorder', 'Maps', 'Measures', 'Mediating', 'Meta-Analysis', 'Methods', 'Molecular', 'Nervous system structure', 'Neurologic Symptoms', 'Neuronal Plasticity', 'Neurosciences', 'PF4 Gene', 'Pathway interactions', 'Pattern', 'Permeability', 'Play', 'Population', 'Prefrontal Cortex', 'Protein Isoforms', 'Proteins', 'RNA', 'RNA Editing', 'Regulation', 'Reproducibility', 'Reproducibility of Results', 'Research Personnel', 'Risk', 'Role', 'Sampling', 'Shapes', 'Signal Transduction', 'Site', 'Staging', 'Suicide', 'Testing', 'Training', 'Translating', 'United States', 'Validation', 'Variant', 'Viral', 'Virus Diseases', 'Western Blotting', 'Work', 'accomplished suicide', 'adenosine deaminase', 'biomarker discovery', 'computational pipelines', 'computerized tools', 'data visualization', 'detector', 'differential expression', 'disability-adjusted life years', 'dopaminergic differentiation', 'excitotoxicity', 'experience', 'immune activation', 'in silico', 'innate immune pathways', 'insight', 'knowledge base', 'machine learning algorithm', 'mood regulation', 'nerve stem cell', 'nervous system disorder', 'neuropsychiatric disorder', 'neurotransmission', 'novel marker', 'precision medicine', 'preservation', 'relating to nervous system', 'release of sequestered calcium ion into cytoplasm', 'serotonin receptor', 'side effect', 'skills', 'social', 'spatiotemporal', 'suicidal risk', 'tool', 'transcriptome', 'transcriptome sequencing', 'virtual machine']",NIMH,KENT STATE UNIVERSITY,F31,2020,29830,5062280,-0.08194200563987764
"Abiotic-Biotic Interfaces for Ophthalmology Symposium ABSTRACT This proposal seeks funding to support a symposium, Abiotic-Biotic Interfaces for Ophthalmology (ABI), which will bring together recognized world experts in clinical, research, vision science, engineering, industrial and pharmaceutical communities as well as junior investigators (i.e., young faculty and those in training) to discuss the current state of ABI, ranging from bioelectronic implantable and wearable devices, to nanoscale scaffolds for stem cell and gene therapies. Given the multidisciplinary nature of this field, it is essential to bring together researchers and clinicians with varying levels of expertise across many domains related to ABI to advance the progress of this novel field, identify challenges of advancement, and develop a strategic action plan to overcome these challenges. The timing to have such a symposium to further the application of implantable and/or wearable bioengineered systems in ophthalmology is now as we focus on precision and personalized medicine and leverage the revolution in deep learning artificial intelligence algorithms. Through symposium talks, sessions, and discussions we will cover the fundamentals and also identify innovative and cutting-edge strategies and methodologies to accelerate the rate of major discoveries and development of novel therapeutics. The specific aims of this symposium are: Specific Aim 1. To bring together both established and junior investigators representing a broad range of disciplines to discuss cutting edge research in this novel field, catalyze the development of cross-disciplinary and translational approaches to advance abiotic-biotic interfaces for ophthalmology, and identify gaps in knowledge and barriers to advancement. We will identify research questions and develop an agenda to guide future research that is consistent with the objectives and interests of NEI. Specific Aim 2. Develop a junior investigator program to motivate a diverse group of students and junior investigators to pursue research careers in vision science and ophthalmologic therapeutic development, who will ultimately submit grant proposals to NEI solicitations and contribute to the scientific literature. Specific Aim 3. Develop a strategic action plan to set priorities for future studies that will encourage inter-agency collaborations (e.g., NEI, NSF, DARPA, etc.). This is critical because often certain engineering tasks are best suited to be supported by NSF or DARPA whereas the biological testing of the engineered systems lends itself to funding from NEI. Hence such inter-agency or cross-agency efforts can help leverage the funding to develop sophisticated abiotic-biotic systems NARRATIVE This meeting is the first on this topic dedicated to the broad use of implantable and/or wearable bioelectronics for ophthalmological applications. It is anticipated that the strategic action plan will significantly impact the field by greatly accelerating the translation of basic science and engineering research findings to stimulate the development of novel treatments and improve clinical practice. Key topics include visual restoration, drug and gene delivery, and sensing intraocular pressure. This meeting will foster training and development of future leaders in this emerging field and promote collaboration and exchange of knowledge and ideas among junior and established investigators.",Abiotic-Biotic Interfaces for Ophthalmology Symposium,10070800,R13EY031988,"['Algorithms', 'Applications Grants', 'Artificial Intelligence', 'Basic Science', 'Biological Testing', 'Biomedical Engineering', 'Cellular Phone', 'Clinical Research', 'Collaborations', 'Communities', 'Computer software', 'Contact Lenses', 'Custom', 'Data', 'Development', 'Devices', 'Diagnosis', 'Discipline', 'Disease', 'Drug Delivery Systems', 'Electronics', 'Engineering', 'Eye', 'Faculty', 'Fostering', 'Funding', 'Future', 'Gene Delivery', 'Glass', 'Industrialization', 'Intraocular lens implant device', 'Knowledge', 'Literature', 'Medicine', 'Methodology', 'Nature', 'Neural Retina', 'Ophthalmology', 'Optics', 'Pharmacologic Substance', 'Physiologic Intraocular Pressure', 'Physiological', 'Research', 'Research Personnel', 'Route', 'Scientific Inquiry', 'Scientist', 'Senior Scientist', 'Students', 'System', 'Time', 'Training', 'Translations', 'Virtual and Augmented reality', 'Visual', 'base', 'career', 'clinical practice', 'deep learning', 'gene therapy', 'implantable device', 'improved', 'innovation', 'intelligent algorithm', 'interest', 'meetings', 'multidisciplinary', 'nanoscale', 'neural network', 'novel', 'novel therapeutics', 'personalized medicine', 'portability', 'precision medicine', 'programs', 'restoration', 'scaffold', 'stem cell therapy', 'symposium', 'therapeutic development', 'translational approach', 'vision science', 'wearable device']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R13,2020,42465,324592664,-0.005168822732552619
"Coupling a multifunctional tag to scalable endogenous tagging technology for improved genome-wide perturbation screens Project Summary  Characterizing the functions of protein-coding genes is an important goal in the post-genomic era. While proteins are the ultimate effectors of most cellular functions, including those mis-regulated in disease, we have an extremely limited understanding of the roles of the majority of proteins in the human proteome. Though powerful, existing technologies for the high-throughput interrogation of protein-coding genes, including CRISPR/Cas9-based approaches and RNA interference, require extended periods of time to effect changes in protein levels, and thus suffer two critical shortcomings. First, they are unable to detect the contribution of growth- essential genes to any cellular process other than viability, as any cell carrying a perturbation in such a gene would fail to propagate. Second, compensatory and adaptive effects have ample opportunity to manifest, thus convoluting screen results by ameliorating the effect of the perturbation, or by generating a novel, unrelated effect. To address these critical limitations, I propose to develop a new screening technology that will minimize the time between perturbation and screen readout by inducibly and rapidly degrading endogenous proteins. This is made possible by a readily scalable endogenous tagging technology that harnesses homology-independent targeted integration to insert a synthetic exon into the intron of a protein-coding gene at the site of a double strand break. The synthetic exon will encode a multifunctional ligand-binding protein that depending on the ligand, will lead to fluorescence or rapid degradation. Pooled libraries of sgRNAs targeting different introns allows for the creation of custom libraries of cells, where each cell carries this multifunctional tag on a different protein. The utility of this approach will be established aims 1 and 2 by testing (1) whether cells that have undergone rapid depletion of growth-essential proteins are maintained in the cell library at the end of the short perturbation window and (2) whether rapid depletion and CRISPR knockout at the same protein produce different effects on a well-established phenotype, due to the distorting effects of adaptation events in the knockout. Aim 3 witnesses the use of a machine learning approach and the data from thousands of attempted tagging events to identify how the features of a potential tag site dictate the likelihood that a functional protein carrying the multifunctional tag will be produced. The resulting model will be unleashed on the protein-coding genome to predict high-quality tag sites for as many protein-coding genes as possible. This will establish an improved screening paradigm that will allow for the pooled interrogation of the contributions of thousands of proteins to a phenotype of interest, will thus accelerate the rate at which we come to understand the poorly understood elements of the protein-coding genome. These efforts will be well supported by the outstanding resources for experimentation and mentorship at both the University of Pennsylvania and the Children’s Hospital of Philadelphia, and will provide excellent training in experimental techniques for protein perturbation and characterization, as well computational literacy in the broadly useful field of machine learning. Project Narrative  Proteins are the ultimate effectors of most cellular functions in health and disease. Yet, the functions of the majority proteins encoded by the human genome are very poorly understood. A novel high-throughput screening strategy leveraging rapid protein degradation will dramatically accelerate functional characterization of the human proteome.",Coupling a multifunctional tag to scalable endogenous tagging technology for improved genome-wide perturbation screens,9989245,F31HG011185,"['Address', 'Benchmarking', 'Binding', 'Binding Proteins', 'CRISPR/Cas technology', 'Cell division', 'Cell physiology', 'Cells', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'Coupling', 'Custom', 'DNA', 'DNA lesion', 'Data', 'Development', 'Disease', 'Dropout', 'Drops', 'Elements', 'Ensure', 'Essential Genes', 'Event', 'Exons', 'Fluorescence', 'Fluorescence-Activated Cell Sorting', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Health', 'Hour', 'Human', 'Human Genome', 'Immune', 'Inclusion Bodies', 'Introns', 'Investigation', 'Knock-out', 'Lead', 'Lesion', 'Libraries', 'Ligands', 'Machine Learning', 'Measurement', 'Measures', 'Mentorship', 'Methodology', 'Methods', 'Modeling', 'Motivation', 'Pediatric Hospitals', 'Pennsylvania', 'Phenotype', 'Philadelphia', 'Play', 'Process', 'Proteins', 'Proteome', 'Publishing', 'RNA Interference', 'Reporter', 'Resources', 'Role', 'Screening Result', 'Shunt Device', 'Site', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'base', 'blind', 'cost', 'experimental study', 'fitness', 'genome editing', 'genome-wide', 'high throughput screening', 'high throughput technology', 'improved', 'interest', 'knockout gene', 'literacy', 'mutant', 'novel', 'protein degradation', 'protein function', 'protein structure', 'screening', 'single-cell RNA sequencing']",NHGRI,UNIVERSITY OF PENNSYLVANIA,F31,2020,45520,593605914,-0.0027781390445081513
"Transfer learning to improve the re-usability of computable biomedical knowledge Candidate: With my multidisciplinary background in Artificial Intelligence (PhD), Public Health Informatics (MS), Epidemiology and Health Statistics (MS), and Preventive Medicine (Bachelor of Medicine), my career goal is to become an independent investigator working at the intersection of Artificial Intelligence and Biomedicine, with a particular emphasis initially in machine learning and public health. Training plan: My K99/R00 training plan emphasizes machine learning, deep learning and scientific communication skills (presentation, writing articles, and grant applications), which will complement my current strengths in artificial intelligence, statistics, medicine and public health. I have a very strong mentoring team. My mentors, Drs. Michael Becich (primary), Gregory Cooper, Heng Huang, and Michael Wagner, all of whom are experienced with research and professional career development. Research plan: The research goal of my proposed K99/R00 grant is to increase the re-use of computable biomedical knowledge, which is knowledge represented in computer-interpretable formalisms such as Bayesian networks and neural networks. I refer to such representations as models. Although models can be re-used in toto in another setting, there may be loss of performance or, even more problematically, fundamental mismatches between the data required by the model and the data available in the new setting making their re-use impossible. The field of transfer learning develops algorithms for transferring knowledge from one setting to another. Transfer learning, a sub-area of machine learning, explicitly distinguishes between a source setting, which has the model that we would like to re-use, and a target setting, which has data insufficient for deriving a model from data and therefore needs to re-use a model from a source setting. I propose to develop and evaluate several Bayesian Network Transfer Learning (BN- TL) algorithms and a Convolutional Neural Network Transfer Learning algorithm. My specific research aims are to: (1) further develop and evaluate BN-TL for sharing computable knowledge across healthcare settings; (2) develop and evaluate BN-TL for updating computable knowledge over time; and (3) develop and evaluate a deep transfer learning algorithm that combines knowledge in heterogeneous scenarios. I will do this research on models that are used to automatically detect cases of infectious disease such as influenza. Impact: The proposed research takes advantage of large datasets that I previously developed; therefore I expect to quickly have results with immediate implications for how case detection models are shared from a region that is initially experiencing an epidemic to another location that wishes to have optimal case-detection capability as early as possible. More generally, it will bring insight into machine learning enhanced biomedical knowledge sharing and updating. This training grant will prepare me to work independently and lead efforts to develop computational solutions to meet biomedical needs in future R01 projects. Transfer learning to improve the re-usability of computable biomedical knowledge Narrative Re-using computable biomedical knowledge in the form of a mathematical model in a new setting is challenging because the new setting may not have data needed as inputs to the model. This project will develop and evaluate transfer learning algorithms, which are computer programs that adapt a model to a new setting by removing and adding local variables to it. The developed methods for re-using models are expected to benefit the public’s health by: (1) improving case detection during epidemics by enabling re-use of automatic case detectors developed in the earliest affected regions with other regions, and, more generally, (2) increasing the impact of NIH’s investment in machine learning by enabling machine-learned models to be used in more institutions and locations.",Transfer learning to improve the re-usability of computable biomedical knowledge,9952803,K99LM013383,"['Affect', 'Algorithms', 'Applications Grants', 'Area', 'Artificial Intelligence', 'Bayesian Method', 'Bayesian Modeling', 'Bayesian Network', 'Big Data', 'Clinical', 'Communicable Diseases', 'Communication', 'Complement', 'Computerized Medical Record', 'Computers', 'Data', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Doctor of Philosophy', 'Epidemic', 'Epidemiology', 'Future', 'Goals', 'Grant', 'Health', 'Healthcare Systems', 'Heterogeneity', 'Influenza', 'Institution', 'Investigation', 'Investments', 'Knowledge', 'Lead', 'Location', 'Lung diseases', 'Machine Learning', 'Medical center', 'Medicine', 'Mentors', 'Methods', 'Modeling', 'Natural Language Processing', 'Parainfluenza', 'Patients', 'Performance', 'Play', 'Preventive Medicine', 'Process', 'Psychological Transfer', 'Public Health', 'Public Health Informatics', 'Research', 'Research Personnel', 'Role', 'Semantics', 'Societies', 'Source', 'Testing', 'Time', 'Training', 'Twin Multiple Birth', 'Unified Medical Language System', 'United States National Institutes of Health', 'Universities', 'Update', 'Utah', 'Work', 'Writing', 'base', 'career', 'career development', 'computer program', 'convolutional neural network', 'deep learning', 'deep neural network', 'detector', 'experience', 'health care settings', 'improved', 'insight', 'large datasets', 'learning algorithm', 'mathematical model', 'multidisciplinary', 'neural network', 'skills', 'statistics', 'usability']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K99,2020,92359,570146095,-0.011740511432079598
"Algorithmic identification of binding specificity mechanisms in proteins Project Summary Variations in protein binding preferences are a critical barrier to the precision treatment of disease. When high resolution structures of a protein are available, and many isoforms of the protein have been connected to dif- fering binding preferences, it is possible in principle to model the structures of all isoforms and discover the mechanisms that cause variations in binding preferences. Unfortunately, this discovery process depends on human expertise for examining molecular structure, and given that hundreds of isoforms may exist, a human would be overwhelmed to objectively examine many similar isoforms. To fill this gap, this project will (A1) de- velop software that identifies structural mechanisms that cause differential binding preferences, categorizes similar structural mechanisms, and explains the mechanisms in English. The second aim of this project (A2) is to validate the software at a large scale on families of proteins that exhibit a variety of well-examined binding preferences, and through blind predictions with experimental collaborators. Our approach involves creating software that mimics the visual reasoning techniques employed by structural biologists when examining molecular structures. Not only are these techniques responsible for most major dis- coveries in structural biology, but they are also straightforward to understand by non-computational research- ers. This property will enable our software to immediately integrate into existing workflows at labs that do not focus on computational methods. This property also contrasts from existing methods, which generally output structural models, potential energies, p-values and structural scores which are difficult for non-experts to un- derstand or incorporate into their research. Often, an expert in biophysics is required to interpret the outputs so that they can be operationalized in laboratory environments. In preliminary results, our methods have already identified molecular mechanisms that govern specificity in several families of proteins. Verification against peer-reviewed experimentation has proven the preliminary results correct in almost all cases. Our methods have also been applied to make a blind prediction of binding mechanisms in the ricin toxin, which binds to and damages the human ribosome. With experimental collabo- rators, we showed that our methods correctly identified and predicted the roles of several amino acids with a hitherto unknown role in recognizing the ribosome. Using our methodological approach and our rigorous valida- tion strategy, this project will produce a highly validated, usable software package that will bridge a critical gap in the development of precision therapies and diagnostics. Variations in protein binding preferences are a critical barrier to precision medicine and precise diagnostics. We will develop software that will identify and categorize molecular mechanisms that cause these variations. The resulting insights will enable clinicians to more precisely select therapies to achieve superior outcomes.",Algorithmic identification of binding specificity mechanisms in proteins,10164894,R01GM123131,"['Address', 'Algorithms', 'Amino Acids', 'Artificial Intelligence', 'Benchmarking', 'Binding', 'Binding Proteins', 'Binding Sites', 'Biochemical', 'Biophysical Process', 'Biophysics', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Computing Methodologies', 'Development', 'Diagnosis', 'Disease', 'Docking', 'Drug Targeting', 'Electrostatics', 'Elements', 'English Language', 'Environment', 'Evaluation', 'Exhibits', 'Feedback', 'HIV Protease', 'Hot Spot', 'Human', 'Hydrogen Bonding', 'Hydrophobicity', 'Immune', 'Individual', 'Influentials', 'Laboratories', 'Letters', 'Ligand Binding', 'Ligands', 'Link', 'Literature', 'Major Histocompatibility Complex', 'Maps', 'Measures', 'Mechanics', 'Methodology', 'Methods', 'Molecular', 'Molecular Conformation', 'Molecular Structure', 'Mutation', 'Nicotinic Receptors', 'Outcome', 'Output', 'Patients', 'Peer Review', 'Peptide Hydrolases', 'Population', 'Potential Energy', 'Precision therapeutics', 'Process', 'Property', 'Protein Family', 'Protein Isoforms', 'Proteins', 'Research', 'Resolution', 'Ribosomes', 'Ricin', 'Role', 'Serine Protease', 'Shapes', 'Site', 'Specificity', 'Structural Biologist', 'Structural Models', 'Structure', 'Techniques', 'Testing', 'Text', 'Toxin', 'Tweens', 'Universities', 'Validation', 'Variant', 'Visual', 'base', 'blind', 'human-in-the-loop', 'hydropathy', 'inhibitor/antagonist', 'insight', 'mutant', 'novel', 'personalized diagnostics', 'precision medicine', 'preference', 'protein structure', 'prototype', 'receptor', 'simulation', 'software development', 'structural biology', 'therapy development', 'tool', 'tumor']",NIGMS,LEHIGH UNIVERSITY,R01,2020,100356,6676095,0.020769665611445658
"West Coast Metabolomics Center for Compound Identification Project Summary – Overall West Coast Metabolomics Center for Compound Identification (WCMC) The West Coast Metabolomics Center for Compound Identification (WCMC) is committed to the overall goals of the NIH Common Fund Metabolomics Initiative and specifically aims to largely improve small molecule identifications. Understanding metabolism is important to gain insight into biochemical processes and relevant to battle diseases such as cancer, obesity and diabetes. Compound identification in metabolomics is still a daunting task with many unknown compounds and false positive identifications. The major goal of the WCMC is therefore to develop processes and resources that accelerate and improve the accuracy of the compound identification workflow for experts and medical professionals. The WCMC for Compound Identification is structured in three different entities: the Administrative Core, the Computational Core and the Experimental Core. The Center is led by the Director Prof. Fiehn in close collaboration with quantum chemistry experts Prof. Wang and Prof. Tantillo, and metabolomics experts Dr. Barupal and Dr. Kind with broad support from mass spectrometry, computational metabolomics and programming experts. The Administrative Core will assist the Computational and Experimental Core to develop and validate large in-silico mass spectral libraries, retention time prediction models and innovative methods for constraining and ranking lists of isomers in an integrated process of cheminformatics tools and databases. The developed tools and databases will be made available to all Common Fund Metabolomics Consortium (CF-MC) members and professional working groups. The WCMC will also provide guidance for compound identification to the National Metabolomics Data Repository. The broad dissemination of developed compound identification protocols, training for compound identification workflows, databases and distribution of internal reference standard kits for metabolomic standardization will overall widely support the metabolomics community. Project Narrative – Overall West Coast Metabolomics Center for Compound Identification (WCMC) Understanding metabolism is relevant to find both markers and mechanisms of diseases and health phenotypes, including obesity, diabetes, and cancer. The West Coast Metabolomics Center for Compound Identification at UC Davis will use advanced experimental and computational mass spectrometry methods to significantly improve compound identification rates in metabolomics. Such identification will lead to breakthroughs in more precise diagnostics as well as finding the causes of diseases.",West Coast Metabolomics Center for Compound Identification,10258317,U2CES030158,"['Achievement', 'Amines', 'Benchmarking', 'Biochemical Process', 'Biodiversity', 'Biological Assay', 'Blinded', 'Chemicals', 'Chemistry', 'Collaborations', 'Communication', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Reporting', 'Databases', 'Deuterium', 'Diabetes Mellitus', 'Disease', 'Ensure', 'Enzymes', 'Finding by Cause', 'Funding', 'Goals', 'Guidelines', 'Health', 'Hybrids', 'Hydrogen', 'Isomerism', 'Leadership', 'Libraries', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mass Chromatography', 'Mass Fragmentography', 'Mass Spectrum Analysis', 'Medical', 'Metabolism', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Monitor', 'North America', 'Obesity', 'Phenotype', 'Policies', 'Process', 'Protocols documentation', 'Reaction', 'Reference Standards', 'Research Design', 'Resolution', 'Resources', 'Software Tools', 'Solvents', 'Standardization', 'Structure', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Vendor', 'Vertebral column', 'base', 'chemical standard', 'cheminformatics', 'computing resources', 'data acquisition', 'data warehouse', 'database design', 'deep learning', 'heuristics', 'improved', 'in silico', 'innovation', 'insight', 'member', 'metabolomics', 'model building', 'molecular dynamics', 'novel', 'organizational structure', 'personalized diagnostics', 'predictive modeling', 'quantum chemistry', 'repository', 'small molecule', 'tool', 'training opportunity', 'working group']",NIEHS,UNIVERSITY OF CALIFORNIA AT DAVIS,U2C,2020,157500,254622553,-0.0007603717306627328
"Big Flow Cytometry Data: Data Standards, Integration and Analysis PROJECT SUMMARY Flow cytometry is a single-cell measurement technology that is data-rich and plays a critical role in basic research and clinical diagnostics. The volume and dimensionality of data sets currently produced with modern instrumentation is orders of magnitude greater than in the past. Automated analysis methods in the field have made great progress in the past five years. The tools are available to perform automated cell population identification, but the infrastructure, methods and data standards do not yet exist to integrate and compare non-standardized big flow cytometry data sets available in public repositories. This proposal will develop the data standards, software infrastructure and computational methods to enable researchers to leverage the large amount of public cytometry data in order to integrate, re-analyze, and draw novel biological insights from these data sets. The impact of this project will be to provide researchers with tools that can be used to bridge the gap between inference from isolated single experiments or studies, to insights drawn from large data sets from cross-study analysis and multi-center trials. PROJECT NARRATIVE The aims of this project are to develop standards, software and methods for integrating and analyzing big and diverse flow cytometry data sets. The project will enable users of cytometry to directly compare diverse and non-standardized cytometry data to each other and make biological inferences about them. The domain of application spans all disease areas where cytometry is utilized.","Big Flow Cytometry Data: Data Standards, Integration and Analysis",9969443,R01GM118417,"['Address', 'Adoption', 'Advisory Committees', 'Archives', 'Area', 'Basic Science', 'Bioconductor', 'Biological', 'Biological Assay', 'Cells', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Data Files', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Environment', 'Flow Cytometry', 'Foundations', 'Genes', 'Goals', 'Heterogeneity', 'Immune System Diseases', 'Immunologic Monitoring', 'Industry', 'Informatics', 'Infrastructure', 'International', 'Knock-out', 'Knowledge', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Meta-Analysis', 'Metadata', 'Methods', 'Modernization', 'Mouse Strains', 'Multicenter Trials', 'Mus', 'Output', 'Phenotype', 'Play', 'Population', 'Procedures', 'Protocols documentation', 'Reagent', 'Research', 'Research Personnel', 'Retrieval', 'Role', 'Societies', 'Software Tools', 'Standardization', 'Technology', 'Testing', 'Validation', 'Work', 'automated analysis', 'base', 'bioinformatics tool', 'body system', 'cancer diagnosis', 'clinical diagnostics', 'community based evaluation', 'computerized tools', 'data exchange', 'data integration', 'data standards', 'data submission', 'data warehouse', 'experimental study', 'human disease', 'insight', 'instrument', 'instrumentation', 'large datasets', 'mammalian genome', 'multidimensional data', 'novel', 'operation', 'phenotypic data', 'public repository', 'repository', 'research and development', 'software development', 'software infrastructure', 'statistics', 'supervised learning', 'tool', 'vaccine development']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2020,158388,758431960,0.005968832321221359
"Classifying addictions using machine learning analysis of multidimensional data ABSTRACT This Independent Scientist Award will significantly enhance my research capabilities, enabling me to become a leading quantitative investigator in the field of substance use disorders (SUDs). Specifically, it will allow me to increase my knowledge in the areas of SUD phenotypes, treatment and genetics. SUDs are clinically and etiologically heterogeneous and their classification has been difficult. This application reflects my ongoing commitment to developing an innovative and interdisciplinary research program on the classification of SUDs through quantitative analysis of multidimensional data. My extensive training in computational science and prior research on biomedical informatics have provided me with the skills to design, implement and evaluate advanced algorithms and sophisticated analyses to solve challenging problems in classifying SUDs. My ongoing NIDA-funded R01 employs a large (n=~12,000) sample aggregated from multiple genetic studies of cocaine, opioid, and alcohol dependence to develop and evaluate novel statistical models to generate clinical SUD subtypes that are optimized for gene finding. This K02 proposal extends that work to evaluate treatment outcome in refined subgroups of SUD populations using data from treatment studies for cocaine, opioid, alcohol and multiple substance dependence. This project will integrate data from diagnostic behavioral variables and genotypes, as well as biological/neurobiological features of the disorders and repeated measures of treatment outcome. The primary career development goals of this application are to: (1) understand the reliability, validity and functional mechanisms of various phenotyping methods; (2) to continue training in the genetics of addictions; and (3) to gain greater knowledge of different treatment approaches and their efficacy. A solid foundation in these areas will enhance my ability to realize the full potential of the data collected and aggregated from multiple dimensions, and to use the data to design the most clinically useful analysis and generate innovative solutions to diagnostic and predictive challenges in SUD research. Through formal coursework, directed readings, individual tutoring and intensive multidisciplinary collaboration with a diverse team of world-renowned researchers, I will receive training and collect pilot data for future R01 projects by examining (Aim I): whether clinically-defined highly heritable subtypes derived in my current R01 project predict differential treatment response; (Aim II) whether new statistical models that directly combine treatment data with behavioral, biological, and genomic data identify refined subtypes with confirmatory multilevel evidence; and (Aim III) whether there are genetic and social moderators of treatment outcome by subtype. The overall goal of this proposal is to further my independent and multidisciplinary research program in the development of statistical methods for refined classification of SUDs. The K02 award will provide me with the protected time necessary to fully engage in the training activities described that will enhance my knowledge and skills to enable me to make important, novel contributions to the genetics and treatment of SUD. PROJECT NARRATIVE This project will develop novel statistical and quantitative tools to identify homogeneous subtypes of substance use disorders (SUDs) and other complex diseases to enhance gene finding and treatment matching. The proposed project will perform secondary analyses of existing data from treatment studies of cocaine, opioid, alcohol, and mixed SUDs. The proposed novel approaches are expected to advance precision medicine approaches to SUDs by enabling treatment matching and a more refined SUD classification to gene finding.",Classifying addictions using machine learning analysis of multidimensional data,9851853,K02DA043063,"['Adherence', 'Aftercare', 'Alcohol dependence', 'Alcohols', 'Algorithms', 'Area', 'Behavioral', 'Biological', 'Biological Markers', 'Biosensor', 'Characteristics', 'Classification', 'Clinical', 'Cluster Analysis', 'Cocaine', 'Cocaine Dependence', 'Collaborations', 'Combined Modality Therapy', 'Complex', 'Computational Science', 'DSM-IV', 'DSM-V', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic and Statistical Manual of Mental Disorders', 'Dimensions', 'Disease', 'Drug Use Disorder', 'Electroencephalography', 'Etiology', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Heritability', 'Heterogeneity', 'Independent Scientist Award', 'Individual', 'Interdisciplinary Study', 'Investigation', 'Joints', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Neurobiology', 'Opiate Addiction', 'Opioid', 'Patients', 'Pattern', 'Pharmacogenetics', 'Pharmacotherapy', 'Phenotype', 'Population', 'Reading', 'Recording of previous events', 'Research', 'Research Personnel', 'Risk Factors', 'Sampling', 'Scientist', 'Signs and Symptoms', 'Solid', 'Statistical Methods', 'Statistical Models', 'Subgroup', 'Substance Addiction', 'Substance Use Disorder', 'Surveys', 'Symptoms', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Work', 'addiction', 'alcohol use disorder', 'biomarker performance', 'biomedical informatics', 'career development', 'cocaine use', 'contingency management', 'design', 'disease classification', 'disorder subtype', 'endophenotype', 'genetic association', 'genomic data', 'imaging genetics', 'improved', 'innovation', 'multidimensional data', 'neural correlate', 'novel', 'novel strategies', 'opioid use disorder', 'outcome prediction', 'personalized medicine', 'precision medicine', 'programs', 'recruit', 'secondary analysis', 'skills', 'social', 'tool', 'treatment planning', 'treatment response', 'tutoring']",NIDA,UNIVERSITY OF CONNECTICUT STORRS,K02,2020,161422,36067938,-0.027878295650114515
"Automated, model-guided phenotyping to identify metabolite/gene/microbe interactions Project Summary/Abstract DNA sequencing has spawned the “microbiome revolution” -- thousands of microbes and a dizzying number of microbial interactions that are associated with human health and disease. Unfortunately, most species in the microbiome are known only by a (partial) genome. The limited phenotypic data on newly discovered bacteria reveal species that behave unlike any of our model organisms. While genome-scale modeling plays an important role in understanding the microbiome, the paucity of phenotypic data for most species prevents detailed simulation of the microbial communities that affect our health. This project will develop an automated system for profiling, synthesizing, and modeling microbial communities. The center of our approach is Deep Phenotyping, an automated robotic platform that performs complex growth experiments on demand. Data from Deep Phenotyping will be used to train metabolic and statistical models of the oral pathogens Streptococcus mutans and Candida albicans to predict conditions that keep both microbes in a nonpathogenic state. Project Narrative The microbiome revolution has uncovered thousands of species of bacteria with roles in health and disease. This project automates the identification of interactions between environments, genes, and the microbes that live in and around us. Understanding these interactions is a critical step in re-engineering the microbiome to improve human health.","Automated, model-guided phenotyping to identify metabolite/gene/microbe interactions",9852330,R21EB027396,"['Affect', 'Animal Model', 'Antibiotics', 'Area', 'Bacteria', 'Biochemical Pathway', 'Bioinformatics', 'Biological', 'Biological Assay', 'Candida albicans', 'Carbon', 'Coculture Techniques', 'Collaborations', 'Combinatorics', 'Communities', 'Complex', 'Computer software', 'DNA sequencing', 'Data', 'Development', 'Disease', 'Engineering', 'Environment', 'Environmental Risk Factor', 'Exposure to', 'Future', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Growth', 'Health', 'Human', 'Human Microbiome', 'Hybrids', 'Image', 'Individual', 'Knock-out', 'Knowledge', 'Link', 'Liquid substance', 'Machine Learning', 'Maps', 'Metabolic', 'Methods', 'Microbe', 'Microbiology', 'Modeling', 'Oral', 'Oral candidiasis', 'Pathway interactions', 'Phenotype', 'Play', 'Positioning Attribute', 'Regulator Genes', 'Research Personnel', 'Robotics', 'Role', 'Shapes', 'Source', 'Statistical Models', 'Streptococcus mutans', 'Structure', 'System', 'Technology', 'Time', 'Training', 'Work', 'base', 'combinatorial', 'cost', 'design', 'dysbiosis', 'experience', 'experimental study', 'fitness', 'fungus', 'genome-wide', 'improved', 'instrumentation', 'large datasets', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microorganism interaction', 'network models', 'open source', 'oral pathogen', 'pathogenic fungus', 'phenotypic data', 'preference', 'prevent', 'screening', 'simulation', 'transcription factor', 'transcriptome sequencing', 'transposon sequencing']",NIBIB,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,R21,2020,178949,76545728,-0.005399863681654778
"Real time optimization of electron-based fragmentation for middle and top-down proteomics in mass spectrometry The identification and quantification of biological macromolecules remains challenging despite major advances in the speed, resolution and mass accuracy of modern mass spectrometers. A key weakness with current instrumentation lies in the methods used to induce fragmentation. The reliance in particular on collision-induced dissociation (CID) has limited such analyses to bottom-up workflows of trypsin-digested peptides of 10-30 residues. At e-MSion, we have developed an efficient electron-fragmentation technology called ExD for large proteins and are now co-marketed our ExD Option with Agilent, and soon will be with Thermo and Waters instruments. What has really captured the interest of the biopharma and top-down communities in the past year is the exceptional sequence coverage of native proteins we obtain with the same ExD cell. The resulting spectra are less congested than those obtained with currently available ETD/UVPD/CID fragmentation methodologies. We have shown that our technology works faster and gives cleaner spectra with more complete dissociation with larger macromolecular protein complexes than has ever been possible before, while still preserving labile post translational modifications. In addition, fragmentation with higher energy electrons can be used to provide complementary data to improve protein and glycan identification. The challenge now has become how to optimally collect and process these data to maximize the utility of ExD fragmentation. Last summer, Xilinx released its Versal Adaptive Compute Acceleration Platform (ACAP), a massively parallel processor with 50 billion transistors targeted to transform digital signal processing, handling of big data and artificial intelligence. This ACAP technology has already accelerated Illumina DNA sequence assembly by 90-fold. Our feasibility question asks how to effectively harness this new highly parallelized technology to preprocess complex top-down mass spectra on- the-fly. This will allow us to actively optimize data acquisition by enabling adaptive operation of the ExD cell and mass spectrometer. The objective is to maximize both fragmentation and dissociation of native proteins, enabling faster and comprehensive characterization of challenging proteoforms important to the biopharmaceutical industry and biomedical researchers.  Success will offer an extremely fast, cost-effective solution to characterize complexes of macromolecules under native conditions with increased accuracy, speed, and fewer misidentifications. Our ExD technology with the Versal ACAP can be both retrofitted into existing mass spectrometers as well as being available in new generations of mass spectrometers at a price below other less-effective alternative fragmentation technologies like ETD and UVPD. Thus, it will provide new abilities for many NIH investigators to advance basic research, probe disease mechanisms and permit more sophisticated searches for both diagnostic and therapeutic biomarkers. Even with all of the scientific progress made to date, the complexity of disease-affected tissues still challenges our ability to probe what makes people sick. The goal of this Phase I SBIR project is to develop a powerful computer technology to aid in characterizing biological molecules that will improve the diagnosis and treatment of diseases ranging from arthritis, cancer, diabetes to heart disease and neurodegeneration.",Real time optimization of electron-based fragmentation for middle and top-down proteomics in mass spectrometry,10081127,R43GM139467,"['Acceleration', 'Affect', 'Arthritis', 'Artificial Intelligence', 'Automobile Driving', 'Basic Science', 'Big Data', 'Biological', 'Biological Products', 'Biological Response Modifier Therapy', 'Businesses', 'Cells', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computers', 'Continuous Infusion', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Collection', 'Diabetes Mellitus', 'Diagnosis', 'Digital Signal Processing', 'Disease', 'Dissociation', 'Electronics', 'Electrons', 'Engineering', 'Face', 'Family', 'Feasibility Studies', 'Generations', 'Goals', 'Grant', 'Health', 'Heart Diseases', 'Individual', 'Industrialization', 'Industry', 'Ions', 'Isoleucine', 'Laboratories', 'Leucine', 'Macromolecular Complexes', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Methodology', 'Methods', 'Modernization', 'Multiprotein Complexes', 'Nerve Degeneration', 'Noise', 'Optics', 'Peptides', 'Periodicity', 'Phase', 'Polysaccharides', 'Post-Translational Protein Processing', 'Price', 'Process', 'Protein Analysis', 'Protein Fragment', 'Proteins', 'Proteomics', 'Reading', 'Research Personnel', 'Resolution', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Speed', 'Structure', 'Techniques', 'Technology', 'Time', 'Tissues', 'Transistors', 'Trypsin', 'United States National Institutes of Health', 'Vendor', 'Water', 'Work', 'base', 'blind', 'computational platform', 'computerized data processing', 'cost effective', 'data acquisition', 'diagnostic biomarker', 'disulfide bond', 'electron energy', 'encryption', 'experience', 'fragment X', 'improved', 'instrument', 'instrumentation', 'interest', 'macromolecule', 'mass spectrometer', 'meetings', 'operation', 'preservation', 'programs', 'protein complex', 'signal processing', 'success', 'therapeutic biomarker']",NIGMS,"E-MSION, INC.",R43,2020,212830,959155,-0.015880542337064506
"Integrative Predictors of Temporomandibular Osteoarthritis ABSTRACT This application proposes the development of efficient web-based data management, mining, and analytics, to integrate and analyze clinical, biological, and high dimensional imaging data from TMJ OA patients. Based on our published results, we hypothesize that patterns of condylar bone structure, clinical symptoms, and biological mediators are unrecognized indicators of the severity of progression of TMJ OA. Efficiently capturing, curating, managing, integrating and analyzing this data in a manner that maximizes its value and accessibility is critical for the scientific advances and benefits that such comprehensive TMJ OA patient information may enable. High dimensional databases are increasingly difficult to process using on-hand database management tools or traditional processing applications, creating a continuing demand for innovative approaches. Toward this end, the DCBIA at the Univ. of Michigan has partnered with the University of North Carolina, the University of Texas MD Anderson Cancer Center and Kitware Inc. Through high-dimensional quantitative characterization of individuals with TMJ OA, at molecular, clinical and imaging levels, we will identify phenotypes at risk for more severe prognosis, as well as targets for future therapies. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA. Due to its ubiquitous design in the web, DSCI software installation will no longer be required. Our long-term goal is to create software and data repository for Osteoarthritis of the TMJ. Such repository requires maintaining the data in a distributed computational environment to allow contributions to the database from multi-clinical centers and to share trained models for TMJ classification. In years 4 and 5 of the proposed work, the dissemination and training of clinicians at the Schools of Dentistry at the University of North Carol, Univ. of Minnesota and Oregon Health Sciences will allow expansion of the proposed studies. In Aim 1, we will test state-of-the-art neural network structures to develop a combined software module that will include the most efficient and accurate neural network architecture and advanced statistics to mine imaging, clinical and biological TMJ OA markers identified at baseline. In Aim 2, we propose to develop novel data analytics tools, evaluating the performance of various machine learning and statistical predictive models, including customized- Gaussian Process Regression, extreme boosted trees, Multivariate Varying Coefficient Model, Lasso, Ridge and Elastic net, Random Forest, pdfCluster, decision tree, and support vector machine. Such automated solutions will leverage emerging computing technologies to determine risk indicators for OA progression in longitudinal cohorts of TMJ health and disease. PROJECT NARRATIVE This application proposes the development of efficient web-based data management, mining, and analytics of clinical, biological, and high dimensional imaging data from TMJ OA patients. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA.",Integrative Predictors of Temporomandibular Osteoarthritis,10224492,R01DE024450,"['3-Dimensional', 'Age', 'Architecture', 'Arthritis', 'Benchmarking', 'Biological', 'Biological Markers', 'Blood', 'Bone remodeling', 'Bone structure', 'Cancer Center', 'Chronic', 'Classification', 'Clinical', 'Clinical Markers', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Diagnosis', 'Country', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Storage and Retrieval', 'Database Management Systems', 'Databases', 'Decision Trees', 'Degenerative polyarthritis', 'Dental', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Environment', 'Fibrocartilages', 'Future', 'Gaussian model', 'Goals', 'Hand', 'Health', 'Health Sciences', 'Image', 'Image Analysis', 'Individual', 'Inflammation Mediators', 'Inflammatory', 'Internet', 'Joints', 'Lasso', 'Longitudinal cohort', 'Machine Learning', 'Mandibular Condyle', 'Mediator of activation protein', 'Medicine', 'Methods', 'Michigan', 'Mining', 'Minnesota', 'Modeling', 'Molecular', 'Morphology', 'North Carolina', 'Online Systems', 'Oregon', 'Outcome', 'Pain', 'Paper', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Phenotype', 'Process', 'Property', 'Proteins', 'Publishing', 'Replacement Arthroplasty', 'Resolution', 'Risk', 'Saliva', 'School Dentistry', 'Scientific Advances and Accomplishments', 'Severities', 'Slice', 'Structure', 'Study models', 'Symptoms', 'System', 'Technology', 'Temporomandibular Joint', 'Temporomandibular joint osteoarthritis', 'Testing', 'Texas', 'Three-Dimensional Imaging', 'Training', 'Trees', 'Universities', 'University of Texas M D Anderson Cancer Center', 'Work', 'X-Ray Computed Tomography', 'analytical tool', 'base', 'bone', 'cartilage degradation', 'clinical center', 'clinical diagnostics', 'cone-beam computed tomography', 'craniofacial', 'craniomaxillofacial', 'data warehouse', 'deep learning', 'deep neural network', 'design', 'high dimensionality', 'imaging biomarker', 'improved', 'innovation', 'joint destruction', 'machine learning algorithm', 'neural network', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'outcome forecast', 'predictive modeling', 'prospective', 'quantitative imaging', 'random forest', 'repository', 'scale up', 'screening', 'serial imaging', 'software repository', 'statistical and machine learning', 'statistics', 'subchondral bone', 'support vector machine', 'tool']",NIDCR,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,233900,641965656,0.0028683574205746794
"Algorithmic identification of binding specificity mechanisms in proteins Project Summary Variations in protein binding preferences are a critical barrier to the precision treatment of disease. When high resolution structures of a protein are available, and many isoforms of the protein have been connected to dif- fering binding preferences, it is possible in principle to model the structures of all isoforms and discover the mechanisms that cause variations in binding preferences. Unfortunately, this discovery process depends on human expertise for examining molecular structure, and given that hundreds of isoforms may exist, a human would be overwhelmed to objectively examine many similar isoforms. To fill this gap, this project will (A1) de- velop software that identifies structural mechanisms that cause differential binding preferences, categorizes similar structural mechanisms, and explains the mechanisms in English. The second aim of this project (A2) is to validate the software at a large scale on families of proteins that exhibit a variety of well-examined binding preferences, and through blind predictions with experimental collaborators. Our approach involves creating software that mimics the visual reasoning techniques employed by structural biologists when examining molecular structures. Not only are these techniques responsible for most major dis- coveries in structural biology, but they are also straightforward to understand by non-computational research- ers. This property will enable our software to immediately integrate into existing workflows at labs that do not focus on computational methods. This property also contrasts from existing methods, which generally output structural models, potential energies, p-values and structural scores which are difficult for non-experts to un- derstand or incorporate into their research. Often, an expert in biophysics is required to interpret the outputs so that they can be operationalized in laboratory environments. In preliminary results, our methods have already identified molecular mechanisms that govern specificity in several families of proteins. Verification against peer-reviewed experimentation has proven the preliminary results correct in almost all cases. Our methods have also been applied to make a blind prediction of binding mechanisms in the ricin toxin, which binds to and damages the human ribosome. With experimental collabo- rators, we showed that our methods correctly identified and predicted the roles of several amino acids with a hitherto unknown role in recognizing the ribosome. Using our methodological approach and our rigorous valida- tion strategy, this project will produce a highly validated, usable software package that will bridge a critical gap in the development of precision therapies and diagnostics. Variations in protein binding preferences are a critical barrier to precision medicine and precise diagnostics. We will develop software that will identify and categorize molecular mechanisms that cause these variations. The resulting insights will enable clinicians to more precisely select therapies to achieve superior outcomes.",Algorithmic identification of binding specificity mechanisms in proteins,10021688,R01GM123131,"['Address', 'Algorithms', 'Amino Acids', 'Artificial Intelligence', 'Benchmarking', 'Binding', 'Binding Proteins', 'Binding Sites', 'Biochemical', 'Biophysical Process', 'Biophysics', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Computing Methodologies', 'Development', 'Diagnosis', 'Disease', 'Docking', 'Drug Targeting', 'Electrostatics', 'Elements', 'English Language', 'Environment', 'Evaluation', 'Exhibits', 'Feedback', 'HIV Protease', 'Hot Spot', 'Human', 'Hydrogen Bonding', 'Hydrophobicity', 'Immune', 'Individual', 'Influentials', 'Laboratories', 'Letters', 'Ligand Binding', 'Ligands', 'Link', 'Literature', 'Major Histocompatibility Complex', 'Maps', 'Measures', 'Mechanics', 'Methodology', 'Methods', 'Molecular', 'Molecular Conformation', 'Molecular Structure', 'Mutation', 'Nicotinic Receptors', 'Outcome', 'Output', 'Patients', 'Peer Review', 'Peptide Hydrolases', 'Population', 'Potential Energy', 'Precision therapeutics', 'Process', 'Property', 'Protein Family', 'Protein Isoforms', 'Proteins', 'Research', 'Resolution', 'Ribosomes', 'Ricin', 'Role', 'Serine Protease', 'Shapes', 'Site', 'Specificity', 'Structural Biologist', 'Structural Models', 'Structure', 'Techniques', 'Testing', 'Text', 'Toxin', 'Tweens', 'Universities', 'Validation', 'Variant', 'Visual', 'base', 'blind', 'human-in-the-loop', 'hydropathy', 'inhibitor/antagonist', 'insight', 'mutant', 'novel', 'personalized diagnostics', 'precision medicine', 'preference', 'protein structure', 'prototype', 'receptor', 'simulation', 'software development', 'structural biology', 'therapy development', 'tool', 'tumor']",NIGMS,LEHIGH UNIVERSITY,R01,2020,252470,6676095,0.020769665611445658
"Automated Molecular Identity Disambiguator (AutoMID) PROJECT SUMMARY Small molecules are one of the most important classes of therapeutics alleviating suffering and in many cases death for hundreds of millions of people worldwide. Small molecules also serve as invaluable tools to study biology, often with the goal to validate novel targets for the development of future therapeutic drugs. Reproducibility of experimental results and the interoperability and reusability of resulting datasets depend on accurate descriptions of associated research objects, and most critically on correct representations of small molecules that are tested in biological assays. For example, it is not possible to develop predictive models of protein target - small molecule interactions if their chemical structure representations are not correct. Many factors contribute to errors in reported chemical structures in small molecule screening and omics reference databases, scientific publications, and many other web-based resources and documents. Because of the complexity of representing small molecules chemical structure graphs and the lack of thorough curation, errors are frequently introduced by non-experts and error propagation across different digital research assets is a pervasive problem. To address this challenging problem via a scalable approach, we propose the Automated Molecular Identity Disambiguator (AutoMID). AutoMID will be usable in batch mode at scale via an API, for example to assist chemical structure standardization and registration by maintainers of digital research assets, and also via interactive (UI) mode for everyday researchers to quickly and easily validate or correct their small molecule representations. AutoMID will leverage extensive highly standardized linked databases of chemical structures and associated information including names, synonyms, biological activity and physical properties and their sources / provenance and leverage expert rules and AI to enable reliable disambiguation of chemical structure identities at scale. PROJECT NARRATIVE Small molecules are one of the most important types of drugs. They also serve as invaluable tools to study biology. The complexity of representing chemical graphs and the lack of thorough curation leads to frequent small molecule structure errors, which propagate across digital research assets, impeding their interoperability and reusability. To address this challenging problem, we propose the Automated Molecular Identity Disambiguator (AutoMID). Built on expert knowledge and AI, AutoMID will enable researchers and maintainers of data repositories to reliably identify and resolve ambiguities in chemical structures at scale.",Automated Molecular Identity Disambiguator (AutoMID),9987129,R01LM013391,"['Address', 'Adoption', 'Biological', 'Biological Assay', 'Biology', 'Categories', 'Cessation of life', 'Chemical Structure', 'Chemicals', 'Classification', 'Complex', 'Data', 'Data Element', 'Data Set', 'Data Sources', 'Databases', 'Deposition', 'Detection', 'Development', 'FAIR principles', 'Future', 'Goals', 'Graph', 'Hand', 'Hybrids', 'In Vitro', 'Individual', 'Knowledge', 'Legal patent', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Metadata', 'Modeling', 'Molecular', 'Molecular Structure', 'Names', 'Pharmaceutical Chemistry', 'Pharmaceutical Preparations', 'Postdoctoral Fellow', 'Privatization', 'Property', 'Proteins', 'Publications', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Semantics', 'Source', 'Standardization', 'Structure', 'Testing', 'Therapeutic', 'Time', 'Training', 'base', 'cheminformatics', 'data harmonization', 'data modeling', 'data warehouse', 'design', 'digital', 'high throughput screening', 'improved', 'in silico', 'in vivo', 'interoperability', 'knowledge curation', 'novel', 'online resource', 'physical property', 'predictive modeling', 'relational database', 'screening', 'small molecule', 'software systems', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,R01,2020,293345,157845771,-0.006490420239848862
"Inferential methods for functional data from wearable devices Project Summary/Abstract This is a project to develop new statistical methods for comparing groups of subjects in terms of health outcomes that are assessed using data from wearable devices. Inexpensive wearable sensors for health monitoring are now capable of generating massive amounts of data collected longitudinally, up to months at a time. The project will develop inferential methods that can deal with the complexity of such data. A serious challenge is the presence of unmeasured time-dependent confounders (e.g., circadian and dietary patterns), making direct comparisons or borrowing strength across subjects untenable unless the studies are carried out in controlled experimental con- ditions. Generic data mining and machine learning tools have been widely used to provide predictions of health status from such data. However, such tools cannot be used for signiﬁcance testing of covariate effects, which is necessary for designing precision medicine interventions, for example, without taking the inherent model selection or the presence of the unmeasured confounders into account. To overcome these difﬁculties, a systematic de- velopment of inferential methods for functional outcome data obtained from wearable devices will be carried out. There are three speciﬁc aims: 1) Develop metrics for functional outcome data from wearable devices, 2) Develop nonparametric estimation and testing methods for activity proﬁles and a screening method for predictors of activity proﬁles, 3) Implement the methods in an R package and carry out two case studies using accelerometer data. For Aim 1, the approach is to reduce the sensor data to occupation time proﬁles (e.g., as a function of activity level), and formulate the statistical modeling in terms of these proﬁles using survival and functional data analytic meth- ods. This will have a number of advantages, the principal one being that time-dependent confounders become less problematic because the effect of differences in temporal alignment across subjects is mitigated. In addition, survival analysis methods can be applied by viewing the occupation time as a time-to-event outcome indexed by activity level. For Aim 2, nonparametric methods will be used to compare and order occupation time distributions between groups of subjects that are speciﬁed in terms of baseline covariate levels or treatment groups. Further, a new method of post-selection inference based on marginal screening for function-on-scalar regression will be developed to identify and formally test whether covariates are signiﬁcantly associated with activity proﬁles. Aim 3 will develop an R-package implementation, and as a test-bed for the proposed methods they will be applied to two Columbia-based clinical studies: to the study of physical activity in children enrolled in New York City Head Start, and to the study of experimental drugs for the treatment of mitochondrial depletion syndrome. Project Narrative The relevance of the project to public health is that it will develop statistical methods for the physiological eval- uation of patients on the basis of data collected by inexpensive wearable sensors (e.g., accelerometers). By introducing methods for the rigorous comparison of healthcare status among groups of patients observed longi- tudinally over time using such devices, treatment decisions that can beneﬁt targeted populations of patients in terms of continuously-assessed health outcomes will become possible.",Inferential methods for functional data from wearable devices,9924432,R01AG062401,"['Acceleration', 'Accelerometer', 'Beds', 'Bypass', 'Case Study', 'Characteristics', 'Child', 'Clinical Research', 'Computer software', 'Data', 'Data Analytics', 'Development', 'Devices', 'Dietary Practices', 'Drug Combinations', 'Enrollment', 'Evaluation', 'Event', 'Grant', 'Head Start Program', 'Health', 'Health Status', 'Healthcare', 'Intervention', 'Lead', 'Machine Learning', 'Measures', 'Methods', 'Mitochondria', 'Modeling', 'Molecular', 'Monitor', 'Motivation', 'Nature', 'New York City', 'Obesity', 'Occupations', 'Outcome', 'Outcome Measure', 'Patients', 'Pharmacotherapy', 'Physical activity', 'Physiological', 'Preschool Child', 'Process', 'Proxy', 'Public Health', 'Recording of previous events', 'Regimen', 'Signal Transduction', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Stochastic Processes', 'Survival Analysis', 'Syndrome', 'Target Populations', 'Techniques', 'Testing', 'Time', 'Work', 'analytical method', 'base', 'circadian', 'data mining', 'design', 'experimental study', 'functional outcomes', 'indexing', 'interest', 'lower income families', 'novel', 'patient population', 'precision medicine', 'screening', 'sensor', 'theories', 'time use', 'tool', 'treatment group', 'wearable device', 'wearable sensor technology']",NIA,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2020,298890,558628098,0.0017333610305145734
"Statistical Methods in Trans-Omics Chronic Disease Research Project Summary The broad, long-term objectives of this research are the development of novel and high-impact statistical methods for medical studies of chronic diseases, with a focus on trans-omics precision medicine research. The speciﬁc aims of this competing renewal application include: (1) derivation of efﬁcient and robust statistics for integrative association analysis of multiple omics platforms (DNA sequences, RNA expressions, methylation proﬁles, protein expressions, metabolomics proﬁles, etc.) with arbitrary patterns of missing data and with detection limits for quantitative measurements; (2) exploration of statistical learning approaches for handling multiple types of high- dimensional omics variables with structural associations and with substantial missing data; and (3) construction of a multivariate regression model of the effects of somatic mutations on gene expressions in cancer tumors for discovery of subject-speciﬁc driver mutations, leveraging gene interaction network information and accounting for inter-tumor heterogeneity in mutational effects. All these aims have been motivated by the investigators' applied research experience in trans-omics studies of cancer and cardiovascular diseases. The proposed solutions are based on likelihood and other sound statistical principles. The theoretical properties of the new statistical methods will be rigorously investigated through innovative use of advanced mathematical arguments. Computationally efﬁcient and numerically stable algorithms will be developed to implement the inference procedures. The new methods will be evaluated extensively with simulation studies that mimic real data and applied to several ongoing trans-omics precision medicine projects, most of which are carried out at the University of North Carolina at Chapel Hill. Their scientiﬁc merit and computational feasibility are demonstrated by preliminary simulation results and real examples. Efﬁcient, reliable, and user-friendly open-source software with detailed documentation will be produced and disseminated to the broad scientiﬁc community. The proposed work will advance the ﬁeld of statistical genomics and facilitate trans-omics precision medicine studies of chronic diseases. Project Narrative The proposed research intends to develop novel and high-impact statistical methods for integrative analysis of trans-omics data from ongoing precision medicine studies of chronic diseases. The goal is to facilitate the creation of a new era of medicine in which each patient receives individualized care that matches their genetic code.",Statistical Methods in Trans-Omics Chronic Disease Research,9855035,R01HG009974,"['Accounting', 'Address', 'Algorithms', 'Applied Research', 'Biological', 'Cardiovascular Diseases', 'Characteristics', 'Chronic Disease', 'Communities', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Derivation procedure', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Documentation', 'Equation', 'Formulation', 'Gene Expression', 'Genes', 'Genetic Code', 'Genetic Transcription', 'Genomics', 'Goals', 'Grant', 'Information Networks', 'Institution', 'Inter-tumoral heterogeneity', 'Joints', 'Knowledge', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Medicine', 'Mental disorders', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular', 'Molecular Abnormality', 'Molecular Profiling', 'Mutation', 'Mutation Analysis', 'National Human Genome Research Institute', 'North Carolina', 'Patients', 'Pattern', 'Precision Medicine Initiative', 'Prevention', 'Procedures', 'Process', 'Property', 'Public Health', 'Research', 'Research Personnel', 'Resources', 'Somatic Mutation', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Tail', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'disease phenotype', 'driver mutation', 'experience', 'gene interaction', 'genome sequencing', 'high dimensionality', 'innovation', 'machine learning method', 'metabolomics', 'multidimensional data', 'multiple omics', 'novel', 'open source', 'outcome prediction', 'personalized care', 'precision medicine', 'programs', 'protein expression', 'research and development', 'semiparametric', 'simulation', 'sound', 'statistical learning', 'statistics', 'theories', 'tool', 'tumor', 'tumor heterogeneity', 'user-friendly']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2020,305167,511185245,0.002075909372766252
"Sample-specific Models for Molecular Portraits of Diseases in Precision Medicine A fundamental challenge in precision medicine is to understand the patterns of differentiation between individuals. To address this challenge, we propose to go beyond the traditional `one disease--one model' view of bioinformatics and pursue a new view built upon personalized patient models that facilitates precision medicine by leveraging both commonalities within a patient cohort as well as signatures unique to every individual patient. With the emergence of large-scale databases such as The Cancer Genome Atlas (TCGA), the International Cancer Genome Consortium (ICGC), and the Gene Expression Omnibus (GEO), which collect multi-omic data on many different diseases, a new “pan-omics” and “pan-disease” paradigm has emerged to jointly analyze all patients in a disease cohort while accounting for patient-specific effects. An example of this is the recently released Pan-Cancer Atlas. At the same time, next generation statistical tools to accurately and rigorously draw the necessary inferences are lacking. In this project we propose a series of mathematically rigorous, statistically sound, and computationally feasible approaches to infer sample-specific models, providing a more complete view of heterogeneous datasets. By bringing together ideas from the machine learning, statistics, and mathematical optimization communities, we provide a rigorous framework for precision medicine via sample-specific statistical models. Crucially, we propose to analyze this framework and prove strong theoretical guarantees under weak assumptions--this dramatically distinguishes our framework from much of the existing literature. Towards these goals, we propose the following aims: Aim 1: Discovery of new molecular profiles with sample-specific statistical models. We propose a general framework for inferring sample-specific models with low-rank structure based on the novel concept of distance-matching. This allows us to infer statistical models at the level of a single patient without overfitting, and is general enough to be applied for prediction, classification, and network inference as well as a variety of diseases and phenotypes. Aim 2: Multimodal approaches to personalized diagnosis--contextually interpretable models for actionable clinical decision support. In order to translate these models into practice, we propose a novel interpretable predictive model that supports complex, multimodal data types such as images and text combined with high-level interpretable features such as SNP data, gender, age, etc. This framework simultaneously boosts the accuracy of clinical predictions by exploiting sample heterogeneity while providing human-digestable explanations for the predictions being made. Aim 3: Next-generation precision medicine--algorithms and software for personalized estimation. To put our models into practical use, we will develop new algorithms for interpretable prediction of personalized clinical outcomes and visualization of personalized statistical models. All of our tools will be combined into a user-friendly software package called PrecisionX that will be freely available to researchers and clinicians everywhere. RELEVANCE (See instructions): Personalization with data is a critical challenge whenever decisions must be made at scale, and has applications that go beyond precision medicine; businesses, educational institutions, and financial institutions are among the many players that have acknowledged a stake in this complex problem. We expect the proposed work to provide a rigorous foundation for personalization with large and high-dimensional datasets, finding use throughout the broader scientific community as well as with industry and educational institutions. Alongside our collaboration with Pitt/UPMC, we will work with physicians and data scientists for practical feedback as well as provide training in the methods developed. n/a",Sample-specific Models for Molecular Portraits of Diseases in Precision Medicine,10133782,R01GM140467,"['Accounting', 'Address', 'Age', 'Algorithmic Software', 'Algorithms', 'Atlases', 'Bioinformatics', 'Businesses', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Scientist', 'Data Set', 'Disease', 'Feedback', 'Foundations', 'Gender', 'Gene Expression', 'Goals', 'Heterogeneity', 'Human', 'Image', 'Individual', 'Industry', 'Institution', 'Instruction', 'International', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Methods', 'Modeling', 'Molecular Profiling', 'Multiomic Data', 'Outcome', 'Patients', 'Pattern', 'Physicians', 'Portraits', 'Research Personnel', 'Sampling', 'Series', 'Statistical Models', 'Structure', 'Text', 'The Cancer Genome Atlas', 'Time', 'Training', 'Translating', 'Visualization', 'Work', 'base', 'cancer genome', 'clinical decision support', 'clinically actionable', 'cohort', 'disease phenotype', 'heterogenous data', 'high dimensionality', 'individual patient', 'large-scale database', 'molecular modeling', 'multimodal data', 'multimodality', 'next generation', 'novel', 'personalized diagnostics', 'personalized predictions', 'precision medicine', 'predictive modeling', 'sound', 'statistics', 'tool', 'user friendly software']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2020,305566,30434536,-0.0007754472745872737
"Image-guided Biocuration of Disease Pathways From Scientific Literature Realization of precision medicine ideas requires an unprecedented rapid pace of translation of biomedical discoveries into clinical practice. However, while many non-canonical disease pathways and uncommon drug actions, which are of vital importance for understanding individual patient-specific disease pathways, are accumulated in the literature, most are not organized in databases. Currently, such knowledge is curated manually or semi-automatically in a very limited scope. Meanwhile, the volume of biomedical information in PubMed (currently 28 million publications) keeps growing by more than a million articles per year, which demands more efficient and effective biocuration approaches.  To address this challenge, a novel biocuration method for automatic extraction of disease pathways from figures and text of biomedical articles will be developed.  Specific Aim 1: To develop focused benchmark sets of articles to assess the performance of the biocuration pipeline.  Specific Aim 2: To develop a method for extraction of components of disease pathways from articles’ figures based on deep-learning techniques.  Specific Aim 3: To develop a method for reconstruction of disease-specific pathways through enrichment and through graph neural network (GNN) approaches.  Specific Aim 4: To conduct a comprehensive evaluation of the pipeline.  The overarching goal of this project is to develop a computer-based automatic biocuration ecosystem for rapid transformation of free-text biomedical literature into a machine-processable format for medical applications.  The overall impact of the proposed project will be to significantly improve health outcomes in individualized patient cases by efficiently bringing the latest biomedical discoveries into a precision medicine setting. It will especially benefit cancer patients for which up-to-date knowledge of newly discovered molecular mechanisms and drug actions is critical. The overall impact of the proposed project will be to significantly improve health outcomes in individualized patient cases by efficiently bringing the latest biomedical discoveries into a precision medicine setting. In this project, a novel biocuration method for an automatic extraction of disease mechanisms from figures and text in scientific literature will be developed. These mechanisms will be stored in a database for further querying to assist in medical diagnosis and treatment.",Image-guided Biocuration of Disease Pathways From Scientific Literature,9987133,R01LM013392,"['Address', 'Architecture', 'Benchmarking', 'Biological', 'Cancer Patient', 'Communities', 'Computers', 'Databases', 'Deposition', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Pathway', 'Ecosystem', 'Elements', 'Evaluation', 'Feedback', 'Genes', 'Goals', 'Graph', 'Health', 'Image', 'Informatics', 'Knowledge', 'Label', 'Language', 'Link', 'Literature', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Manuals', 'Measures', 'Medical', 'Methods', 'Molecular', 'Molecular Analysis', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Oxidative Stress', 'Pathway interactions', 'Patients', 'Performance', 'Phenotype', 'PubMed', 'Publications', 'Regulation', 'Reporting', 'Research', 'Retrieval', 'Selection Criteria', 'Signal Pathway', 'Source', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Translations', 'Visual', 'Work', 'base', 'clinical practice', 'deep learning', 'design', 'detector', 'drug action', 'image guided', 'improved', 'individual patient', 'knowledge base', 'knowledge curation', 'multimodality', 'neural network', 'neural network architecture', 'novel', 'precision medicine', 'reconstruction', 'success', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2020,313495,63611576,-0.0010674808538325975
"Integrated Instrument for non-natural aptamer generation Project Summary DNA and RNA aptamers are a useful class of synthetic affinity reagents. However, their performance can be greatly improved through the site-specific incorporation of chemically modified, ‘non-natural’ nucleotides that provide a greater chemical repertoire to enable superior aptamer affinity and specificity. Because a broad spectrum of chemical functional groups can be incorporated, non-natural aptamers offer the exciting potential for targeting molecules for which the generation of monoclonal antibodies remains difficult, such as small- molecule drugs, metabolites and carbohydrates. Unfortunately, the access to non-natural aptamers is severely limited. This is because the process of generating non-natural aptamers is technically challenging and limited to a few specialized laboratories. The goal of this project is to develop an integrated instrument, the Non-Natural Aptamer Array (N2A2) that eliminates these bottlenecks and enable rapid and facile non-natural aptamer discovery at virtually any research laboratory. The N2A2 will be built on a modified version of a benchtop commercial sequencer (Illumina MiSeq), and will perform every stage of non-natural aptamer discovery— including sequencing, screening and binding measurements—as part of a single work-flow. There are three main innovative aspects of our N2A2 system. First, our approach will entirely eliminate the need for polymerase engineering, and thus allows us to incorporate virtually any chemical functional group through click chemistry. Second, N2A2 will enable us to directly obtain the binding affinity (Kd) of ~10^7 aptamers directly in complex samples (e.g. cell lysate or serum), thereby resulting in aptamers with high-specificity. Finally, we will develop a machine-learning (ML) approach to identify key motifs (“k-mers”) and predict novel sequences with potentially higher affinity and specificity that can be tested using the N2A2 instrument. We believe this powerful combination of massively parallel, sequence-linked binding measurements with ML-based predictions will allow us to explore sequence space that is currently inaccessible to traditional in vitro selection methods, and enable us to discover aptamers with superior performance. The success of this project will produce an integrated instrument that greatly streamlines and accelerates the discovery of non-natural aptamers for a wide range of targets in complex media. The instrument is based on a commercially available sequencer and we will make all software available to the public. In this way, we believe the N2A2 instrument could broadly expand access to robust, high quality, custom affinity reagents for biomedical research and clinical diagnostics. Project Narrative We will develop an integrated instrument that simplifies the discovery of non-natural aptamer reagents for a wide range of molecules that are difficult to target using conventional antibody reagents. The access to these custom reagents will accelerate biomedical research and clinical diagnostics.",Integrated Instrument for non-natural aptamer generation,9872178,R01GM129313,"['Affinity', 'Algorithms', 'Antibodies', 'Binding', 'Biomedical Research', 'Carbohydrates', 'Cells', 'Chemicals', 'Chemistry', 'Chinese Hamster Ovary Cell', 'Complex', 'Computer software', 'Custom', 'DNA', 'Data', 'Directed Molecular Evolution', 'Engineering', 'Generations', 'Goals', 'Graph', 'In Vitro', 'Label', 'Laboratories', 'Laboratory Research', 'Link', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Monoclonal Antibodies', 'Nucleotides', 'Opioid', 'Performance', 'Pharmaceutical Preparations', 'Polymerase', 'Process', 'Proteins', 'RNA', 'Reagent', 'Reproducibility', 'Sampling', 'Serum', 'Site', 'Specificity', 'System', 'Testing', 'Tyrosine', 'Work', 'aptamer', 'base', 'clinical diagnostics', 'data analysis pipeline', 'functional group', 'improved', 'innovation', 'instrument', 'machine learning algorithm', 'novel', 'programmed cell death protein 1', 'scaffold', 'screening', 'small molecule', 'success', 'virtual']",NIGMS,STANFORD UNIVERSITY,R01,2020,314000,560644462,0.0016641053416738133
"The Transporter Classification Database (TCDB) ABSTRACT  Transporters catalyze entry and exit of molecules into and out of cells and organelles. They achieve cellular homeostasis, are responsible for multidrug resistance in pathogens and tumors, and, when defective, cause dozens of important human genetic diseases. Our laboratory maintains, updates and improves the Transporter Classification Database, TCDB, which houses the Transporter Classification (TC) system, adopted officially by the International Union of Biochemistry and Molecular Biology (IUBMB). TCDB is the internationally acclaimed, carefully annotated, universal standard for classifying and providing information about transporters and transport-related proteins in all major domains of life. It presents sequence, biochemical, physiological, pathological, structural and evolutionary data about these proteins and the transport systems they comprise. It uses a successful system of classification based on transporter class, subclass, family, subfamily, individual transport system and constituent proteins. It also includes a superfamily hyperlink.  In this competitive renewal of GM0077402, we propose to continue to expand, update, and semi-automate TCDB. Our specific aims are to (1) upgrade TCDB by characterizing and categorizing protein domains and their topologies, motifs, repeat units, functional interactions, alternative splicing and post-translational modifications, (2) expand TCDB by implementing novel pipelines for data entry that will increase the coverage of transport diversity in TCDB while describing more effectively the complexity of multicomponent transport systems, (3) enter into TCDB transporter modulators such as activators, inhibitors, drugs and xenobiotics as well as internal and external conditions that influence transporter activities, while generating an ontology to describe the effects of chemical modulators that will complement our substrate ontology, (4) incorporate into TCDB synthetic pores/channels (TC subclass 1.D), and carriers (TC subclass 2.B), (5) introduce into TCDB connections between transport and metabolism and (6) expand our plans for long-term TCDB sustainability. PROJECT NARRATIVE  TCDB is the only IUBMB-approved database providing the worldwide scientific community with systematic information about proteins that catalyze transmembrane transport. Transport proteins play critical roles in health-related issues such as personalized medicine, cancer, drug development, bacterial pathogenesis and antimicrobial resistance. Funding of this proposal will allow us to provide research results, as well as high-quality data and software for the identification of transporter proteins useful to scientists whose investigations focus on health issues.",The Transporter Classification Database (TCDB),9895808,R01GM077402,"['Activator Appliances', 'Adopted', 'Affect', 'Alternative Splicing', 'Antimicrobial Resistance', 'Antineoplastic Agents', 'Biochemical', 'Biochemistry', 'Biological Phenomena', 'Biology', 'Biotechnology', 'Carrier Proteins', 'Cells', 'Chemicals', 'Classification', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Data', 'Database Management Systems', 'Databases', 'Development', 'Ecosystem', 'Ensure', 'Eukaryota', 'Family', 'Funding', 'Genetic Diseases', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Homeostasis', 'Human Genetics', 'Individual', 'Information Resources', 'International', 'Investigation', 'Island', 'Knowledge', 'Laboratories', 'Life', 'Ligands', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Membrane Proteins', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Multi-Drug Resistance', 'Ontology', 'Organelles', 'Pathogenesis', 'Pathologic', 'Pharmaceutical Preparations', 'Phenotype', 'Physiological', 'Play', 'Post-Translational Protein Processing', 'Production', 'Protein Isoforms', 'Proteins', 'Pump', 'RNA Splicing', 'Research', 'Research Personnel', 'Role', 'Scientist', 'Stimulus', 'Structure', 'System', 'Systems Biology', 'Tertiary Protein Structure', 'Time', 'Tissues', 'Training', 'Transmembrane Transport', 'Update', 'Variant', 'Vertebral column', 'Work', 'Xenobiotics', 'base', 'data pipeline', 'drug development', 'improved', 'inhibitor/antagonist', 'insight', 'member', 'metagenome', 'novel', 'pathogen', 'personalized medicine', 'protein transport', 'screening', 'text searching', 'transmission process', 'tumor', 'whole genome', 'willingness']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2020,315000,524978793,0.013487446458726203
"Synergistic integration of topology and machine learning for the predictions of protein-ligand binding affinities and mutation impacts Project Summary Fundamental challenges that hinder the current understanding of biomolecular systems are their tremendous complexity, high dimensionality and excessively large data sets associated with their geometric modeling and simulations. These challenges call for innovative strategies for handling massive biomolecular datasets. Topology, in contrast to geometry, provides a unique tool for dimensionality reduction and data simplification. However, traditional topology typically incurs with excessive reduction in geometric information. Persistent homology is a new branch of topology that is able to bridge traditional topology and geometry, but suffers from neglecting biological information. Built upon PI’s recent work in the topological data analysis of biomolecules, this project will explore how to integrate topological data analysis and machine learning to significantly improve the current state-of-the-art predictions of protein-ligand binding and mutation impact established in the PI’s preliminary studies. These improvements will be achieved through developing physics-embedded topological methodologies and advanced deep learning architectures for tackling heterogeneous biomolecular data sets arising from a variety of physical and biological considerations. Finally, the PI will establish robust databases and online servers for the proposed predictions. Project Narrative The project concerns the integration of topological data analysis and machine learning architectures for the predictions of protein-ligand binding affinities and mutation induced protein stability changes from massive data sets. This new data approach has considerable impact for future generation methods in computational biophysics and drug design.",Synergistic integration of topology and machine learning for the predictions of protein-ligand binding affinities and mutation impacts,9989158,R01GM126189,"['3-Dimensional', 'Address', 'Affinity', 'Architecture', 'Big Data', 'Binding', 'Binding Proteins', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Biophysics', 'Characteristics', 'Chemicals', 'Classification', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Dimensions', 'Drug Design', 'Electrostatics', 'Elements', 'Free Energy', 'Freedom', 'Future Generations', 'Geometry', 'Handwriting', 'Image Analysis', 'Induced Mutation', 'Ions', 'Learning', 'Ligand Binding', 'Ligands', 'Lipids', 'Machine Learning', 'Medical', 'Membrane', 'Membrane Proteins', 'Metals', 'Methodology', 'Methods', 'Mutation', 'Physics', 'Plant Roots', 'Proteins', 'Psychological Transfer', 'Site', 'Speech', 'System', 'Techniques', 'Thermodynamics', 'Work', 'algebraic topology', 'base', 'cofactor', 'data warehouse', 'deep learning', 'deep learning algorithm', 'direct application', 'diverse data', 'high dimensionality', 'improved', 'innovation', 'language processing', 'large datasets', 'learning algorithm', 'learning strategy', 'machine learning algorithm', 'metallicity', 'models and simulation', 'multi-task learning', 'multitask', 'mutant', 'neglect', 'next generation', 'search engine', 'tool', 'trend', 'user-friendly']",NIGMS,MICHIGAN STATE UNIVERSITY,R01,2020,318777,89938253,0.015889504472531187
"Combining chemical and computational tools for predictive models of microbiome communities ABSTRACT The gut microbiome has a tremendous impact on health and disease, actively contributing to obesity, diabetes, inflammatory bowel disease, cardiovascular diseases, and several poorly understood neurological disorders. We do not yet have the necessary tools to precisely probe these microbial communities, though such tools could unlock extensive benefits to human health. Elucidating the contributions of individual species or consortia of bacteria would provide a rational basis for understanding microbiota-controlled disease and lead to novel therapies. To carry out the fundamental research planned in this proposal, we will tackle three major problems: First, we will build the first set of molecular tools that effectively and precisely modulate the microbiome bacteria; second, we will analyze the multiscale dynamics of microbial communities; and third, we will construct an ingestible biosensor for real-time monitoring of microbiome populations. Although antibiotics and fecal transplants can reconfigure microbial consortia, they do not precisely target individual bacteria. Conversely, antimicrobial peptides (AMPs) have evolved to selectively attack pathogenic bacteria but do not target microbiome bacteria, constituting desirable scaffolds for molecular engineering and potential sources of microbiome-targeting agents. We will develop a new computational peptide design methodology, based on classical and hybrid-quantum mechanical molecular dynamics (MD) simulations, to create a groundbreaking assessment of the dynamical and emergent properties of AMPs. Chemical synthesis and large-scale screening will confirm predicted selectivity against microbiome species, and a machine learning workflow will connect sequences of individual peptides to their dynamics and activity. We will then apply the synthetic AMPs to interrogate the human microbiome by selectively removing species during bacterial consortia experiments, to be carried out in bioreactors, under regular or anaerobic conditions. We will pair our experiments with whole-cell metabolic network models, providing a systems biology perspective to the analysis of inter-species interactions. An integrated ingestible biosensing device will be developed to monitor the microbiome by electrochemically sensing unique biomarkers from gut microbes. This will provide the first real-time measurements of microbiome composition and will be integrated to our bioreactors for testing, to ultimately be used for in vivo tests. This work will build the first set of molecular and computational tools for microbiome engineering and will lay the foundation to address critical gaps in our understanding of the gut micro-environment, and of the contributions of gut bacteria to the etiology of disease. Grounded in our demonstrated expertise in synthetic biology, computer science, microbiology, and electrical engineering, this project will provide a computational- experimental framework for developing a peptide encyclopedia for the gut microbiome, in line with NIH's public health mission and goals. PROJECT NARRATIVE  The gut microbiome plays roles in nutrition, immunity, metabolism, and several poorly understood neurological disorders. Suitable tools, however, do not yet exist for engineering the microbial communities that constitute the human microbiome. The proposed research introduces the first molecular tools to precisely understand the functions of microbiome communities in our health and disease in order to then delineate therapeutic interventions for diseases mediated by the gut microbiota, thereby addressing NIH's public health mission.",Combining chemical and computational tools for predictive models of microbiome communities,10029354,R35GM138201,"['Address', 'Anaerobic Bacteria', 'Antibiotics', 'Bacteria', 'Biochemical Pathway', 'Biological Markers', 'Bioreactors', 'Biosensing Techniques', 'Biosensor', 'Cardiovascular Diseases', 'Cells', 'Chemicals', 'Communities', 'Devices', 'Diabetes Mellitus', 'Disease', 'Electrical Engineering', 'Encyclopedias', 'Engineering', 'Etiology', 'Foundations', 'Goals', 'Health', 'Human', 'Human Microbiome', 'Hybrids', 'Immunity', 'Individual', 'Inflammatory Bowel Diseases', 'Lead', 'Machine Learning', 'Mechanics', 'Mediating', 'Metabolism', 'Methodology', 'Microbiology', 'Mission', 'Molecular', 'Molecular Computations', 'Monitor', 'Obesity', 'Peptides', 'Play', 'Population', 'Property', 'Public Health', 'Research', 'Role', 'Source', 'Systems Biology', 'Testing', 'Therapeutic Intervention', 'United States National Institutes of Health', 'Work', 'antimicrobial peptide', 'base', 'chemical synthesis', 'computer science', 'computerized tools', 'design', 'experimental study', 'fecal transplantation', 'fundamental research', 'gut bacteria', 'gut microbes', 'gut microbiome', 'gut microbiota', 'in vivo evaluation', 'microbial community', 'microbiome', 'microbiome composition', 'microbiota', 'molecular dynamics', 'nervous system disorder', 'network models', 'novel therapeutics', 'nutrition', 'pathogenic bacteria', 'predictive modeling', 'quantum', 'real time monitoring', 'scaffold', 'screening', 'synthetic biology', 'targeted agent', 'temporal measurement', 'tool']",NIGMS,UNIVERSITY OF PENNSYLVANIA,R35,2020,342713,593605914,0.007500690515166488
"Advanced Computational Approaches for NMR Data-mining ABSTRACT Nuclear magnetic resonance spectroscopy (NMR)-based metabolomics is a powerful method for identifying metabolic perturbations that report on different biological states and sample types. Compared to mass spectrometry, NMR provides robust and highly reproducible quantitative data in a matter of minutes, which makes it very suitable for first-line clinical diagnostics. Although the metabolome is known to provide an instantaneous snap-shot of the biological status of a cell, tissue, and organism, the utilization of NMR in clinical practice is hindered by cumbersome data analysis. Major challenges include high-dimensionality of the data, overlapping signals, variability of resonance frequencies (chemical shift), non-ideal shapes of signals, and low signal-to-noise ratio (SNR) for low concentration metabolites. Existing approaches fail to address these challenges and sample analysis is time-consuming, manually done, and requires considerable knowledge of NMR spectroscopy. Recent developments in the field of sparse methods for machine learning and accelerated convex optimization for high dimensional problems, as well as kernel-based spatial clustering show promise at enabling us to overcome these challenges and achieve fully automated, operator-independent analysis. We are developing two novel, powerful, and automated algorithms that capitalize on these recent developments in machine learning. In Aim 1, we describe ‘NMRQuant’ for automated identification and quantification of annotated metabolites irrespective of the chemical shift, low SNR, and signal shape variability. In Aim 2, we describe ‘SPA-STOCSY’ for automated de-novo identification of molecular fragments of unknown, non- annotated metabolites. Based on substantial preliminary data, we propose to evaluate these algorithms' sensitivity, specificity, stability, and resistance to noise on phantom, biological, and clinical samples, comparing them to current methods. We will validate the accuracy of analyses by experimental 2D NMR, spike-in, and mass spectrometry. The proposed efforts will produce new NMR analytical software for discovery of both annotated and non-annotated metabolites, substantially improving accuracy and reproducibility of NMR analysis. Such analytical ability would change the existing paradigm of NMR-based metabolomics and provide an even stronger complement to current mass spectrometry-based methods. This approach, once thoroughly validated, will enable NMR to reach wide network of applications in biomedical, pharmaceutical, and nutritional research and clinical medicine. NARRATIVE This project seeks to develop an advanced and automated platform for identifying NMR metabolomics biomarkers of diseases and for fundamental studies of biological systems. When fully developed, these approaches could be used to detect small molecules in the blood or urine, indicative of the onset of various diseases, drug toxicity, or environmental effects on the organism.",Advanced Computational Approaches for NMR Data-mining,9889134,R01GM120033,"['Address', 'Algorithms', 'Animal Disease Models', 'Biological', 'Biological Markers', 'Blood', 'Cardiovascular Diseases', 'Cells', 'Chemicals', 'Clinic', 'Clinical', 'Clinical Medicine', 'Complement', 'Computer software', 'Consumption', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Drug toxicity', 'Early Diagnosis', 'Frequencies', 'Health', 'Human', 'Knowledge', 'Left', 'Libraries', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Mass Spectrum Analysis', 'Measures', 'Medical', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'NMR Spectroscopy', 'Nature', 'Neurodegenerative Disorders', 'Noise', 'Nuclear Magnetic Resonance', 'Nutritional', 'Obesity', 'Organism', 'Outcome', 'Patients', 'Pharmacologic Substance', 'Phenotype', 'Plague', 'Process', 'Regulation', 'Relaxation', 'Reporting', 'Reproducibility', 'Research', 'Residual state', 'Resistance', 'Sampling', 'Sensitivity and Specificity', 'Shapes', 'Signal Transduction', 'Societies', 'Sodium Chloride', 'Spectrum Analysis', 'Statistical Algorithm', 'Structure', 'Temperature', 'Time', 'Tissues', 'Treatment outcome', 'Urine', 'Variant', 'automated algorithm', 'automated analysis', 'base', 'biological systems', 'biomarker discovery', 'clinical diagnostics', 'clinical implementation', 'clinical practice', 'computational suite', 'data mining', 'experimental analysis', 'experimental study', 'high dimensionality', 'improved', 'infancy', 'machine learning method', 'metabolome', 'metabolomics', 'multidimensional data', 'novel', 'personalized medicine', 'phenotypic biomarker', 'small molecule', 'stem']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2020,356625,323604360,0.01289135021324345
"Center for Machine Learning in Urology PROJECT SUMMARY We propose to establish an Exploratory Center for Interdisciplinary Research in Benign Urology at the Children’s Hospital of Philadelphia (CHOP) and the University of Pennsylvania (Penn), the central mission of which is to apply machine learning to improve the understanding of the pathophysiology, diagnosis, risk stratification, and prediction of treatment responses of benign urological disease among children and adults. The proposed CHOP/Penn Center for Machine Learning in Urology (CMLU) addresses critical structural and scientific barriers that impede the development of new treatments and the effective application of existing treatments for benign urologic disease across the lifespan. Structurally, urologic research occurs in silos, with little interaction among investigators that study different diseases or different populations (e.g. pediatric and adult). Scientifically, analysis of imaging and other types complex data is limited by inter-observer variability, and incomplete utilization of available information. This proposal overcomes these barriers by applying cutting-edge approaches in machine learning to analyze CT images that are routinely obtained for evaluation of individuals with kidney stone disease. Central to the CHOP/Penn CMLU is the partnership of urologists and experts in machine learning, which will bring a new approach to generating knowledge that advances research and clinical care. In addition, the CMLU will expand the urologic research community by providing a research platform and standalone machine learning executables that could be applied to other datasets. The Center’s mission will be achieved through the following Aims, with progress assessed through systematic evaluation: Aim 1. To expand the research base investigating benign urological disease. We will establish a community with the research base, particularly with the KURe, UroEpi programs, other P20 Centers, and O’Brien Centers. We will build this community by providing mini-coaching clinics to facilitate application of machine learning to individual projects, developing an educational hub for synchronous and asynchronous engagement with the research base, and making freely available all source codes and standalone executables for all machine learning tools. Aim 2. To improve prediction of ureteral stone passage using machine learning of CT images. The CMLU has developed deep learning methods that segment and automate measurement of urinary stones and adjacent renal anatomy. In the Research Project, we will compare these methods to existing segmentation methods and the current gold standard of manual measurement. We will then extract informative features from thousands of CT scans to predict the probability of spontaneous passage of ureteral stones for children and adults evaluated in the CHOP and Penn healthcare systems. Aim 3. To foster collaboration in benign urological disease research across levels of training and centers through an Educational Enrichment Program. We will amplify interactions across institutions and engage investigators locally and nationally by providing summer research internships, and interinstitutional exchange program, and an annual research symposium. PROJECT NARRATIVE The proposed CHOP/Penn O’Brien Center for Machine Learning in Urology addresses critical structural and scientific barriers that impede development of new treatments and the effective application of existing treatments for benign urologic disease across the lifespan. This application overcomes these barriers by applying cutting- edge approaches in machine learning to analyze complex imaging data for individuals with kidney stone disease.The Center’s strategic vision of using machine learning to generate knowledge that improves diagnosis, risk stratification strategies, and prediction of outcomes among children and adults will be achieved through the implementation of a Educational Enrichment Program and a Research Project.",Center for Machine Learning in Urology,10133362,P20DK127488,"['Address', 'Adult', 'Algorithms', 'Anatomy', 'Area', 'Benign', 'Characteristics', 'Child', 'Childhood', 'Clinic', 'Clinical', 'Clinical Investigator', 'Code', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Doctor of Philosophy', 'Educational Status', 'Evaluation', 'Fostering', 'Functional disorder', 'Funding', 'Future', 'Gold', 'Healthcare Systems', 'Image', 'Individual', 'Infrastructure', 'Institution', 'Interdisciplinary Study', 'Internships', 'Interobserver Variability', 'Investigation', 'Kidney', 'Kidney Calculi', 'Knowledge', 'Lead', 'Longevity', 'Machine Learning', 'Manuals', 'Measurement', 'Methods', 'Mission', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Patient Care', 'Pattern', 'Pattern Recognition', 'Pediatric Hospitals', 'Pennsylvania', 'Philadelphia', 'Population', 'Prediction of Response to Therapy', 'Predictive Analytics', 'Probability', 'Publishing', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk stratification', 'Site', 'Source Code', 'Structure', 'Students', 'Techniques', 'United States National Institutes of Health', 'Universities', 'Urinary Calculi', 'Urologic Diseases', 'Urologist', 'Urology', 'Vision', 'Visit', 'X-Ray Computed Tomography', 'base', 'clinical care', 'complex data ', 'deep learning', 'deep neural network', 'design', 'experience', 'feature selection', 'human error', 'improved', 'interdisciplinary collaboration', 'interest', 'learning strategy', 'novel strategies', 'outcome prediction', 'peer', 'programs', 'routine imaging', 'senior faculty', 'skills', 'summer research', 'symposium', 'tool', 'urologic', 'web page']",NIDDK,CHILDREN'S HOSP OF PHILADELPHIA,P20,2020,358890,178185562,-0.004968937132172475
"Lagrangian computational modeling for biomedical data science The goal of the project is to develop a new mathematical and computational modeling framework for from biomedical data extracted from biomedical experiments such as voltages, spectra (e.g. mass, magnetic resonance, impedance, optical absorption, …), microscopy or radiology images, gene expression, and many others. Scientists who are looking to understand relationships between different molecular and cellular measurements are often faced with questions involving deciphering differences between different cell or organ measurements. Current approaches (e.g. feature engineering and classification, end-to-end neural networks) are often viewed as “black boxes,” given their lack of connection to any biological mechanistic effects. The approach we propose builds from the “ground up” an entirely new modeling framework build based on recently developed invertible transformation. As such, it allows for any machine learning model to be represented in original data space, allowing for not only increased accuracy in prediction, but also direct visualization and interpretation. Preliminary data including drug screening, modeling morphological changes in cancer, cardiac image reconstruction, modeling subcellular organization, and others are discussed. Mathematical data analysis algorithms have enabled great advances in technology for building predictive models from biological data which have been useful for learning about cells and organs, as well as for stratifying patient subgroups in different diseases, and other applications. Given their lack to fundamental biophysics properties, the modeling approaches in current existence (e.g. numerical feature engineering, artificial neural networks) have significant short-comings when applied to biological data analysis problems. The project describes a new mathematical data analysis approach, rooted on transport and related phenomena, which is aimed at greatly enhance our ability to extract meaning from diverse biomedical datasets, while augmenting the accuracy of predictions.",Lagrangian computational modeling for biomedical data science,9874005,R01GM130825,"['3-Dimensional', 'Accountability', 'Address', 'Algorithmic Analysis', 'Area', 'Biological', 'Biological Models', 'Biology', 'Biophysics', 'Brain', 'Cancer Detection', 'Cartilage', 'Cell model', 'Cells', 'Classification', 'Collaborations', 'Communication', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Data Analyses', 'Data Reporting', 'Data Science', 'Data Scientist', 'Data Set', 'Development', 'Disease', 'Drug Screening', 'Engineering', 'Flow Cytometry', 'Fluorescence', 'Gene Expression', 'Generations', 'Goals', 'Heart', 'Image', 'Knee', 'Laboratories', 'Learning', 'Letters', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Medical Imaging', 'Methodology', 'Modeling', 'Molecular', 'Morphology', 'Optics', 'Organ', 'Performance', 'Plant Roots', 'Population', 'Pythons', 'Research', 'Scientist', 'Signal Transduction', 'System', 'Techniques', 'Technology', 'Training', 'Universities', 'Virginia', 'Visualization', 'absorption', 'algorithm development', 'artificial neural network', 'base', 'biomedical data science', 'biophysical properties', 'brain morphology', 'cellular imaging', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data space', 'deep learning', 'deep neural network', 'effectiveness testing', 'electric impedance', 'experimental study', 'graphical user interface', 'gray matter', 'heart imaging', 'image reconstruction', 'learning strategy', 'mathematical algorithm', 'mathematical model', 'mathematical theory', 'microscopic imaging', 'models and simulation', 'neural network', 'patient stratification', 'patient subsets', 'predictive modeling', 'radiological imaging', 'technology research and development', 'tool', 'voltage']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2020,360227,169622494,0.003453196574271495
"Interpretable and extendable deep learning model for biological sequence analysis and prediction Project Abstract Bioinformatics and computational biology have become the core of biomedical research. The PI Dr. Dong Xu's work in this area focuses on development of novel computational algorithms, software and information systems, as well as on broad applications of these tools and other informatics resources for diverse biological and medical problems. He works on many research problems in protein structure prediction, post-translational modification prediction, high-throughput biological data analyses, in silico studies of plants, microbes and cancers, biological information systems, and mobile App development for healthcare. He has published more than 300 papers, with about 12,000 citations and H-index of 55. In this project, the PI proposes to develop deep-learning algorithms, tools, web resources for analyses and predictions of biological sequences, including DNA, RNA, and protein sequences. The availability of these data provides emerging opportunities for precision medicine and other areas, while deep learning as a cutting-edge technology in machine learning, presents a new powerful method for analyses and predictions of biological sequences. With rapidly accumulating sequence data and fast development of deep-learning methods, there is an urgent need to systematically investigate how to best apply deep learning in sequence analyses and predictions. For this purpose, the PI will develop cutting-edge deep-learning methods with the following goals for the next five years:  (1) Develop a series of novel deep-learning methods and models to specifically target biological sequence analyses and predictions in: (a) general unsupervised representations of DNA/RNA, protein and SNP/mutation sequences that capture both local and global features for various applications; (b) methods to make deep-learning models interpretable for understanding biological mechanisms and generating hypotheses; (c) “rule learning”, which abstracts the underlying “rules” by combining unsupervised learning of large unlabeled data and supervised learning of small labeled data so that it can classify new unlabeled data.  (2) Apply the proposed deep-learning model to DNA/RNA sequence annotation, genotype-phenotype analyses, cancer mutation analyses, protein function/structure prediction, protein localization prediction, and protein post-translational modification prediction. The PI will exploit particular properties associated with each of these problems to improve the deep-learning models. He will develop a set of related prediction and analysis tools, which will improve the state-of-art performance and shed some light on related biological mechanisms.  (3) Make the data, models, and tools freely accessible to the research community. The system will be designed modular and open-source, available through GitHub. They will be available like integrated circuit modules, which are universal and ready to plug in for different applications. The PI will develop a web resource for biological sequence representations, analyses, and predictions, as well as tutorials to help biologists with no computational knowledge to apply deep learning to their specific research problems. Relevance to Public Health Biological sequences, including DNA, RNA and protein sequences, represent the largest sources of growing big data in current biology and medicine, which provide tremendous opportunities for precision medicine, synthetic biology, and other areas. Deep learning as an emerging machine-learning method has a great potential in utilizing these data in biomedical research. This project will develop and apply cutting-edge deep- learning methods to deliver various sequence-based computational tools for gaining new knowledge, accelerating drug development, and improving personalized diagnosis and treatment.",Interpretable and extendable deep learning model for biological sequence analysis and prediction,9925232,R35GM126985,"['Algorithmic Software', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Big Data', 'Bioinformatics', 'Biological', 'Biological Models', 'Biology', 'Biomedical Research', 'Communities', 'Computational Biology', 'Computational algorithm', 'DNA', 'DNA Sequence', 'Data', 'Data Analyses', 'Development', 'Genotype', 'Goals', 'Healthcare', 'Information Systems', 'Knowledge', 'Label', 'Learning', 'Light', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Medicine', 'Methods', 'Microbe', 'Modeling', 'Mutation', 'Mutation Analysis', 'Paper', 'Performance', 'Phenotype', 'Plants', 'Plug-in', 'Post-Translational Protein Processing', 'Property', 'Proteins', 'Public Health', 'Publishing', 'RNA', 'RNA Sequences', 'Research', 'Resource Informatics', 'Sequence Analysis', 'Series', 'Source', 'System', 'Technology', 'Work', 'computerized tools', 'deep learning', 'deep learning algorithm', 'design', 'drug development', 'improved', 'in silico', 'indexing', 'learning strategy', 'machine learning method', 'mobile application', 'novel', 'online resource', 'open source', 'personalized diagnostics', 'personalized medicine', 'precision medicine', 'protein structure function', 'protein structure prediction', 'software systems', 'supervised learning', 'synthetic biology', 'tool', 'unsupervised learning']",NIGMS,UNIVERSITY OF MISSOURI-COLUMBIA,R35,2020,378183,63611576,0.025952459790816354
"Towards a comprehensive multiscale 3D human interactome network PROJECT SUMMARY/ABSTRACT Almost all proteins function through interacting with other proteins. On average, a protein interacts with ~5 other protein partners in the current human interactome. Therefore, it is of great importance to accurately determine the interface of each interaction, in order to understand how each protein works with different partners to carry out different functions. In our previous Nature Biotechnology study, we implemented a proteome-scale homology modeling approach to generate the first 3D human structural interactome: the interface for each interaction in this network was determined at atomic resolution through co-crystal structures and homology models. Using our 3D interactome, we found that, among >1,800 known disease genes associated with two or more clinically distinctly disorders, pairs of mutations on the same gene but in different interfaces with different partners are significantly more likely to cause distinct diseases. However, only 4,150 human protein interactions have co-crystal structures and 2,921 have high-quality homology models. ~50,000 interactions (87% of the current human interactome) are not amenable to current structural modeling methods. Here, we propose to develop a big-data-driven machine-learning approach integrating biophysiochemical, evolutionary, structural, and population genetic features to identify interaction- specific interfaces for the whole human interactome. Because several key features are unavailable for many proteins and interactions, we propose an innovative approach to use an ensemble of random forest classifiers, named Ensemble Protein Interface Classifier (EPIC), to address this large-scale non-random missing data problem (Aim 1). The high throughput of our massively parallel Clone-seq and INtegrated PrOtein INteractome perTurbation screening (InPOINT) pipeline! uniquely enables us to perform real-time experimental parameter optimization (in Years 2-4 we will clone ~1,500 mutations and examine their impact on ~2,500 interactions every year to iteratively evaluate and refine EPIC; Aim 2). Finally, we will construct a comprehensive multiscale 3D interactome for all known human protein-protein interactions: we will collect/generate atomic- resolution structural models for interactions whenever possible (co-crystal structures and homology models); we will accurately determine interaction-specific interface residues and domains for the whole human interactome. We will deploy an interactive web portal to disseminate our results and allow functional genomic inference in the context of our structural interactome (Aim 3). Our comprehensive multiscale 3D human interactome and the accompanying web portal will greatly reduce the barrier-to-entry for performing systematic structural analysis on a large number of proteins and their interactions, and open the flood gates for such analyses in genomic studies. NARRATIVE Almost all proteins function through interacting with other proteins and the structural details of these interaction interfaces are key in understanding protein function. However, the interfaces for vast majority of human protein interactions are currently unknown. Here, we propose to establish an innovative ensemble classifier approach and implement an unprecedented large-scale computational-experimental iterative learning scheme to predict interfaces for the whole human interactome, in anticipation that our predicted interfaces will help dissect functional sites of disease mutations and be useful for rational drug design to target these sites.",Towards a comprehensive multiscale 3D human interactome network,9963288,R01GM124559,"['3-Dimensional', 'Address', 'Big Data', 'Binding', 'Biotechnology', 'Cells', 'Clinical', 'Crystallization', 'Data', 'Disease', 'Docking', 'Drug Design', 'Evolution', 'Floods', 'Genes', 'Genomics', 'Goals', 'Graph', 'Homology Modeling', 'Human', 'Individual', 'Learning', 'Letters', 'Machine Learning', 'Methods', 'Missense Mutation', 'Modeling', 'Molecular', 'Mutation', 'Names', 'Nature', 'Population Genetics', 'Proteins', 'Proteome', 'Publishing', 'Resolution', 'Resources', 'Scheme', 'Site', 'Structural Models', 'Structure', 'Study models', 'Techniques', 'Testing', 'Time', 'Variant', 'Work', 'biological research', 'functional genomics', 'human interactome', 'improved', 'innovation', 'machine learning method', 'protein function', 'protein protein interaction', 'random forest', 'screening', 'three dimensional structure', 'user-friendly', 'web portal', 'web-accessible']",NIGMS,CORNELL UNIVERSITY,R01,2020,384165,91477866,0.0197534589109463
"Structure prediction and in silico screening of protein-peptide interactions Protein-peptide interactions are prevalent in many cellular processes, such as signal transduction, transcription regulation, and immune response. Peptide-based therapeutics have attracted much attention in recent years, and a significantly growing number of peptide-based medicines have been designed and approved for a variety of diseases. Therefore, studying protein-peptide interactions is of great significance for mechanistic investigation of many biological processes and for peptide therapeutic development. However, because of the difficulties and cost for determining such structures by X-ray crystallography and NMR spectroscopy, currently there are only a limited number of protein-peptide complex structures in the Protein Data Bank. Thus, the ability to predict protein- peptide complex structures will have a far-reaching impact on understanding important biological processes and on designing therapeutic interventions. However, structure prediction for protein-peptide complexes is challenging, particularly due to peptide flexibility. In this project, we will address this challenging issue by innovative integration of bioinformatics and physical modeling approaches. Specifically, we propose to achieve four goals: Goal #1: We will develop novel deep-learning models for protein-peptide structure prediction. Despite successful application of deep learning to protein structure prediction and protein-ligand interaction, deep learning has not been applied to protein-peptide structure prediction yet, due to the flexibility and the resulting large degrees of freedom in peptides. Goal #2: We will develop the first in silico screening method for the search of peptide-based inhibitors, and will construct novel peptide libraries for screening. Our in silico method will be an attractive complement to valuable experimental technologies such as phage display and yeast two-hybrid system for rapid peptide screening at much lower cost. Goal #3: We will convert our computational algorithms into a modular, extensible, open-source software package that can be disseminated to the computational modeling community at no cost. Goal #4. As a proof-of-concept application of our in silico screening method, we will screen for novel peptide leads by targeting β-lactamase to combat antibiotic resistance, in collaboration with my experimental collaborator whose expertise is in molecular biology, biochemistry and microbiology. The ability to predict protein-peptide complex structures will have a far-reaching impact on understanding important biological processes and on designing therapeutic interventions. We propose to tackle the challenge of protein- peptide structure prediction, including developing the first in silico peptide screening method to dramatically save cost and time. We will screen for peptide inhibitors by targeting β-lactamase to combat antibiotic resistance.",Structure prediction and in silico screening of protein-peptide interactions,9932088,R35GM136409,"['Address', 'Antibiotic Resistance', 'Attention', 'Biochemistry', 'Bioinformatics', 'Biological Process', 'Cell physiology', 'Collaborations', 'Communities', 'Complement', 'Complex', 'Computational algorithm', 'Computer Models', 'Computer software', 'Disease', 'Freedom', 'Goals', 'Immune response', 'Investigation', 'Ligands', 'Medicine', 'Methods', 'Microbiology', 'Modeling', 'Molecular Biology', 'NMR Spectroscopy', 'Peptide Library', 'Peptides', 'Phage Display', 'Proteins', 'Signal Transduction', 'Structure', 'Technology', 'Therapeutic', 'Therapeutic Intervention', 'Time', 'Transcriptional Regulation', 'X-Ray Crystallography', 'base', 'beta-Lactamase', 'combat', 'cost', 'data warehouse', 'deep learning', 'design', 'flexibility', 'in silico', 'inhibitor/antagonist', 'innovation', 'novel', 'open source', 'peptide drug', 'peptide structure', 'physical model', 'protein structure prediction', 'screening', 'therapeutic development', 'yeast two hybrid system']",NIGMS,UNIVERSITY OF MISSOURI-COLUMBIA,R35,2020,391207,63611576,0.024556529163714882
"Defining the multi-dimensional code of zinc finger specificity-Resubmission-1 Project Summary The Cys2His2 zinc finger DNA-binding domain is the most common domain in human yet the DNA-binding specificities for the great majority of these proteins remain undefined. Mutations in many of these domains, both with and without known DNA-binding data, have been linked to a host of diseases from Alzheimers (REST) to Cancer (e.g. Slug, WT1, CTCF). Therefore, the characterization of these proteins holds great value. Unfortunately common methodologies used to determine the DNA-binding specificity of transcription factors have failed to address the zinc finger, at least in part because of an inability to fully define the large target specificities required of the average mammalian zinc finger protein. Even when ChIP-Seq data exists it is limited because the size of the genome does not allow us to capture the full binding potential of a factor that could offer a ≥21bp target sequence. As a result, without a comprehensive understanding of a protein’s binding potential, SNPs across the genome will continue to represent potential binding sites that we are unable to predict. In sum, decades of research have enlightened our understanding of this domain but we are still in the dark when it comes to its function as a transcription factors. Recently we have taken an alternative approach to define this domain, demonstrating that a synthetic, one-by-one screen of individual zinc fingers allows us to predict the specificity of multi-fingered proteins with similar or greater accuracy than all prior prediction algorithms. However, this approach fails to take into consideration the influences that adjacent fingers have on one another. We have produced the equivalent of a comprehensive snapshot of what a zinc finger is capable of in just one of many potential contextual environments. Here we propose to scale this approach and screen the zinc finger under an inclusive set of contextual environments. We will consider the most common direct and indirect influences on adjacent finger binding as well as factors that impact the geometry with which the zinc fingers engage the DNA. We will use these results to provide a complete picture of how adjacent zinc fingers determine their specificity and by scaffolding these two-fingered models, predict and design the specificity of large, multi-fingered proteins. In this way, we will define a multi-dimensional code of zinc finger specificity that allows us to predict all zinc finger DNA-binding specificities, how any neighbor finger context would modify this specificity, and the factors that result in adjacent finger incompatibility and loss of DNA-binding function. We will apply this model to predict the specificity of all human zinc finger proteins, validate these predictions through in vivo characterization of an informed set of transcription factors, and test predicted mechanisms of multi-fingered binding with designer, artificial factors. Project Narrative The proposed research is relevant to public health because the ZF domain is the most common in human yet it remains largely uncharacterized. A holistic understanding of ZF function will provide insight into how ZF mutations are related to disease and allow us to predict harmful binding sites due to SNPs across the genome.",Defining the multi-dimensional code of zinc finger specificity-Resubmission-1,9849781,R01GM118851,"['Achievement', 'Address', 'Alzheimer&apos', 's Disease', 'Amino Acids', 'Binding', 'Binding Proteins', 'Binding Sites', 'ChIP-seq', 'Charge', 'Code', 'Collection', 'Communities', 'Comprehension', 'DNA', 'DNA Binding', 'DNA Binding Domain', 'Data', 'Disease', 'Distal', 'Environment', 'Exposure to', 'Fingers', 'Genetic Transcription', 'Genome', 'Geometry', 'Goals', 'Human', 'Hybrids', 'Individual', 'Lead', 'Libraries', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Mammals', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Nucleotides', 'Proteins', 'Public Health', 'Reporter', 'Research', 'Sampling', 'Schizophrenia', 'Series', 'Specificity', 'Structure', 'Sum', 'System', 'Systems Biology', 'Testing', 'WT1 gene', 'Work', 'Zinc Fingers', 'base', 'design', 'exhaustion', 'experimental study', 'in vivo', 'insight', 'loss of function', 'model design', 'prediction algorithm', 'predictive modeling', 'predictive test', 'scaffold', 'screening', 'slug', 'transcription factor', 'user-friendly', 'web site']",NIGMS,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,R01,2020,393339,329565273,0.003535696619737962
"In-silico prediction of protein-peptide interactions. IN-SILICO PREDICTION OF PROTEIN-PEPTIDE INTERACTIONS Automated docking methods are used extensively for gaining a mechanistic understanding of the molecular interactions underpinning cellular processes. While these tools work well for small molecules they perform poorly for peptides and cannot handle Intrinsically Disordered Proteins (IDPs) which play very important roles in these processes. The goal of this project is the development of an efficient and practical peptide docking software, useful for designing therapeutic peptides and gaining insight into IDPs binding ordered proteins. The proposed software supports biomedical applications ranging from investigating chemical pathways to designing and optimizing therapeutic molecules for diseases such as cancer and metabolic disorders. Under the previous award we developed and released a new method for docking fully-flexible peptides with up to 20 standard amino acids: AutoDock CrankPep (ADCP). We showed that it outperforms current state-of-the-art docking methods. For the next award, we propose to: 1) further develop ADCP to support docking IPDs with up to 70 amino acids and improve support for therapeutic peptides containing modified amino acids and complex macrocycles; 2) develop peptide-specific scoring functions to increase docking success rates and methods for predicting the free energy of binding of peptides. This will be done by exploiting the latest advances in statistical potentials for docking, as well as applying machine-learning techniques; 3) test and validate the software on our datasets, community benchmarks, and through our collaborations with outstanding biologists working on biomedical applications spanning from designing drugs for thrombosis and influenza, to modeling IDPs interacting with globular proteins; and 4) document the software and release it under an open source license on a regular basis along with datasets we compile and update on regularly. The proposed research will occur in the context of collaborations with experimental biologists working on highly relevant biomedical projects and providing experimental feedback and validation. In addition, this project will benefit from various collaborations with experts in the fields of computational biology, applied mathematics and artificial intelligence. This docking software tool will be developed by applying best practices in software engineering and be implemented as a modular, extensible, component-based software framework for peptide docking. This docking engine will be part of the widely used AutoDock software suite. The ability to model complexes formed by proteins and fully-flexible peptides or IDPs is in high demand and will greatly extend the range of peptide-based therapeutic approaches for which automated docking can be successfully applied. It will also support gaining insights into interactions of IDPs with proteins. As such, it will impact the research of many medicinal chemists and biologist and extend the use of computational tools to a wider community of scientists, thereby supporting the advancement of biomedical research. Automated docking is a workhorse for rational drug design, however, applying these methods to peptides has remained challenging, thus impeding the designing of therapeutic peptides and the study of Intrinsically Disordered Proteins (IDP) binding to their ordered partners. During the prior funding period, we made substantial progress toward peptide docking, resulting in a new docking engine: AutoDock CrankPep, which outperforms state-of-the-art docking methods for linear and cyclic peptides with up to 20 standard amino acids. We propose to further develop AutoDock CrankPep to support docking of therapeutic peptides with modified amino acids as well as IDPs with up to 70 amino acids, creating a practical docking tool for peptides that will impact the research of many computational and medicinal chemists and biologist, contribute to our understanding of biological processes, and significantly advance biomedical research.",In-silico prediction of protein-peptide interactions.,10116950,R01GM096888,"['Amino Acids', 'Area', 'Artificial Intelligence', 'Automobile Driving', 'Award', 'Benchmarking', 'Binding', 'Binding Proteins', 'Biological', 'Biological Availability', 'Biological Process', 'Biomedical Research', 'Cell physiology', 'Cells', 'Chemicals', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Computer software', 'Cyclic Peptides', 'Data Set', 'Development', 'Disease', 'Docking', 'Documentation', 'Drug Design', 'Educational workshop', 'Feedback', 'Free Energy', 'Funding', 'Goals', 'Half-Life', 'Influenza', 'Insulin', 'Libraries', 'Licensing', 'Ligands', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Mediating', 'Metabolic Diseases', 'Methods', 'Modeling', 'Modernization', 'Mutate', 'Pathway interactions', 'Peptides', 'Performance', 'Peripheral', 'Permeability', 'Pharmaceutical Preparations', 'Play', 'Process', 'Production', 'Property', 'Proteins', 'Renaissance', 'Research', 'Role', 'Scientist', 'Signal Pathway', 'Software Engineering', 'Software Framework', 'Software Tools', 'Specificity', 'Structure', 'Study models', 'Techniques', 'Testing', 'Therapeutic', 'Thrombosis', 'Toxic effect', 'Training', 'Update', 'Validation', 'Work', 'base', 'combinatorial', 'computerized tools', 'computing resources', 'design', 'flexibility', 'globular protein', 'graphical user interface', 'improved', 'improved functioning', 'in silico', 'insight', 'interest', 'interoperability', 'novel', 'open source', 'peptide drug', 'predictive tools', 'programs', 'protein protein interaction', 'receptor', 'screening', 'small molecule', 'success', 'symposium', 'therapeutic target', 'tool', 'translational study', 'virtual screening']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2020,399375,176988430,0.02089043627101302
"Automated Phenotyping in Epilepsy There are 65 million people worldwide with epilepsy and 150,000 new cases of epilepsy are diagnosed in the US annually. However, treatment options for epilepsy remain inadequate, with many patients suffering from treatment-resistant seizures, cognitive comorbidities and the negative side effects of treatment. A major obstacle to progress towards the development of new therapies is the fact that preclinical epilepsy research typically requires labor-intensive and expensive 24/7 video-EEG monitoring of seizures that rests on the subjective scoring of seizure phenotypes by human observers (as exemplified by the widely used Racine scale of behavioral seizures). Recently, the Datta lab showed that complex animal behaviors are structured in stereotyped modules (“syllables”) at sub-second timescales and arranged according to specific rules (“grammar”). These syllables can be detected without observer bias using a method called motion sequencing (MoSeq) that employs video imaging with a 3D camera combined with artificial intelligence (AI)-assisted video analysis to characterize behavior. Through collaboration between the Soltesz and Datta labs, exciting data were obtained that demonstrated that MoSeq can be adapted for epilepsy research to perform objective, inexpensive and automated phenotyping of mice in a mouse model of chronic temporal lobe epilepsy. Here we propose to test and improve MoSeq further to address long-standing, fundamental challenges in epilepsy research. This includes the development of an objective alternative to the Racine scale, testing of MoSeq as an automated anti-epileptic drug (AED) screening method, and the development of human observer- independent behavioral biomarkers for seizures, epileptogenesis, and cognitive comorbidities. In addition, we plan to dramatically extend the epilepsy-related capabilities of MoSeq to include the automated tracking of finer-scale body parts (e.g., forelimb and facial clonus) that are not possible with the current approach. Finally, we propose to develop the analysis pipeline for MoSeq into a form that is intuitive, inexpensive, user-friendly and thus easily sharable with the research community. We anticipate that these results will have a potentially transformative effect on the field by demonstrating the feasibility and power of automated, objective, user- independent, inexpensive analysis of both acquired and genetic epilepsy phenotypes. There is an urgent need for new therapies for patients with uncontrolled epilepsy. The project will develop breakthrough technologies involving artificial intelligence (AI)-assisted analysis of 3-dimensional video data of mouse behavior to address long-standing, fundamental challenges in preclinical epilepsy research. If successful, this innovative approach is expected to have a significant and sustained impact on epilepsy research by enabling investigators to perform objective, automated, inexpensive, reproducible assessment of epilepsy in experimental animals to aid the testing of anti-seizure drugs and other novel therapies.",Automated Phenotyping in Epilepsy,10024094,R01NS114020,"['3-Dimensional', 'Address', 'Animal Behavior', 'Animal Model', 'Animals', 'Antiepileptic Agents', 'Artificial Intelligence', 'Behavior', 'Behavioral', 'Biological Markers', 'Body part', 'Cannabidiol', 'Carbamazepine', 'Cells', 'Chronic', 'Clonazepam', 'Clonus', 'Cognitive', 'Collaborations', 'Communities', 'Complex', 'Data', 'Development', 'Diagnosis', 'Drug Screening', 'Electroencephalography', 'Epilepsy', 'Epileptogenesis', 'Face', 'Forelimb', 'Future', 'Genetic', 'Genetic Models', 'Head', 'Hippocampus (Brain)', 'Human', 'Image', 'Imaging Techniques', 'Intervention', 'Intuition', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Mus', 'Observer Variation', 'Patients', 'Pharmaceutical Preparations', 'Phenotype', 'Phenytoin', 'Pilocarpine', 'Probability', 'Reproducibility', 'Research', 'Research Personnel', 'Resistance', 'Resolution', 'Rest', 'Seizures', 'Specificity', 'Stereotyping', 'Structure', 'Tail', 'Technology', 'Temporal Lobe Epilepsy', 'Testing', 'Three-dimensional analysis', 'Time', 'Treatment Side Effects', 'Valproic Acid', 'analysis pipeline', 'base', 'behavior test', 'behavioral phenotyping', 'comorbidity', 'drug use screening', 'improved', 'innovation', 'kainate', 'method development', 'mouse model', 'novel therapeutics', 'optogenetics', 'pre-clinical', 'pre-clinical research', 'selective expression', 'user-friendly']",NINDS,STANFORD UNIVERSITY,R01,2020,422740,560644462,-5.320053283198475e-05
"Advanced approaches to protein structure prediction Abstract The success of genome sequencing over the last four decades has resulted in a rapidly increasing gap between the number of known protein sequences and the number of known protein structures and functions. Because protein sequence on its own cannot tell us what each molecule does in cells, the large-scale absence of protein structure and function information severely hinders the progress of contemporary biological and medical studies. These gaps in understanding strongly call for efficient computational approaches for automated, yet highly accurate protein structure prediction and function annotation. The PI’s lab has a successful track record in developing and disseminating high-quality structural bioinformatics methods which have been widely used by the global community. In this project, the lab seeks to develop new advanced methods for both tertiary and quaternary protein structure prediction. Built on the tools and databases previously developed in the PI’s lab, new deep neural-network based techniques will be extended to residue-level intra- and inter-chain contact- and distance-map predictions. These predictions will then be used to constrain the conformational searching space of threading-based fragment assembly simulations, with the aim to significantly improve the accuracy and success rate of structure modeling of monomeric proteins and protein-protein interactions (PPIs), especially for the difficult targets that lack homologous templates in the Protein Data Bank. Next, the structure and PPI network information will be used to help elucidate multiple levels of biological and biomedical functions for protein molecules, including mutation-induced changes in protein stability and human disease predictions. The long- term goals of this project are to significantly improve the state of the art of protein structure prediction and to narrow the gap between the abundance of protein sequence information and the dearth of protein structure and function data, thus significantly enhancing the usefulness and impact of structural bioinformatics. Success in this project will also help reveal the general principles governing the fundamental relations across sequence, structure and function of protein molecules. Relevance Researchers in contemporary drug industry need to use the knowledge of 3-dimensional structure of protein molecules for designing synthetic compounds to fight against human diseases. But many pharmaceutically important proteins do not have experimentally solved structures. This project seeks to develop advanced computer methods for high-quality protein structure prediction that can be used to function annotation and compound screening; these should have an important and general impact on drug discovery and human health.",Advanced approaches to protein structure prediction,9932039,R35GM136422,"['ART protein', 'Amino Acid Sequence', 'Bioinformatics', 'Biological', 'Cells', 'Communities', 'Computing Methodologies', 'Data', 'Databases', 'Drug Industry', 'Goals', 'Health', 'Human', 'Induced Mutation', 'Information Networks', 'Knowledge', 'Maps', 'Medical', 'Methods', 'Molecular Conformation', 'Network-based', 'Pharmacologic Substance', 'Proteins', 'Quaternary Protein Structure', 'Research Personnel', 'Structural Models', 'Structural Protein', 'Structure', 'Techniques', 'base', 'data warehouse', 'deep neural network', 'design', 'drug discovery', 'fight against', 'genome sequencing', 'human disease', 'improved', 'protein function', 'protein protein interaction', 'protein structure function', 'protein structure prediction', 'screening', 'simulation', 'success', 'three dimensional structure', 'tool']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R35,2020,431776,641965656,0.024024483266580392
"Designing neutralization antibodies against Sars-Cov-2 Project Summary COVID-19 has become a worldwide pandemic whose rapid spread and mortality rate threatens millions of lives and the global economic system. Developing effective treatment such as neutralization antibodies is an urgent need. We propose here to develop a new method to design antibodies strongly bind to the SARS-CoV-2 receptor binding domain (RBD) that is necessary for viral entrance to human cells. We will develop a novel approach that combines directed evolution, deep sequencing and interpretable neural network models to efficiently identify strong and specific antibodies. This method will allow analyzing large sequencing data sets of antibody variants against the SARS-CoV-2 RBD in order to derive superior binders that do not exist in the original library. Iteration through directed evolution and computational design will efficiently identify neutralization antibody candidates that can be used as potent therapeutics to treat COVID-19. Narrative: Developing neutralization antibodies is critical to provide effective treatment for Covid-19.",Designing neutralization antibodies against Sars-Cov-2,10173204,R21AI158114,"['2019-nCoV', 'Affinity', 'Amino Acids', 'Antibodies', 'Binding', 'COVID-19', 'Cells', 'Cessation of life', 'Clinical Trials', 'Consumption', 'Data', 'Data Set', 'Development', 'Directed Molecular Evolution', 'Economics', 'Epitopes', 'Future', 'Gene Library', 'Histones', 'Human', 'Human Engineering', 'Immunoglobulin G', 'Lead', 'Libraries', 'Machine Learning', 'Methods', 'Modeling', 'Monoclonal Antibodies', 'Mutate', 'Mutation', 'Nature', 'Network-based', 'Neural Network Simulation', 'Peptides', 'Positioning Attribute', 'Process', 'Reporting', 'Resistance', 'Screening procedure', 'Solubility', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Variant', 'Viral', 'Virus', 'Virus Diseases', 'base', 'clinical efficacy', 'data archive', 'deep learning', 'deep sequencing', 'design', 'drug candidate', 'effective therapy', 'machine learning method', 'mortality', 'mutant', 'neural network', 'neutralizing antibody', 'novel strategies', 'pandemic disease', 'receptor binding', 'screening', 'trend']",NIAID,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2020,433750,524978793,-0.004227482771852919
"Next generation machine vision for automated behavioral phenotyping of knock-in ALS-FTD mouse models Project Summary Amyotrophic lateral sclerosis (ALS) and Frontotemporal Dementia FTD are devastating neurodegenerative disorders that lie on a genetic and mechanistic continuum. ALS is a disease of motor neurons that that is almost uniformly lethal within only 3-5 years of diagnosis. FTD is a heterogeneous, rapidly progressing syndrome that is among the top three causes of presenile dementia. About 10% of ALS cases are caused by dominantly transmitted gene defects. SOD1 and FUS mutations cause aggressive motor neuron pathology while TDP43 mutations cause ALS-FTD. Further, wild type FUS and TDP43 are components of abnormal inclusions in many FTD cases, suggesting a mechanistic link between these disorders. Early phenotypes are of particular interest because these could lead to targeted interventions aimed at the root cause of the disorder that could stem the currently inexorable disease progression. Elucidating such early, potentially shared characteristics of these disorders should be greatly aided by: 1) knock-in animal models expressing familial ALS-FTD genes; 2) sensitive, rigorous and objective behavioral phenotyping methods to analyze and compare models generated in different laboratories. In published work the co-PIs applied their first-generation, machine vision-based automated phenotyping method, ACBM ‘1.0’ (automated continuous behavioral monitoring) to detect and quantify the earliest-observed phenotypes in Tdp43Q331K knock-in mice. This method entails continuous video recording for 5 days to generate >14 million frames/mouse. These videos are then scored by a trained computer vision system. In addition to its sensitivity, objectivity and reproducibility, a major advantage of this method is the ability to acquire and archive video recordings and to analyze the data at sites, including the Cloud, remote from those of acquisition. We will use Google Cloud TPUs supercomputers that have been designed from the ground up to accelerate cutting-edge machine learning workloads, with a special focus on deep learning. We will analyze this data using Bayesian hierarchical spline models that describe the different mouse behaviors along the circadian rhythm. The current proposal has two main goals: 1) Use deep learning to refine and apply a Next Generation ACBM - ‘2.0’ - that will allow for more sensitive, expansive and robust automated behavioral phenotyping of four novel knock-in models along with the well characterized SOD1G93A transgenic mouse. 2) To establish and validate procedures to enable remote acquisition of video recording data with cloud-based analysis. Our vision is to establish sensitive, robust, objective, and open-source machine vision-based behavioral analysis tools that will be widely available to researchers in the field. Since all the computer-annotated video data is standardized in ACBM 2.0 and will be archived, we envision a searchable ‘behavioral database’, that can be freely mined and analyzed. Such tools are critical to accelerate the development of novel and effective therapeutics for ALS-FTD. Narrative ALS and Frontotemporal Dementia (FTD) are devastating, rapidly progressing diseases and current treatments are of limited value. In this proposal a neuroscientist and a computer scientist have teamed up to develop a new machine vision-based method for behavioral analysis novel mouse models of ALS-FTD. The ultimate goal is to reveal early phenotypes in ALS-FTD models that can be used in understanding disease pathology and in the development of new therapeutic targets.",Next generation machine vision for automated behavioral phenotyping of knock-in ALS-FTD mouse models,9979408,R21NS112743,"['Amyotrophic Lateral Sclerosis', 'Animal Model', 'Archives', 'Behavior', 'Behavior monitoring', 'Behavioral', 'Characteristics', 'Circadian Rhythms', 'Computer Vision Systems', 'Computers', 'Data', 'Data Set', 'Databases', 'Defect', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Expression Profiling', 'Familial Amyotrophic Lateral Sclerosis', 'Frontotemporal Dementia', 'Gene Expression', 'Generations', 'Genes', 'Genetic', 'Goals', 'Hour', 'Human', 'Intervention', 'Knock-in', 'Knock-in Mouse', 'Laboratories', 'Lead', 'Link', 'Machine Learning', 'Methods', 'Modeling', 'Motor Neuron Disease', 'Motor Neurons', 'Mus', 'Mutation', 'Neurodegenerative Disorders', 'Paralysed', 'Pathology', 'Phenotype', 'Plant Roots', 'Presenile Dementia', 'Procedures', 'Publishing', 'Reproducibility', 'Research', 'Research Personnel', 'Respiratory Paralysis', 'Scientist', 'Site', 'Standardization', 'Syndrome', 'TensorFlow', 'Time', 'Training', 'Transgenic Mice', 'Transgenic Organisms', 'Treatment Efficacy', 'Video Recording', 'Vision', 'Work', 'Workload', 'base', 'behavioral phenotyping', 'cloud based', 'data archive', 'deep learning', 'design', 'frontotemporal lobar dementia-amyotrophic lateral sclerosis', 'interest', 'knockin animal', 'machine vision', 'mouse model', 'new therapeutic target', 'next generation', 'novel', 'open source', 'programs', 'protein TDP-43', 'stem', 'supercomputer', 'superoxide dismutase 1', 'tool']",NINDS,BROWN UNIVERSITY,R21,2020,446875,127562714,-0.006044568514636388
"Novel Statistical Inference for Biomedical Big Data Project Summary This project develops novel statistical inference procedures for biomedical big data (BBD), including data from diverse omics platforms, various medical imaging technologies and electronic health records. Statistical inference, i.e., assess- ing uncertainty, statistical signiﬁcance and conﬁdence, is a key step in computational pipelines that aim to discover new disease mechanisms and develop effective treatments using BBD. However, the development of statistical inference procedures for BBD has lagged behind technological advances. In fact, while point estimation and variable selection procedures for BBD have matured over the past two decades, existing inference procedures are either limited to simple methods for marginal inference and/or lack the ability to integrate biomedical data across multiple studies and plat- forms. This paucity is, in large part, due to the challenges of statistical inference in high-dimensional models, where the number of features is considerably larger than the number of subjects in the study. Motivated by our team's extensive and complementary expertise in analyzing multi-omics data from heterogenous studies, including the TOPMed project on which multiple team members currently collaborate, the current proposal aims to address these challenges. The ﬁrst aim of the project develops a novel inference procedure for conditional parameters in high-dimensional models based on dimension reduction, which facilitates seamless integration of external biological information, as well as biomedical data across multiple studies and platforms. To expand the application of this method to very high-dimensional models that arise in BBD applications, the second aim develops a data-adaptive screening procedure for selecting an optimal subset of relevant variables. The third aim develops a novel inference procedure for high-dimensional mixed linear models. This method expands the application domain of high-dimensional inference procedures to studies with longitu- dinal data and repeated measures, which arise commonly in biomedical applications. The fourth aim develops a novel data-driven procedure for controlling the false discovery rate (FDR), which facilitates the integration of evidence from multiple BBD sources, while minimizing the false negative rate (FNR) for optimal discovery. Upon evaluation using ex- tensive simulation experiments and application to multi-omics data from the TOPMed project, the last aim implements the proposed methods into easy-to-use open-source software tools leveraging the R programming language and the capabilities of the Galaxy workﬂow system, thus providing an expandable platform for further developments for BBD methods and tools. Public Health Relevance Biomedical big data (BBD), including large collections of omics data, medical imaging data, and electronic health records, offer unprecedented opportunities for discovering disease mechanisms and developing effective treatments. However, despite their tremendous potential, discovery using BBD has been hindered by computational challenges, including limited advances in statistical inference procedures that allow biomedical researchers to investigate uncon- founded associations among biomarkers of interest and various biological phenotypes, while integrating data from multiple BBD sources. The current proposal bridges this gap by developing novel statistical machine learning methods and easy-to-use open-source software for statistical inference in BBD, which are designed to facilitate the integration of data from multiple studies and platforms.",Novel Statistical Inference for Biomedical Big Data,9969887,R01GM133848,"['Address', 'Adoption', 'Behavioral', 'Big Data Methods', 'Biological', 'Biological Assay', 'Biological Markers', 'Code', 'Collection', 'Communities', 'Computer software', 'Data', 'Data Sources', 'Development', 'Dimensions', 'Disease', 'Electronic Health Record', 'Evaluation', 'Fostering', 'Galaxy', 'Genetic study', 'Goals', 'Heart', 'Imaging technology', 'Individual', 'Linear Models', 'Measurement', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular', 'Multiomic Data', 'Outcome', 'Phenotype', 'Procedures', 'R programming language ', 'Research Personnel', 'Sample Size', 'Scientist', 'Screening procedure', 'Software Tools', 'Structure', 'System', 'Testing', 'Trans-Omics for Precision Medicine', 'Uncertainty', 'Work', 'base', 'big biomedical data', 'computational pipelines', 'data integration', 'design', 'diverse data', 'effective therapy', 'experimental study', 'heterogenous data', 'high dimensionality', 'interest', 'machine learning method', 'member', 'novel', 'open source', 'public health relevance', 'screening', 'simulation', 'statistical and machine learning', 'structured data', 'tool', 'treatment strategy', 'user friendly software']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,456980,533302350,0.006084804866881252
"Scientific Questions: A New Target for Biomedical NLP Project Summary  Natural language processing (NLP) technology is now widespread (e.g. Google Translate) and has several important applications in biomedical research. We propose a new target for NLP: extraction of scientific questions stated in publications. A system that automatically captures and organizes scientific questions from across the biomedical literature could have a wide range of significant impacts, as attested to in our diverse collection of support letters from researchers, journal editors, educators and scientific foundations. Prior work focused on making binary (or probabilistic) assessments of whether a text is hedged or uncertain, with the goal of downgrading such statements in information extraction tasks—not computationally capturing what the uncertainty is about. In contrast, we propose an ambitious plan to identify, represent, integrate and reason about the content of scientific questions, and to demonstrate how this approach can be used to address two important new use cases in biomedical research: contextualizing experimental results and enhancing literature awareness. Contextualizing results is the task of linking elements of genome-scale results to open questions across all of biomedical research. Literature awareness is the ability to understand important characteristics of large, dynamic collections of research publications as a whole. We propose to produce rich computational representations of the dynamic evolution of research questions, and to prototype textual and visual interfaces to help students and researchers explore and develop a detailed understanding of key open scientific questions in any area of biomedical research. Project Narrative The scientific literature is full of statements of important unsolved questions. By using artificial intelligence systems to identify and categorize these questions, the proposed work would help other researchers discover when their findings might address an important question in another scientific area. This work would also make it easier for students, journal editors, conference organizers and others understand where science is headed by tracking the evolution of scientific questions.",Scientific Questions: A New Target for Biomedical NLP,10069773,R01LM013400,"['Address', 'Area', 'Artificial Intelligence', 'Awareness', 'Biomedical Research', 'Characteristics', 'Collection', 'Computerized Patient Records', 'Cues', 'Data', 'Elements', 'Environment', 'Evolution', 'Expert Systems', 'Foundations', 'Genes', 'Goals', 'Gold', 'Information Retrieval', 'Journals', 'Letters', 'Link', 'Literature', 'Manuals', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Pathway interactions', 'Performance', 'Phenotype', 'Proteomics', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Semantics', 'Services', 'Signal Transduction', 'Source', 'Students', 'System', 'Taxonomy', 'Technology', 'Text', 'Time', 'Translating', 'Uncertainty', 'Update', 'Visual', 'Work', 'design', 'dynamical evolution', 'experimental study', 'genome wide association study', 'genome-wide', 'graduate student', 'high throughput screening', 'innovation', 'journal article', 'news', 'novel', 'pharmacovigilance', 'prototype', 'symposium', 'text searching', 'tool', 'transcriptome sequencing', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2020,462393,292134808,0.008914543274848375
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9979659,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'MeSH Thesaurus', 'Measures', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'deep learning algorithm', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'large scale data', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'public repository', 'specific biomarkers']",NLM,UNIVERSITY OF CENTRAL FLORIDA,U01,2020,467177,16827427,0.006598087424920724
"Advancing Protein Engineering Using Artificial Intelligence and the ProtaBank Mutation Database PROJECT SUMMARY Therapeutic antibodies, specialized enzymes for drug manufacturing, small molecule drug screening agents, and other proteins have been instrumental in advancing biotechnology and medicine. Protein therapeutics alone represents a rapidly growing $100+ billion market with broad applications in the treatment of cancer, inflammatory and metabolic diseases, and numerous other disorders. Most of the antibodies and other protein therapeutics developed in the last several years have been engineered, leading to improvements in important properties such as efficacy, binding affinity, expression, stability, and immunogenicity. However, improving protein properties through sequence modification remains a challenging task. Artificial intelligence (AI), which has been enormously successful in several fields (e.g., image recognition, self-driving cars, natural language processing), is now being applied to protein engineering and has the potential to transform this field as well. AI and machine learning (ML) can take advantage of large and diverse datasets to identify correlations, predict beneficial mutations, and explore novel protein sequences in ways that are not possible using other techniques. Other advantages include the ability to simultaneously optimize multiple protein properties and explore sequence space more efficiently. In Phases I and II of this project, we developed the ProtaBank database as a central repository to store, organize, and annotate protein mutation data spanning a broad range of properties. ProtaBank is the largest and only database actively collecting such a comprehensive set of sequence mutation data and is growing rapidly due to the wealth of data being generated with advanced automation and next-generation sequencing techniques. ProtaBank's depth and breadth makes it an ideal data source to train ML models. This proposal aims to create the ProtaBank AI Platform to enable the use of AI and ML tools to apply the data in ProtaBank to engineer proteins. The platform will provide fully customizable computational tools and will invoke protein-specific knowledge to properly prepare data for use with ML models. An interface to popular ML frameworks will be provided so that scientists can use these techniques to discover new predictive algorithms and enhance their ability to design proteins with the desired properties. Specific aims include: (1) integrating peer validated ML methods and proprietary technology for protein engineering into the ProtaBank AI Platform, (2) developing dynamic ML dataset creation tools, (3) expanding and improving the ProtaBank database by reaching out to scientists to contribute data, (4) enhancing our data deposition tools, and (5) integrating ProtaBank with the Protein Data Bank structure database and other databases. ! Project Narrative Protein engineering has enabled significant advances in health care by playing a key role in the development of antibodies and other protein therapeutics (e.g., for the treatment of cancer, inflammatory and metabolic diseases, and other disorders), highly selective enzymes for drug manufacturing, and novel proteins for use in diagnostics and the identification of new small molecule drugs. This project will enable the power of artificial intelligence (AI) to be applied to accelerate the engineering of proteins with new and improved properties. AI approaches can capitalize on the large amounts of protein mutation data being generated and stored in our recently developed ProtaBank protein mutation database to transform the way in which protein therapeutics and reagents are discovered and developed.!",Advancing Protein Engineering Using Artificial Intelligence and the ProtaBank Mutation Database,9994932,R44GM117961,"['Affinity', 'Amino Acid Sequence', 'Antibodies', 'Artificial Intelligence', 'Automation', 'Automobile Driving', 'Base Sequence', 'Binding', 'Biological Assay', 'Biotechnology', 'Collection', 'Communities', 'Coupled', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Deposition', 'Development', 'Diagnostic', 'Disease', 'Drug Screening', 'Engineering', 'Enzymes', 'Generations', 'Healthcare', 'Image', 'Inflammatory', 'Knowledge', 'Link', 'Machine Learning', 'Medicine', 'Metabolic Diseases', 'Meteor', 'Modeling', 'Modification', 'Mutation', 'Natural Language Processing', 'Pharmaceutical Preparations', 'Phase', 'Play', 'Preparation', 'Property', 'Protein Engineering', 'Proteins', 'Protocols documentation', 'Reagent', 'Research Personnel', 'Sampling', 'Scientist', 'Standardization', 'Structure', 'Techniques', 'Technology', 'Therapeutic antibodies', 'Training', 'base', 'cancer therapy', 'computerized tools', 'data submission', 'data warehouse', 'database structure', 'deep learning', 'design', 'flexibility', 'immunogenicity', 'improved', 'insight', 'learning strategy', 'machine learning method', 'multitask', 'next generation sequencing', 'novel', 'peer', 'prediction algorithm', 'protein structure', 'repository', 'small molecule', 'therapeutic protein', 'tool']",NIGMS,"PROTABIT, LLC",R44,2020,495160,495160,0.01681773266472744
"Predicting Human Olfactory Perception from Molecular Structure PROJECT SUMMARY Modern technology makes it possible to capture a visual scene as a photograph, alter it, send it to another country nearly instantaneously, and store it without concern for degradation. None of this is currently possible in olfaction. Although perfumers and flavorists are adept at mixing odorous molecules to produce a desired perceptual effect, the rules underlying this process are poorly understood at a quantitative level. Current methods for displaying odors to a subject are akin to requiring a Polaroid of every visual stimulus of interest. A more efficient method for probing the olfactory system would be to use a set of 'primary odors'—some limited number of odors from which all other complex odors could be reproduced by appropriate mixtures. Both auditory and visual stimuli have been digitized, and this will eventually be possible in olfaction as well. Predicting odor from chemical structure has been a problem in the field since its inception, but recent advances in machine learning algorithms have made great progress in analogous problems, such as facial recognition. The research proposed here will combine these machine learning techniques with high quality human psychophysics to understand how to predict the smell of a molecule or mixture of odorants, which will ultimately help improve our understanding of disease diagnosis using odors as well as eating-related health and illness. HEALTH RELEVANCE The sense of smell plays a critical role in preferences and aversions for specific foods. The proposed research will combine machine learning techniques with high quality human psychophysics to create a model that can predict the smell of odorous molecules. This model will allow us to describe and control odors, which will increase our understanding of food preference and eating-related health and wellness.",Predicting Human Olfactory Perception from Molecular Structure,9887973,R01DC017757,"['Algorithms', 'Characteristics', 'Chemical Structure', 'Chemicals', 'Code', 'Collection', 'Color', 'Communities', 'Complex', 'Complex Mixtures', 'Country', 'Data', 'Data Set', 'Descriptor', 'Detection', 'Development', 'Eating', 'Enrollment', 'Face', 'Food', 'Food Preferences', 'Frequencies', 'Health', 'Human', 'Ligands', 'Machine Learning', 'Mass Fragmentography', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modernization', 'Molecular Structure', 'Neurosciences', 'Non-linear Models', 'Numerical value', 'Odors', 'Olfactory Pathways', 'Perception', 'Play', 'Process', 'Psychophysics', 'Research', 'Research Personnel', 'Role', 'Smell Perception', 'Stimulus', 'Techniques', 'Technology', 'Training', 'Translating', 'Vision', 'Visual', 'Vocabulary', 'Work', 'auditory stimulus', 'base', 'computer monitor', 'disease diagnosis', 'experience', 'high dimensionality', 'improved', 'in silico', 'interest', 'machine learning algorithm', 'member', 'novel', 'physical property', 'predictive modeling', 'predictive test', 'preference', 'receptor', 'relating to nervous system', 'single molecule', 'visual stimulus']",NIDCD,MONELL CHEMICAL SENSES CENTER,R01,2020,496911,6406933,-0.011459454946718626
"Integrative Predictors of Temporomandibular Osteoarthritis ABSTRACT This application proposes the development of efficient web-based data management, mining, and analytics, to integrate and analyze clinical, biological, and high dimensional imaging data from TMJ OA patients. Based on our published results, we hypothesize that patterns of condylar bone structure, clinical symptoms, and biological mediators are unrecognized indicators of the severity of progression of TMJ OA. Efficiently capturing, curating, managing, integrating and analyzing this data in a manner that maximizes its value and accessibility is critical for the scientific advances and benefits that such comprehensive TMJ OA patient information may enable. High dimensional databases are increasingly difficult to process using on-hand database management tools or traditional processing applications, creating a continuing demand for innovative approaches. Toward this end, the DCBIA at the Univ. of Michigan has partnered with the University of North Carolina, the University of Texas MD Anderson Cancer Center and Kitware Inc. Through high-dimensional quantitative characterization of individuals with TMJ OA, at molecular, clinical and imaging levels, we will identify phenotypes at risk for more severe prognosis, as well as targets for future therapies. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA. Due to its ubiquitous design in the web, DSCI software installation will no longer be required. Our long-term goal is to create software and data repository for Osteoarthritis of the TMJ. Such repository requires maintaining the data in a distributed computational environment to allow contributions to the database from multi-clinical centers and to share trained models for TMJ classification. In years 4 and 5 of the proposed work, the dissemination and training of clinicians at the Schools of Dentistry at the University of North Carol, Univ. of Minnesota and Oregon Health Sciences will allow expansion of the proposed studies. In Aim 1, we will test state-of-the-art neural network structures to develop a combined software module that will include the most efficient and accurate neural network architecture and advanced statistics to mine imaging, clinical and biological TMJ OA markers identified at baseline. In Aim 2, we propose to develop novel data analytics tools, evaluating the performance of various machine learning and statistical predictive models, including customized- Gaussian Process Regression, extreme boosted trees, Multivariate Varying Coefficient Model, Lasso, Ridge and Elastic net, Random Forest, pdfCluster, decision tree, and support vector machine. Such automated solutions will leverage emerging computing technologies to determine risk indicators for OA progression in longitudinal cohorts of TMJ health and disease. PROJECT NARRATIVE This application proposes the development of efficient web-based data management, mining, and analytics of clinical, biological, and high dimensional imaging data from TMJ OA patients. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA.",Integrative Predictors of Temporomandibular Osteoarthritis,10017950,R01DE024450,"['3-Dimensional', 'Age', 'Architecture', 'Arthritis', 'Benchmarking', 'Biological', 'Biological Markers', 'Blood', 'Bone remodeling', 'Bone structure', 'Cancer Center', 'Chronic', 'Classification', 'Clinical', 'Clinical Markers', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Diagnosis', 'Country', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Storage and Retrieval', 'Database Management Systems', 'Databases', 'Decision Trees', 'Degenerative polyarthritis', 'Dental', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Environment', 'Fibrocartilages', 'Future', 'Gaussian model', 'Goals', 'Hand', 'Health', 'Health Sciences', 'Image', 'Image Analysis', 'Individual', 'Inflammation Mediators', 'Inflammatory', 'Internet', 'Joints', 'Lasso', 'Longitudinal cohort', 'Machine Learning', 'Mandibular Condyle', 'Mediator of activation protein', 'Medicine', 'Methods', 'Michigan', 'Mining', 'Minnesota', 'Modeling', 'Molecular', 'Morphology', 'North Carolina', 'Online Systems', 'Oregon', 'Outcome', 'Pain', 'Paper', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Phenotype', 'Process', 'Property', 'Proteins', 'Publishing', 'Replacement Arthroplasty', 'Resolution', 'Risk', 'Saliva', 'School Dentistry', 'Scientific Advances and Accomplishments', 'Severities', 'Slice', 'Structure', 'Study models', 'Symptoms', 'System', 'Technology', 'Temporomandibular Joint', 'Temporomandibular joint osteoarthritis', 'Testing', 'Texas', 'Three-Dimensional Imaging', 'Training', 'Trees', 'Universities', 'University of Texas M D Anderson Cancer Center', 'Work', 'X-Ray Computed Tomography', 'analytical tool', 'base', 'bone', 'cadherin 5', 'cartilage degradation', 'clinical center', 'clinical diagnostics', 'cone-beam computed tomography', 'craniofacial', 'craniomaxillofacial', 'data warehouse', 'deep learning', 'deep neural network', 'design', 'high dimensionality', 'imaging biomarker', 'improved', 'innovation', 'joint destruction', 'machine learning algorithm', 'neural network', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'outcome forecast', 'predictive modeling', 'prospective', 'quantitative imaging', 'random forest', 'repository', 'scale up', 'screening', 'serial imaging', 'software repository', 'statistical and machine learning', 'statistics', 'subchondral bone', 'support vector machine', 'tool']",NIDCR,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,513041,641965656,0.0028683574205746794
"An Integrated Multilevel Modeling Framework for Repertoire-Based Diagnostics Immune-repertoire sequence, which consists of an individual's millions of unique antibody and T-cell receptor (TCR) genes, encodes a dynamic and highly personalized record of an individual's state of health. Our long- term goal is to develop the computational models and tools necessary to read this record, to one day be able diagnose diverse infections, autoimmune diseases, cancers, and other conditions directly from repertoire se- quence. The key problem is how to find patterns of specific diseases in repertoire sequence, when repertoires are so complex. Our hypothesis is that a combination of bottom-up (sequence-level) and top-down (systems- level) modeling can reveal these patterns, by encoding repertoires as simple but highly informative models that can be used to build highly sensitive and specific disease classifiers. In preliminary studies, we introduced two new modeling approaches for this purpose: (i) statistical biophysics (bottom-up) and (ii) functional diversity (top-down), and showed their ability to elucidate patterns related to vaccination status (97% accuracy), viral infection, and aging. Building on these studies, we will test our hypothesis through two specific aims: (1) We will develop models and classifiers based on the bottom-up approach, statistical biophysics; and (2) we will de- velop the top-down approach, functional diversity, to improve these classifiers. To achieve these aims, we will use our extensive collection of public immune-repertoire datasets, beginning with 391 antibody and TCR da- tasets we have characterized previously. Our team has deep and complementary expertise in developing computational tools for finding patterns in immune repertoires (Dr. Arnaout) and in the mathematics that under- lie these tools (Dr. Altschul), with additional advice available as needed regarding machine learning (Dr. AlQuraishi). This proposal is highly innovative for how our two new approaches address previous issues in the field. (i) Statistical biophysics uses a powerful machine-learning method called maximum-entropy modeling (MaxEnt), improving on past work by tailoring MaxEnt to learn patterns encoded in the biophysical properties (e.g. size and charge) of the amino acids that make up antibodies/TCRs; these properties ultimately determine what targets antibodies/TCRs can bind, and therefore which sequences are present in different diseases. (ii) Functional diversity fills a key gap in how immunological diversity has been measured thus far, by factoring in whether different antibodies/TCRs are likely to bind the same target. This proposal is highly significant for (i) developing an efficient, accurate, generative, and interpretable machine-learning method for finding diagnostic patterns in repertoire sequence; (ii) applying a robust mathematical framework to the measurement of immuno- logical diversity; (iii) impacting clinical diagnostics; and (iv) adding a valuable new tool for integrative/big-data medicine. The expected outcome of this proposal is an integrated pair of robust and well validated new tools/models for classifying specific disease exposures directly from repertoire sequence. This proposal in- cludes plans to make these tools widely available, to maximize their positive impact across medicine. The proposed research is relevant to public health because B cells/antibodies and T cells play vital roles across such a vast range of health conditions, from infection, to autoimmunity, to cancer, that the ability to de- code what they are doing would be an important step forward for diagnosing these conditions. The proposed research is relevant to the NIH's mission of fostering fundamental creative discoveries, innovative research strategies, and their applications as a basis for ultimately protecting and improving health, specifically relating to the diagnosis of human diseases.",An Integrated Multilevel Modeling Framework for Repertoire-Based Diagnostics,10050030,R01AI148747,"['Address', 'Affect', 'Aging', 'Amino Acid Motifs', 'Amino Acids', 'Antibodies', 'Autoimmune Diseases', 'Autoimmunity', 'B-Lymphocytes', 'Base Sequence', 'Big Data', 'Binding', 'Biophysics', 'Characteristics', 'Charge', 'Classification', 'Clinical', 'Code', 'Collection', 'Complex', 'Computer Models', 'Data Set', 'Dependence', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Ensure', 'Entropy', 'Fostering', 'Gene Frequency', 'Genes', 'Goals', 'Health', 'Human', 'Immune', 'Immunology', 'Individual', 'Infection', 'Influenza vaccination', 'Intuition', 'Learning', 'Letters', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Measures', 'Medicine', 'Methods', 'Mission', 'Modeling', 'Outcome', 'Pattern', 'Performance', 'Persons', 'Physics', 'Play', 'Population Heterogeneity', 'Privatization', 'Property', 'Public Health', 'Reading', 'Reporting', 'Research', 'Role', 'Sample Size', 'Sampling', 'Sampling Errors', 'Signs and Symptoms', 'Speed', 'Statistical Study', 'System', 'T-Cell Receptor', 'T-Cell Receptor Genes', 'T-Lymphocyte', 'Testing', 'United States National Institutes of Health', 'Vaccination', 'Virus Diseases', 'Work', 'base', 'biophysical properties', 'clinical diagnostics', 'computerized tools', 'diagnostic accuracy', 'human disease', 'immunological diversity', 'improved', 'information model', 'innovation', 'machine learning method', 'multidisciplinary', 'multilevel analysis', 'novel', 'novel strategies', 'tool']",NIAID,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2020,535171,135941803,-0.0038117696391233983
"Quantifying molecular consequences of human missense variants with large-scale interactome perturbation studies Almost all proteins function through interacting with other proteins. Previous studies have shown that the vast majority of damaging single amino acid mutations in proteins disrupt only a subset of specific protein-protein interactions, and that mutations in the same protein that disrupt different interactions tend to cause clinically distinct disorders. Therefore, it is of great importance to determine interaction-specific disruptions caused by each mutation. Furthermore, rapid advances in sequencing technologies have enabled the identification of tens of millions of single nucleotide variants (SNVs) in the human population, driving an urgent need to understand the impact of each SNV on the human interactome network. Unfortunately, there is currently no method that is capable of predicting the specific impact of a large fraction of these SNVs on individual protein-protein interactions. To address this issue, we propose to leverage our massively-parallel site-directed mutagenesis pipeline, Clone-seq, to generate clones for ~6,000 coding SNVs in the human population: ~4,000 from gnomAD and ~2,000 to be submitted by the international human genetics community. We will then experimentally examine the impact on protein stability and individual protein-protein interactions for every variant using high-throughput DUAL-FLUO and InPOINT (integrating PCA, LUMIER, Y2H, and wNAPPA) assays. This proposal brings together three groups with complementary expertise in high-throughput interactome experiments and network analysis from the Yu lab, in genomic and population genetic studies from the Clark lab, and in comprehensive biophysical and structural modeling of mutation’s impact on binding free energy of protein interactions from the Alexov lab. Out of the ~6,000 SNVs, we expect to identify ~1,200 disruptive SNVs and ~4,000 different SNV-interaction pairs where the SNV disrupt that specific interaction. The data produced by our project will increase the available experimental information by >140× in number of human proteins and >500× in number of interactions, allowing us for the first time to comprehensively assess the relationships between the impact of SNVs on interactions and their various population genetic attributes (including, but not limited to, allele frequency and flanking haplotype, inter-population differentiation, local rate of recombination, allele age, modes of selection). Finally, we will establish a computational-experimental- integrated iterative learning scheme to build a multi-layer random-forest-based framework, SIMPACT, which can accurately predict specific impacts on all individual protein-protein interactions for all missense SNVs. Our proposed work will fuel hypothesis-driven research, will significantly improve our functional understanding of variants, and will likely fundamentally change the experimental design and data interpretation for whole genome/exome studies going forward. The dramatic increase of DNA variants discovered through advances in sequencing technologies has been inadequately translated into therapeutic successes. Although many of these variants are related to human disorders, the overwhelming number of non-functional variants makes the assessment of functional significance a steep challenge. In this study, we aim to develop a high-throughput pipeline to quickly clone and directly test a large number of coding variants for their impact on the human interactome network and use the results to build a machine learning pipeline to predict functional impact of all coding variants, in anticipation that both our experimental data and computational pipeline will lead to broad clinical and therapeutic applications.",Quantifying molecular consequences of human missense variants with large-scale interactome perturbation studies,9872026,R01GM125639,"['Address', 'Age', 'Alleles', 'Amino Acids', 'Automobile Driving', 'Binding', 'Biochemical', 'Biological Assay', 'Cells', 'Clinical', 'Code', 'Communities', 'Coupling', 'Crystallization', 'DNA', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease', 'Experimental Designs', 'Free Energy', 'Gene Frequency', 'Genes', 'Genetic Recombination', 'Genetic study', 'Genome', 'Genomics', 'Haplotypes', 'Homology Modeling', 'Human', 'Human Genetics', 'Individual', 'International', 'Learning', 'Letters', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Molecular', 'Mutation', 'Output', 'Pathway Analysis', 'Phenotype', 'Population', 'Population Genetics', 'Property', 'Proteins', 'Proteomics', 'Publishing', 'Research', 'Resolution', 'Resources', 'Scheme', 'Single Nucleotide Polymorphism', 'Site-Directed Mutagenesis', 'Structural Models', 'Structure', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Translating', 'Variant', 'Work', 'base', 'biological research', 'biophysical model', 'biophysical techniques', 'computational pipelines', 'data pipeline', 'exome', 'experimental study', 'human interactome', 'improved', 'interest', 'molecular phenotype', 'mutant', 'next generation sequencing', 'protein function', 'protein protein interaction', 'random forest', 'screening', 'success', 'web portal', 'whole genome']",NIGMS,CORNELL UNIVERSITY,R01,2020,563248,91477866,0.007632084166965168
"Development of a novel method for cryopreservation of Drosophila melanogaster PROJECT SUMMARY This proposal seeks to develop a resource for the preservation of the fruit fly, Drosophila melanogaster. This insect is a foundational model organism for biological research. Over a century of work, an enormous number of fly strains harboring different mutant alleles or transgenic constructs have been generated. However, one limitation of working with flies is that there is as yet no practical method for cryopreservation of Drosophila strains. Conventional methods of vitrifying Drosophila were developed in the early 1990s and were never widely adopted due to the difficulty in performing the protocols. This is a problem from a practical perspective since all these strains need to be individually maintained in continuous culture at substantial cost and labor, and also from a scientific perspective, since in the process of continuous culture mutations can accumulate and contamination can occur, degrading the value of these resources for future experiments. A novel approach for cryopreservation of Drosophila is proposed for this R24 resource center. Isolated embryonic nuclei, rather than intact embryos, will be cryopreserved and then nuclear transplantation via microinjection will be used to create clones derived from the cryopreserved nuclei. This approach avoids the issues associated with the impermeability of embryonic membranes that have prevented the use of conventional cryopreservation approaches that have been used with other organisms. Embryonic nuclei will be cryopreserved using a naturally inspired approach. Diverse biological systems (plants, insects, etc.) survive dehydration, drought, freezing temperatures and other stresses through the use of osmolytes. On an applied level, the proposed investigation has the potential to transform preservation of Drosophila lines by 1) preserving subcellular components (specifically nuclei) as opposed to embryos; and 2) automating much of the workflow. In the long- term, the goal of this resource center is to develop a robust and scalable protocol for cryopreservation of Drosophila, thus reducing the cost and improving the quality of long-term strain maintenance. PROJECT NARRATIVE The fruit fly, Drosophila melanogaster, is a very important model organism for biomedical research. The goal of this resource center is to develop effective methods of preserving fruit flies in order to lower the costs and improve the quality of stock maintenance. The approach leverages recent scientific advances to develop a new, highly automated approach for preserving fruit flies.",Development of a novel method for cryopreservation of Drosophila melanogaster,9935719,R24OD028444,"['Adopted', 'Algorithms', 'Alleles', 'Animal Model', 'Asses', 'Automation', 'Biological', 'Biomedical Research', 'Cell Nucleus', 'Cells', 'Cellular biology', 'Communities', 'Cryopreservation', 'Dehydration', 'Development', 'Developmental Biology', 'Drosophila genus', 'Drosophila melanogaster', 'Droughts', 'Embryo', 'Engineering', 'Evolution', 'Formulation', 'Foundations', 'Freezing', 'Future', 'Genetic', 'Genome', 'Genotype', 'Goals', 'Image', 'Individual', 'Insecta', 'Investigation', 'Machine Learning', 'Maintenance', 'Mechanics', 'Membrane', 'Methods', 'Microinjections', 'Molecular Biology', 'Monoclonal Antibody R24', 'Mutation', 'Neurosciences', 'Nuclear', 'Organism', 'Plants', 'Process', 'Protocols documentation', 'Raman Spectrum Analysis', 'Recovery', 'Resources', 'Robotics', 'Scientific Advances and Accomplishments', 'Spectrum Analysis', 'Stress', 'System', 'Techniques', 'Temperature', 'Testing', 'Transgenic Organisms', 'Work', 'biological research', 'biological systems', 'cold temperature', 'cost', 'epigenome', 'experimental study', 'fly', 'genetic technology', 'high throughput screening', 'improved', 'individual response', 'mutant', 'novel', 'novel strategies', 'nuclear transfer', 'preservation', 'prevent', 'tool']",OD,UNIVERSITY OF MINNESOTA,R24,2020,599090,340417756,0.006132438111025017
"Development and Deployment of Artificial Intelligence (AI) Driven Methods to Enable Chest X-ray Radiography as an Alternative Diagnostic Method for COVID-19 Pneumonia ABSTRACT In this competitive revision, within the same scope of developing and deploying algorithms to make a quantum leap in clinical diagnosis as that in our current U01EB021183, we would like to revise the original aims to add a new Aim to leverage our expertise in the areas of algorithm development and clinical translation to make immediate contributions to combat the COVID-19 pandemic. Specifically, we propose to develop and deploy artificial intelligence (AI) methods to enable chest x-ray radiography (CXR) as an alternative diagnostic tool to diagnose COVID-19 pneumonia, to rapidly triage patients for appropriate treatment, to monitor the treatment response in a contained environment, and to optimize the distribution of the limited medical resources during the current COVID-19 crisis. PROJECT NARRATIVE In this project, our overarching objective is to develop automated artificial intelligence (AI)-based algorithms to help radiologists to differentiate COVID-19 related pneumonia from other non-COVID-19 related pneumonia using CXR images. The advantages of the proposed AI equipped CXR technique include: i) widely available, ii) inexpensive, iii) excellent coronavirus exposure profile for patient, technologist, and equipment, and iv) rapid and automated DL interpretation, which is effectively instantaneous.",Development and Deployment of Artificial Intelligence (AI) Driven Methods to Enable Chest X-ray Radiography as an Alternative Diagnostic Method for COVID-19 Pneumonia,10156179,U01EB021183,"['Accident and Emergency department', 'Air', 'Algorithms', 'American College of Radiology', 'Anosmia', 'Appearance', 'Area', 'Artificial Intelligence', 'Bilateral', 'COVID-19', 'COVID-19 pandemic', 'Case Study', 'Cessation of life', 'China', 'Clinic', 'Clinical', 'Communities', 'Containment', 'Coronavirus', 'Coughing', 'Country', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Procedure', 'Diagnostic Sensitivity', 'Diagnostic radiologic examination', 'Diarrhea', 'Disease', 'Disease Outbreaks', 'Dyspnea', 'Environment', 'Equipment', 'European', 'Exposure to', 'Fatigue', 'Fever', 'Glass', 'Gold', 'Health Personnel', 'Health care facility', 'Hospitals', 'Human', 'Image', 'Individual', 'Investigation', 'Lung', 'Lung diseases', 'Medical', 'Medical Imaging', 'Methods', 'Monitor', 'North America', 'Parents', 'Pathway interactions', 'Patient Triage', 'Patients', 'Performance', 'Persons', 'Pleural effusion disorder', 'Pneumonia', 'Process', 'Radiology Specialty', 'Reading', 'Reporting', 'Resources', 'Reverse Transcriptase Polymerase Chain Reaction', 'Rural', 'Sensitivity and Specificity', 'Societies', 'Symptoms', 'Techniques', 'Technology', 'Testing', 'Thoracic Radiography', 'Time', 'Triage', 'United States', 'Viral Pneumonia', 'War', 'World Health Organization', 'X-Ray Computed Tomography', 'accurate diagnosis', 'algorithm development', 'base', 'chest computed tomography', 'clinical Diagnosis', 'clinical translation', 'combat', 'deep learning', 'high risk', 'high risk population', 'imaging facilities', 'imaging modality', 'improved', 'intelligent algorithm', 'neural network architecture', 'pandemic disease', 'prevent', 'profiles in patients', 'quantum', 'radiologist', 'screening', 'success', 'tool', 'treatment response', 'urgent care']",NIBIB,UNIVERSITY OF WISCONSIN-MADISON,U01,2020,605070,338121506,-0.0026706011576970294
"Integrating Ethics into Machine Learning for Precision Medicine The application of new computerized methods of data analysis to vast collections of medical, biological, and other data is emerging as a central feature of a broad vision of precision medicine (PM) in which systems based on artificial intelligence (AI) assist clinicians in treatment, diagnosis, or prognosis. The use of AI to analyze big data for clinical decision-making opens up a new domain for ELSI inquiry to address a possible future when the implications of genetics and genomics become embedded into algorithms, pervasive yet implicit and difficult to identify. Thus, an important target of inquiry is the development and developers of these algorithms. There are three distinctive features of the application of AI, and in particular machine learning (ML), to the domain of PM that create the need for ELSI inquiry. First, the process of developing ML-based systems for PM goals is technically and organizationally complex. Thus, members of development teams will likely have different expertise and assumptions about norms, responsibilities, and regulation. Second, machine learning does not solely operate through predetermined rules, and is thus difficult to hold accountable for its conclusions or reasoning. Third, designers of ML systems for PM may be subject to diverse and divergent interests and needs of multiple stakeholders, yet unaware of the associated ethical and values implications for design. These distinctive features of ML in PM could lead to difficulties in detecting misalignment of design with values, and to breakdown in responsibility for realignment. Because machine learning in the context of precision medicine is such a new phenomenon, we have very little understanding of actual practices, work processes and the specific contexts in which design decisions are made. Importantly, we have little knowledge about the influences and constraints on these decisions, and how they intersect with values and ethical principles. Although the field of machine learning for precision medicine is still in its formative stage, there is growing recognition that designers of AI systems have responsibilities to ask such questions about values and ethics. In order to ask these questions, designers must first be aware that there are values expressed by design. Yet, there are few practical options for designers to learn how to increase awareness. Our specific aims are: Aim 1 To map the current state of ML in PM by identifying and cataloging existing US-based ML in PM  projects and by exploring a range of values expressed by stakeholders about the use of ML in PM through  a combination of multi-method review, and interviews of key informants and stakeholders. Aim 2 To characterize decisions and rationales that shape ML in PM and explore whether and how  developers perceive values as part of these rationales through interviews of ML developers and site visits. Aim 3 To explore the feasibility of using design rationale as a framework for increasing awareness of the  existence of values, and multiple sources of values, in decisions about ML in PM through group-based  exercises with ML developers from academic and commercial settings. The overall goal of this project is to understand how to encourage and enable people who are developing artificial intelligence for personalized health care to be aware of values in their daily practice. We will examine actual practices and contexts in which design decisions are made for precision medicine applications, and use this information to design group-based workshop exercises to increase awareness of values.",Integrating Ethics into Machine Learning for Precision Medicine,9941090,R01HG010476,"['Address', 'Algorithms', 'Artificial Intelligence', 'Awareness', 'Big Data', 'Biological', 'Cataloging', 'Catalogs', 'Clinical', 'Collection', 'Complex', 'Computers', 'Data', 'Data Analyses', 'Development', 'Diagnosis', 'Educational workshop', 'Electronic Health Record', 'Engineering', 'Ethics', 'Evolution', 'Exercise', 'Expert Systems', 'Foundations', 'Future', 'Genetic', 'Genomics', 'Goals', 'Healthcare', 'Interview', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Outcome', 'Process', 'Regulation', 'Research', 'Resources', 'Sampling', 'Scholarship', 'Scientist', 'Shapes', 'Site Visit', 'Source', 'System', 'Time', 'Vision', 'Work', 'base', 'biobank', 'clinical decision-making', 'computerized', 'design', 'ethical legal social implication', 'genomic data', 'informant', 'innovation', 'interest', 'member', 'new technology', 'outcome forecast', 'personalized health care', 'precision medicine']",NHGRI,STANFORD UNIVERSITY,R01,2020,605875,560644462,0.008701397099824997
"PREMIERE: A PREdictive Model Index and Exchange REpository The confluence of new machine learning (ML) data-driven approaches; increased computational power; and access to the wealth of electronic health records (EHRs) and other emergent types of data (e.g., omics, imaging, mHealth) are accelerating the development of biomedical predictive models. Such models range from traditional statistical approaches (e.g., regression) through to more advanced deep learning techniques (e.g., convolutional neural networks, CNNs), and span different tasks (e.g., biomarker/pathway discovery, diagnostic, prognostic). Two issues have become evident: 1) as there are no comprehensive standards to support the dissemination of these models, scientific reproducibility is problematic, given challenges in interpretation and implementation; and 2) as new models are put forth, methods to assess differences in performance, as well as insights into external validity (i.e., transportability), are necessary. Tools moving beyond the sharing of data and model “executables” are needed, capturing the (meta)data necessary to fully reproduce a model and its evaluation. The objective of this R01 is the development of an informatics standard supporting the requisite information for scientific reproducibility for statistical and ML-based biomedical predictive models; from this foundation, we then develop new computational approaches to compare models' performance. We begin by extending the current Predictive Model Markup Language (PMML) standard to fully characterize biomedical datasets and harmonize variable definitions; to elucidate the algorithms involved in model creation (e.g., data preprocessing, parameter estimation); and to explain the validation methodology. Importantly, models in this PMML format will become findable, accessible, interoperable, and reusable (i.e., following FAIR principles). We then propose novel meth- ods to compare and contrast predictive models, assessing transportability across datasets. While metrics exist for comparing models (e.g., c-statistics, calibration), often the required case-level information is not available to calculate these measures. We thus introduce an approach to simulate cases based on a model's reported da- taset statistics, enabling such calculations. Different levels of transportability are then assigned to the metrics, determining the extent to which a selected model is applicable to a given population/cohort (i.e., helping answer the question, can I use this published model with my own data?). We tie these efforts together in our proposed framework, the PREdictive Model Index & Exchange REpository (PREMIERE). We will develop an online portal and repository for model sharing around PREMIERE, and our efforts will include fostering a community of users to guide its development through workshops, model-thons, and other activities. To demonstrate these efforts, we will bootstrap PREMIERE with predictive models from a targeted domain (risk assessment in imaging-based lung cancer screening). Our efforts to evaluate these developments will engage a range of stakeholders (model developers, users) to inform the completeness of our standard; and biostatisticians and clinical experts to guide assessment of model transportability. PROGRAM NARRATIVE With growing access to information contained in the electronic health record and other data sources, the appli- cation of statistical and machine learning methods are generating more biomedical predictive models. However, there are significant challenges to reproducing these models for purposes of comparison and application in new environments/populations. This project develops informatics standards to facilitate the sharing and reproducibil- ity of these models, enabling a suite of comparative methods to evaluate model transportability.",PREMIERE: A PREdictive Model Index and Exchange REpository,10016297,R01EB027650,"['Access to Information', 'Address', 'Algorithms', 'Area', 'Attention', 'Bayesian Network', 'Big Data', 'Biological Markers', 'Calibration', 'Characteristics', 'Clinical', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Decision Making', 'Decision Trees', 'Dermatology', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Ecosystem', 'Educational workshop', 'Electronic Health Record', 'Environment', 'Evaluation', 'FAIR principles', 'Fostering', 'Foundations', 'Goals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Language', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Ophthalmology', 'Pathway interactions', 'Performance', 'Population', 'Publications', 'Publishing', 'Radiology Specialty', 'Receiver Operating Characteristics', 'Reporting', 'Reproducibility', 'Reproduction', 'Research Personnel', 'Risk Assessment', 'Source', 'Techniques', 'Testing', 'Training', 'Validation', 'Variant', 'Work', 'base', 'bioimaging', 'biomarker discovery', 'case-based', 'cohort', 'collaborative environment', 'comparative', 'computer aided detection', 'convolutional neural network', 'data sharing', 'deep learning', 'design', 'experience', 'feature selection', 'indexing', 'innovation', 'insight', 'interest', 'interoperability', 'learning network', 'lung basal segment', 'lung cancer screening', 'mHealth', 'machine learning method', 'model development', 'novel', 'novel strategies', 'online repository', 'predictive modeling', 'prognostic', 'repository', 'software repository', 'statistical and machine learning', 'statistics', 'stem', 'tool', 'web portal']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2020,673491,673201228,-0.007657162372478689
"Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Efforts to include behavioral measures in large-scale studies as envisioned by precision medicine are hampered by the time and expertise required. Paper-and-pencil tests currently dominating clinical assessment and neuropsychological testing are plainly unfeasible. The NIH Toolbox contains many computerized tests and clinical assessment tools varying in feasibility. Unique in the Toolbox is the Penn Computerized Neurocognitive Battery (CNB), which contains 14 tests that take one hour to administer. CNB has been validated with functional neuroimaging and in multiple normative and clinical populations across the lifespan worldwide, and is freely available for research. Clinical assessment tools are usually devoted to specific disorders, and scales vary in their concentration on symptoms that are disorder specific. We have developed a broad assessment tool (GOASSESS), which currently takes about one hour to administer. These instruments were constructed, optimized and validated with classical psychometric test theory (CTT), and are efficient as CTT allows. However, genomic studies require even more time-efficient tools that can be applied massively.  Novel approaches, based on item response theory (IRT) can vastly enhance efficiency of testing and clinical assessment. IRT shifts the emphasis from the test to the items composing it by estimating item parameters such as “difficulty” and “discrimination” within ranges of general trait levels. IRT helps shorten the length of administration without compromising data quality, and for many domains leads to computer adaptive testing (CAT) that further optimizes tests to individual abilities. We propose to develop and validate adaptive versions of the CNB and GOASSESS, resulting in a neurocognitive and clinical screener that, using machine learning tools, will be continually optimized, becoming shorter and more precise as it is deployed. The tool will be in the Toolbox available in the public domain. We have item-level information to perform IRT analyses on existing data and use this information to develop CAT implementations and generate item pools for adaptive testing. Our Specific Aims are: 1. Use available itemwise data on the Penn CNB and the GOASSESS and add new tests and items to generate item pools for extending scope while abbreviating tests using IRT-CAT and other methods. The current item pool will be augmented to allow large selection of items during CAT administration and add clinical items to GOASSESS. New items will be calibrated through crowdsourcing. 2. Produce a modular CAT version of a neurocognitive and clinical assessment battery that covers major RDoC domains and a full range of psychiatric symptoms. We have implemented this procedure on some CNB tests and clinical scales and will apply similar procedures to remaining and new tests as appropriate. 3. Validate the CAT version in 100 individuals with psychosis spectrum disorders (PS), 100 with depression/anxiety disorders (DA), and 100 healthy controls (HC). We will use this dataset to implement and test data mining algorithms that optimize prediction of specific outcomes. All tests, algorithms and normative data will be in the toolbox. Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Narrative Large scale genomic studies are done in the context of precision medicine, and for this effort to benefit neuropsychiatric disorders such studies should include behavioral measures of clinical symptoms and neurocognitive performance. Current tools are based on classical psychometric theory, and we propose to apply novel approaches of item response theory to develop a time-efficient adaptive tool for assessing broad neurocognitive functioning and psychopathology. The tool will be available in the public domain (NIH Toolbox) and will facilitate incorporation of psychiatric disorders into the precision medicine initiative.",Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan,9920211,R01MH117014,"['Algorithms', 'Anxiety', 'Anxiety Disorders', 'Assessment tool', 'Behavior', 'Biological Markers', 'Calibration', 'Characteristics', 'Classification', 'Clinical', 'Clinical Assessment Tool', 'Clinical assessments', 'Cognitive', 'Collection', 'Complex', 'Computers', 'Data', 'Data Compromising', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Discrimination', 'Disease', 'Environmental Risk Factor', 'Feedback', 'Female', 'Genomics', 'Hour', 'Individual', 'Internet', 'Internet of Things', 'Intervention Studies', 'Length', 'Link', 'Longevity', 'Machine Learning', 'Measures', 'Medicine', 'Mental Depression', 'Mental disorders', 'Methods', 'Molecular Genetics', 'Moods', 'Neurocognitive', 'Neurocognitive Deficit', 'Neuropsychological Tests', 'Neurosciences', 'Outcome', 'Paper', 'Pathway interactions', 'Performance', 'Phenotype', 'Population', 'Precision Medicine Initiative', 'Preparation', 'Preventive Intervention', 'Procedures', 'Psychiatry', 'Psychometrics', 'Psychopathology', 'Psychotic Disorders', 'Public Domains', 'Research', 'Research Domain Criteria', 'Sampling', 'Screening procedure', 'Sensitivity and Specificity', 'Severities', 'Speed', 'Structure', 'Symptoms', 'Tablets', 'Testing', 'Time', 'Translational Research', 'United States National Institutes of Health', 'Validation', 'base', 'behavior measurement', 'cognitive performance', 'computerized', 'crowdsourcing', 'data mining', 'data quality', 'digital', 'genomic variation', 'improved', 'individualized prevention', 'instrument', 'male', 'mobile computing', 'neuroimaging', 'neuropsychiatric disorder', 'novel', 'novel strategies', 'open source', 'precision medicine', 'protective factors', 'psychiatric symptom', 'response', 'symptom cluster', 'theories', 'tool', 'trait', 'validation studies']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2020,709525,593605914,-0.008867055270126819
"Blending deep learning with probabilistic mechanistic models to predict and understand the evolution and function of adaptive immune receptors Project Summary Scientific understanding of adaptive immune receptors (i.e. antibodies and T cell receptors) has the potential to revolutionize prophylaxis, diagnosis, and treatment of disease. High‐throughput DNA sequencing and functional experiments have now brought the study of adaptive immune receptors into the big‐data era. To realize this potential of these data they must be matched with appropriately powerful analytical techniques. Existing probabilistic and mechanistic models are insufficient to capture the complexities of these data, while a naïve application of machine learning cannot leverage our profound existing knowledge of the immune system. The goal of this project is to blend deep learning with mechanistic modeling in order to predict and understand the evolution and function of adaptive immune receptors. Aim 1: Develop generative models of immune receptor sequences that capture the complexity of real adaptive immune receptor repertoires. These will combine deep learning along with our knowledge of VDJ recombination, and provide a rigorous platform for detailed repertoire comparison. Aim 2: Develop quantitative mechanistic models of antibody somatic hypermutation that incorporate the underlying biochemical processes. Estimate intractable likelihoods using deep learning to infer important latent variables, and validate models using knock‐out experiments in cell lines. Aim 3: Develop hybrid deep learning models to predict binding properties from sequence data, combining large experimentally‐derived binding data with even larger sets of immune sequences from human immune memory samples. Incorporate structural information via 3D convolution or distance‐based penalties. These tools will reveal the full power of immune repertoire data for medical applications. We will obtain more rigorous comparisons of repertoires via their distribution in a relevant space. These will reveal the effects of immune perturbations such as vaccination and disease, allowing us to pick out sequences that are impacted by these perturbations. We will have a greater quantitative understanding of somatic hypermutation in vivo, and statistical models that appropriately capture long‐range effects of collections of mutations. We will also have algorithms that will be able to combine repertoire data and sparse binding data to predict binding properties. Put together, these advances will enable rational vaccine design, treatment for autoimmune disease, and identification of T cells that are promising candidates for cancer immunotherapy. Project Narrative Adaptive immune receptors (i.e. antibodies and T cell receptors) enable our body to fight off disease, “remember” pathogens, and train the immune system through vaccination. Immunologists have learned via high‐throughput sequencing that adaptive immune receptors have a truly remarkable diversity. In this proposal, we develop machine‐learning methods for these sequence data, which will allow us to predict the maturation, statistical distribution, and binding properties of adaptive immune receptors, and thus to better design vaccinations, autoimmune disease treatment, and immunotherapy treatment for cancer.",Blending deep learning with probabilistic mechanistic models to predict and understand the evolution and function of adaptive immune receptors,9938424,R01AI146028,"['3-Dimensional', 'Algorithms', 'Animal Model', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Architecture', 'Autoimmune Diseases', 'Automobile Driving', 'Big Data', 'Binding', 'Biochemical', 'Biochemical Process', 'Categories', 'Cell Line', 'Characteristics', 'Collection', 'Complement', 'Complex', 'Data', 'Data Set', 'Dependence', 'Diagnosis', 'Disease', 'Entropy', 'Evolution', 'Exposure to', 'Foundations', 'Gene Conversion', 'Generations', 'Goals', 'High-Throughput DNA Sequencing', 'High-Throughput Nucleotide Sequencing', 'Human', 'Hybrids', 'Immune', 'Immune response', 'Immune system', 'Immunoglobulin Somatic Hypermutation', 'Immunologic Memory', 'Immunologic Receptors', 'Immunological Models', 'Immunologics', 'Immunologist', 'Immunology', 'Immunotherapy', 'In Vitro', 'Individual', 'Knock-out', 'Knowledge', 'Laboratories', 'Machine Learning', 'Medical', 'Methods', 'Modeling', 'Modification', 'Mutation', 'Pathway interactions', 'Population', 'Procedures', 'Process', 'Property', 'Prophylactic treatment', 'Resolution', 'Sampling', 'Science', 'Statistical Distributions', 'Statistical Methods', 'Statistical Models', 'Structure', 'T-Cell Receptor', 'T-Lymphocyte', 'T-cell receptor repertoire', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Update', 'V(D)J Recombination', 'Vaccination', 'Vaccine Design', 'Vaccines', 'Validation', 'Work', 'algorithm training', 'analytical tool', 'base', 'biochemical model', 'cancer immunotherapy', 'cancer therapy', 'complex data ', 'deep learning', 'deep neural network', 'deep sequencing', 'design', 'experimental study', 'fighting', 'functional group', 'in vivo', 'insertion/deletion mutation', 'large datasets', 'machine learning method', 'markov model', 'pathogen', 'progenitor', 'receptor', 'repaired', 'response', 'success', 'three dimensional structure', 'tool']",NIAID,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2020,710049,758431960,-0.010099482473733464
"Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine The overall goal of this project is to develop a safe, broadly effective, and affordable vaccine to prevent group A streptococcal infections. Antibodies against the N-terminal hypervariable region (HVR) of surface M (Emm) proteins of GAS are opsonic and are associated with protection against infection. Immunity has classically been described as “type-specific”, leading to the assumption that natural immunity confers protection against only one of the more than 200 different emm types of GAS. We now have new information that calls into question this classic view and serves as the basis for an entirely different approach to GAS vaccine design and development. A recent comprehensive sequence analysis of M proteins from a global collection of 175 emm types of GAS resulted in a new emm cluster typing system that classified 96.2% of all contemporary GAS isolates into 48 emm clusters containing structurally and functionally related M proteins. Moreover, 117 emm types contained in 16 clusters accounted for 94.4% of GAS infections in the world. Indeed, preclinical studies indicated that a multivalent vaccine containing N-terminal peptides from 30 prevalent M types cross-opsonized a significant number of non-vaccine emm types of GAS that co-localized in clusters with vaccine emm types. The frequency of cross-opsonic antibodies, combined with the emm cluster data, prompted us to conclude that there is a need for a paradigm shift away from the concept of “type-specific” immunity against GAS infections to one of “cluster-specific” immunity. Our overall hypothesis is that immunity to GAS infections is the result of both type-specific and cross-reactive antibodies against the N-terminal regions of M proteins and that a new approach employing computational predictions of peptide structures will result in a multivalent vaccine that will induce broadly protective immunity in populations throughout the world. Our preliminary results indicate the feasibility of using structure-based design to predict the antigenic relatedness of M peptides within a cluster. The specific aims of this proposal are to: 1) Apply computational structure-based design in an iterative process with immunological data from Aim 2 to predict the minimal number of M peptide sequences that are most representative of the structural and physicochemical properties of the peptides in one emm cluster containing 17 GAS emm types, 2) determine the cross-reactive immunogenicity of the selected peptides with all seventeen emm types of GAS in the cluster, and apply the results to refine the computational design predictions in Aim 1, 3) apply the refined computational parameters from Aims 1 and 2 to analyze the remaining epidemiologically important emm clusters, select a comprehensive panel of peptides representing all emm types, construct four multivalent recombinant vaccine proteins, and assess potential cross-protective immunogenicity using in vitro bactericidal assays against all 117 emm types of GAS, and 4) determine the protective immunogenicity of the final multivalent vaccine in unique transgenic mice expressing human C4BP and factor H that will be immunized and then challenged with multiple emm types of GAS. The world needs an effective, safe and affordable vaccine to prevent group A streptococcal (GAS) infections. Although most GAS infections are mild, there are more than 18 million people with a chronic complication of a severe GAS disease worldwide, over 15 million of whom have rheumatic heart disease, another 2 million cases of severe disease occur each year and a total of 517,000 deaths annually are estimated to be due to this organism. Vaccine prevention of even a fraction of these life-threatening diseases could have a significant impact on the health of people around the world.",Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine,9965720,R01AI132117,"['Animals', 'Antibodies', 'Bacteria', 'Base Sequence', 'Binding', 'Biological Assay', 'Cell surface', 'Cells', 'Cessation of life', 'Chronic', 'Collection', 'Complement Factor H', 'Complementarity Determining Regions', 'Complication', 'Computer Analysis', 'Data', 'Development', 'Disease', 'Ensure', 'Enzyme-Linked Immunosorbent Assay', 'Epidemiology', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Human', 'Immune', 'Immune Sera', 'Immunity', 'Immunize', 'Immunologics', 'In Vitro', 'Infection', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Mus', 'N-terminal', 'Natural Immunity', 'Organism', 'Oryctolagus cuniculus', 'Peptide Vaccines', 'Peptide antibodies', 'Peptides', 'Population', 'Prevention', 'Process', 'Property', 'Proteins', 'Recombinant Vaccines', 'Recombinants', 'Rheumatic Heart Disease', 'Sequence Analysis', 'Streptococcal Infections', 'Streptococcal Vaccines', 'Streptococcus pyogenes', 'Structure', 'Surface', 'System', 'Testing', 'Transgenic Mice', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'bactericide', 'base', 'cross reactivity', 'design', 'experimental study', 'flexibility', 'hybrid protein', 'immunogenic', 'immunogenicity', 'innovation', 'molecular dynamics', 'multiple myeloma M Protein', 'novel', 'novel strategies', 'peptide structure', 'preclinical study', 'prevent', 'protein aminoacid sequence', 'protein structure', 'retinal S antigen peptide M', 'synthetic peptide', 'tool', 'vaccine development', 'vaccine evaluation']",NIAID,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2020,741796,46216755,0.013057258446658905
"Mobilize Center: Models for Mobile Sensing and Precision Rehabilitation Limited mobility due to conditions like osteoarthritis (OA), cerebral palsy, and Parkinson’s disease affects millions of individuals, at enormous personal and societal cost. Rehabilitation can dramatically improve mobility and function, but current rehabilitation practice requires in-person guidance by a skilled clinician, increasing expense and limiting access. Mobile sensing technologies are now ubiquitous and have the potential to measure patient function and guide treatment outside the clinic, but they currently fail to capture the characteristics of motion required to accurately monitor function and customize treatment. Millions of low-cost mobile sensors are generating terabytes of data that could be analyzed in combination with other data, such as images, clinical records, and video, to enable studies of unprecedented scale, but machine learning models for analyzing these large-scale, heterogeneous, time-varying data are lacking.  To address these challenges, we will establish a Biomedical Technology Resource Center —The Mobilize Center. Through the leadership of an experienced scientific team, we will create and disseminate innovative tools to quantify movement biomechanics with mobile sensors.  Specifically, we will:  1. Push the bounds of what we can measure via wearable sensors using models that compute muscle  and joint forces and metabolic cost of locomotion. These models, based on biomechanical and machine  learning models, will be disseminated via our newly created OpenSense software, which will be used  by thousands of researchers to gain new insights into patient biomechanics using mobile sensors.  2. Meet the need for tools that analyze data about movement dynamics and develop machine learning  models to analyze and generate insights from unstructured, high-dimensional data, including time-  series (e.g., from mobile sensors), images (e.g., MRI), and video (e.g., smartphone video of a patient’s gait).  3. Provide tools needed to intervene in the real-world. We will develop algorithms to accurately quantify  kinematics outside the lab for long durations using data from inertial measurement units (IMUs). We will  also build behavioral models to adapt and personalize goal setting, drawing on movement records from  6 million individuals, as well as health goals and exercise for 1.7 million people.  Through intensive interactions with our Collaborative Projects, we will focus on improving rehabilitation outcomes for individuals with limited mobility due to osteoarthritis, obesity, Parkinson’s disease, and cerebral palsy. The Center’s tools and services will enable researchers to revolutionize how we diagnose, monitor, and treat mobility disorders, providing tools needed to deliver precision rehabilitation at low cost and on a massive scale in the future. Limited mobility due to conditions like osteoarthritis, cerebral palsy, and Parkinson’s affects millions of individuals, at a great cost to public health and personal well-being. Rehabilitation can dramatically improve mobility and function, but current rehabilitation practice requires in- person guidance by a skilled clinician, increasing expense and limiting access. This project will revolutionize how we diagnose, monitor, and treat mobility limitations and enable personalized rehabilitation at low cost and on a massive scale using wearable sensing technology in the future.",Mobilize Center: Models for Mobile Sensing and Precision Rehabilitation,9855893,P41EB027060,"['Address', 'Affect', 'Algorithms', 'Behavioral Model', 'Biomechanics', 'Biomedical Engineering', 'Biomedical Technology', 'Cellular Phone', 'Cerebral Palsy', 'Characteristics', 'Clinic', 'Clinical', 'Communities', 'Computer software', 'Custom', 'Data', 'Data Science', 'Degenerative polyarthritis', 'Diagnosis', 'Disease', 'Documentation', 'Educational workshop', 'Engineering', 'Exercise', 'Exposure to', 'Feedback', 'Foundations', 'Freezing', 'Future', 'Gait', 'Goals', 'Guidelines', 'Home environment', 'Human', 'Image', 'Individual', 'Joints', 'Leadership', 'Literature', 'Locomotion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measurement', 'Measures', 'Metabolic', 'Modeling', 'Monitor', 'Motion', 'Movement', 'Muscle', 'Obesity', 'Parkinson Disease', 'Pathologic', 'Patients', 'Personal Satisfaction', 'Persons', 'Public Health', 'Records', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Resources', 'Series', 'Services', 'Software Tools', 'Time', 'Training', 'Vision', 'base', 'biomechanical model', 'biomedical informatics', 'cohesion', 'coral', 'cost', 'evidence base', 'experience', 'handheld mobile device', 'health goals', 'improved', 'improved functioning', 'improved mobility', 'individualized medicine', 'industry partner', 'innovation', 'insight', 'joint loading', 'kinematics', 'large scale data', 'mHealth', 'mobile computing', 'multidimensional data', 'open source', 'programs', 'sensor', 'sensor technology', 'smart watch', 'societal costs', 'symposium', 'terabyte', 'tool', 'tool development', 'wearable sensor technology']",NIBIB,STANFORD UNIVERSITY,P41,2020,752316,560644462,0.0046177784435561895
"An integrated electrical impedance myography platform for neuromuscular disease classification and diagnosis Project Summary  Improved methods for the bedside diagnosis and evaluation of neuromuscular disorders are needed. One technology that is finding increasing use for this purpose is electrical impedance myography (EIM). In EIM, a very weak, high frequency electrical current is passed through a muscle of interest and the resulting surface voltages are measured. Disease associated alterations in the composition and microstructural features of the muscle produce characteristic changes that can be used to help classify specific conditions and grade disease severity. To date, most studies using EIM analysis have utilized a fairly limited data set for disease assessment. While effective, this approach ignores a great deal of information locked within the impedance data, including those values that can assist in predicting specific muscle features (such as myofiber diameter) and the presence of pathological change (e.g., fat or connective tissue deposition). In addition, as it stands, the data set is challenging for the clinician to understand without a detailed knowledge of impedance theory. Myolex, Inc is a small business concern located in Boston, MA has as its main focus the development of EIM technologies for clinical use. Myolex recently completed a Phase 1 SBIR that demonstrated the potential capability of machine learning based classification algorithms to effectively discriminate healthy muscle from diseased and to discriminate one disease from another. In this proposed work, we will greatly advance this concept by embodying classification algorithms into a powerful new software suite for Myolex’s current EIM system, the mView. Our underlying hypothesis is that EIM data analysis can be automated to the point that classification systems can provide data on disease diagnosis as well as disease severity for improved ease-of-use. We propose to study this hypothesis via 2 specific aims. In Specific Aim 1, we will design a software suite capable of assisting with artifact-free data collection to be incorporated into our current EIM system, the mViewTM. Then using classification paradigms based on a prodigious amount of previous collected data, we will develop an automated data analysis tool to help provide data on disease category as well as microscopic features, muscle based on the impedance data alone using Microsoft’s Azure Cloud platform. In Specific Aim 2, we will test this developed software suite in a total of180 adult and pediatric neuromuscular disease patients and healthy participants evaluated at Ohio State University Wexner Medical Center (adults) and Boston Children’s Hospital (children). During this data collection period, the Ohio State and Boston Children’s researchers will have real- time access to Myolex staff to provide feedback and have questions/problems answered and addressed. The user interface will continue to be refined and classification algorithms improved. At the conclusion of this work, a new diagnostic tool will be developed for potential 510(k) FDA approval. It will serve as the basis for a continuously self-refining system as additional data sets are collected by end-users employing them in regular clinical use. Project Narrative  Electrical impedance myography (EIM) is a valuable technique to assist with the evaluation of a variety of conditions affecting nerve and muscle. However, to date, only simplistic EIM outcomes have been utilized to assess muscle condition. In this proposed work, we will develop a software platform using machine learning to be incorporated into current EIM technology to allow for automated diseased classification and characterization using the entire large EIM data set collected with each muscle measurement. This will serve as the basis for a new, powerful and convenient tool for neuromuscular diagnosis that will continue to advance over time.",An integrated electrical impedance myography platform for neuromuscular disease classification and diagnosis,10002324,R44NS113756,"['Address', 'Adult', 'Affect', 'Algorithmic Software', 'Amyotrophic Lateral Sclerosis', 'Area', 'Back Pain', 'Boston', 'Businesses', 'Caliber', 'Categories', 'Characteristics', 'Child', 'Childhood', 'Classification', 'Clinical', 'Complex', 'Computer software', 'Connective Tissue', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Set', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Duchenne muscular dystrophy', 'Effectiveness', 'Electrodes', 'Ensure', 'Evaluation', 'Fatty acid glycerol esters', 'Feedback', 'Fiber', 'Frequencies', 'Functional disorder', 'Health', 'Inclusion Body Myositis', 'Individual', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Medical Records', 'Medical Technology', 'Medical center', 'Methods', 'Microscopic', 'Morphologic artifacts', 'Muscle', 'Muscular Dystrophies', 'Myography', 'Myopathy', 'Myositis', 'Nerve', 'Neuromuscular Diseases', 'Neuromuscular conditions', 'Ohio', 'Outcome', 'Participant', 'Pathologic', 'Patients', 'Pediatric Hospitals', 'Performance', 'Phase', 'Physicians', 'Play', 'Positioning Attribute', 'Provider', 'Radiculopathy', 'Research Personnel', 'Role', 'Severities', 'Severity of illness', 'Small Business Innovation Research Grant', 'Specific qualifier value', 'Spinal Muscular Atrophy', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Universities', 'Work', 'advanced analytics', 'base', 'classification algorithm', 'cloud based', 'cloud platform', 'commercialization', 'complex data ', 'data acquisition', 'design', 'diagnosis evaluation', 'disease classification', 'disease diagnosis', 'electric impedance', 'feature extraction', 'improved', 'indexing', 'interest', 'machine learning algorithm', 'method development', 'nerve injury', 'neuromuscular', 'novel diagnostics', 'pediatric patients', 'physical therapist', 'prototype', 'sarcopenia', 'software development', 'success', 'theories', 'tool', 'usability', 'user friendly software', 'user-friendly', 'voltage']",NINDS,"MYOLEX, INC.",R44,2020,869698,869698,0.006769855608420727
"West Coast Metabolomics Center for Compound Identification Project Summary – Overall West Coast Metabolomics Center for Compound Identification (WCMC) The West Coast Metabolomics Center for Compound Identification (WCMC) is committed to the overall goals of the NIH Common Fund Metabolomics Initiative and specifically aims to largely improve small molecule identifications. Understanding metabolism is important to gain insight into biochemical processes and relevant to battle diseases such as cancer, obesity and diabetes. Compound identification in metabolomics is still a daunting task with many unknown compounds and false positive identifications. The major goal of the WCMC is therefore to develop processes and resources that accelerate and improve the accuracy of the compound identification workflow for experts and medical professionals. The WCMC for Compound Identification is structured in three different entities: the Administrative Core, the Computational Core and the Experimental Core. The Center is led by the Director Prof. Fiehn in close collaboration with quantum chemistry experts Prof. Wang and Prof. Tantillo, and metabolomics experts Dr. Barupal and Dr. Kind with broad support from mass spectrometry, computational metabolomics and programming experts. The Administrative Core will assist the Computational and Experimental Core to develop and validate large in-silico mass spectral libraries, retention time prediction models and innovative methods for constraining and ranking lists of isomers in an integrated process of cheminformatics tools and databases. The developed tools and databases will be made available to all Common Fund Metabolomics Consortium (CF-MC) members and professional working groups. The WCMC will also provide guidance for compound identification to the National Metabolomics Data Repository. The broad dissemination of developed compound identification protocols, training for compound identification workflows, databases and distribution of internal reference standard kits for metabolomic standardization will overall widely support the metabolomics community. Project Narrative – Overall West Coast Metabolomics Center for Compound Identification (WCMC) Understanding metabolism is relevant to find both markers and mechanisms of diseases and health phenotypes, including obesity, diabetes, and cancer. The West Coast Metabolomics Center for Compound Identification at UC Davis will use advanced experimental and computational mass spectrometry methods to significantly improve compound identification rates in metabolomics. Such identification will lead to breakthroughs in more precise diagnostics as well as finding the causes of diseases.",West Coast Metabolomics Center for Compound Identification,9965942,U2CES030158,"['Achievement', 'Amines', 'Benchmarking', 'Biochemical Process', 'Biodiversity', 'Biological Assay', 'Blinded', 'Chemicals', 'Chemistry', 'Collaborations', 'Communication', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Reporting', 'Databases', 'Deuterium', 'Diabetes Mellitus', 'Disease', 'Ensure', 'Enzymes', 'Finding by Cause', 'Funding', 'Goals', 'Guidelines', 'Health', 'Hybrids', 'Hydrogen', 'Isomerism', 'Leadership', 'Libraries', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mass Chromatography', 'Mass Fragmentography', 'Mass Spectrum Analysis', 'Medical', 'Metabolism', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Monitor', 'North America', 'Obesity', 'Phenotype', 'Policies', 'Process', 'Protocols documentation', 'Reaction', 'Reference Standards', 'Research Design', 'Resolution', 'Resources', 'Software Tools', 'Solvents', 'Standardization', 'Structure', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Vendor', 'Vertebral column', 'base', 'chemical standard', 'cheminformatics', 'computing resources', 'data acquisition', 'data warehouse', 'database design', 'deep learning', 'heuristics', 'improved', 'in silico', 'innovation', 'insight', 'member', 'metabolomics', 'model building', 'molecular dynamics', 'novel', 'organizational structure', 'personalized diagnostics', 'predictive modeling', 'quantum chemistry', 'repository', 'small molecule', 'tool', 'training opportunity', 'working group']",NIEHS,UNIVERSITY OF CALIFORNIA AT DAVIS,U2C,2020,989602,254622553,-0.0007603717306627328
"Structure based design of trimer interface epitope focused universal influenza vaccines The “Computational Models of Immunity” projects in this application focus on development and implementation of new structure-based design tools for influenza hemagglutinin (HA) protein trimer interface specific antibodies or vaccine antigens. These projects will use knowledge about the structure and function of human neutralizing antibodies to the trimer interface of the HA head that we have in hand or will discover, in order to design new antibodies or vaccines in silico. We have access to peripheral blood cells from a diverse panels of subjects with prior natural infection, or exposure to experimental inoculation with vaccines encoding HA molecules with both seasonal vaccines and unusual experimental influenza subtypes, including H3variant, H5, H6, H7, H9, and H10 viruses. The immune B memory cell populations from these individuals are the ideal starting materials to isolate unusual heterosubtypic antibodies. Recently, we identified the HA head trimer interface as a major new site of vulnerability for universal influenza antibodies and candidate vaccines. Here, we will study existing and isolate additional broadly heterosubtypic human antibodies to the trimer interface of the HA head. We will determine the immunome of the responding heterosubtypic clones using high-throughput next generation sequencing of antibody gene repertoires that comprise the clonal lineages of the most heterosubtypic antibodies isolated. Once antibodies with unusual breadth or activity are isolated, the structure of these antibodies will be determined in complex with purified HA molecules in the Structural Core using crystallography and single particle electron microscopy (EM) studies. Such structures will provide the coordinates for the modeling experiments using Rosetta. We will in silico mature human antibodies to increase affinity for the HA antigen of specific virus types and use multi-state design to maximize breadth, i.e., create antibodies that recognize HAs of all clades, subtypes, groups, or even types. We then will synthesize and express these novel antibodies and determine neutralization activity, binding affinity, and competition binding groups of designed antibodies, using a diverse HA panel and pseudotyped viruses with all type A HAs in nature. The co-crystal structure of these human antibodies with HA will be the template for in silico design of structurally stable epitope-focused immunogens. We will first validate these designed immunogens by testing the interaction with the target human antibodies. Further, these immunogens will be experimentally tested by evaluating immune responses. Then, we will use the novel immunogens to isolate new antibodies from subjects naturally exposed to influenza, to show that the immunogens present antigens recognized by natural immune responses. The next generation of viral vaccines and biologics alike will be designed rationally, based on a structural understanding of how protective antibodies engage the epitope of the target. The multidisciplinary group in this application will develop and implement new structure based computational models, and then validate the power of the computational design approach with laboratory experiments focused on the structural basis of broad neutralization of influenza through recognition of the a novel site of vulnerability in the interface of the hemagglutinin head domain.",Structure based design of trimer interface epitope focused universal influenza vaccines,9950456,U01AI150739,"['Affinity', 'Antibodies', 'Antibody Repertoire', 'Antigens', 'B-Lymphocytes', 'Binding', 'Binding Sites', 'Biological', 'Blood Cells', 'Cells', 'Complex', 'Computer Models', 'Crystallization', 'Crystallography', 'Development', 'Electron Microscopy', 'Engineering', 'Epitopes', 'Exposure to', 'Generations', 'Genes', 'Glycoproteins', 'Goals', 'Hand', 'Head', 'Hemagglutinin', 'Human', 'Immune', 'Immune response', 'Immunity', 'In Vitro', 'Individual', 'Infection', 'Influenza', 'Influenza Hemagglutinin', 'Influenza vaccination', 'Knowledge', 'Laboratories', 'Linear Programming', 'Machine Learning', 'Maps', 'Masks', 'Mass Spectrum Analysis', 'Memory B-Lymphocyte', 'Methods', 'Modeling', 'Nature', 'Polysaccharides', 'Population', 'Proteins', 'Research', 'Research Project Grants', 'Roentgen Rays', 'Sequence Analysis', 'Site', 'Standardization', 'Structure', 'Techniques', 'Testing', 'Therapeutic antibodies', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'Validation', 'Viral', 'Viral Vaccines', 'Virus', 'Virus Diseases', 'X-Ray Crystallography', 'antibody engineering', 'base', 'combat', 'cross reactivity', 'design', 'experimental study', 'immunogenicity', 'in silico', 'in vivo', 'influenza virus vaccine', 'influenzavirus', 'innovation', 'laboratory experiment', 'molecular recognition', 'mouse model', 'multidisciplinary', 'nanoparticle', 'neutralizing antibody', 'next generation', 'next generation sequencing', 'novel', 'pandemic disease', 'particle', 'programs', 'protective efficacy', 'receptor binding', 'response', 'scaffold', 'screening', 'structural biology', 'therapeutic vaccine', 'tool', 'universal influenza vaccine', 'vaccine candidate']",NIAID,VANDERBILT UNIVERSITY MEDICAL CENTER,U01,2020,1265006,377931988,0.0008459400287211496
"Overall: Eunice Kennedy Shriver Intellectual and Developmental Disabilities Research Center at Vanderbilt Founded in 1965 as one of the original Intellectual and Developmental Disorders Research Centers (IDDRC), the Vanderbilt Kennedy Center (VKC) IDDRC serves as the central nexus across Vanderbilt for interdisciplinary research, communication, and training in intellectual and developmental disabilities (IDD). The VKC IDDRC serves as a trans-institutional institute that brings together over 200 faculty from 38 departments in 10 schools at Vanderbilt. The VKC’s mission to facilitate discoveries that inform best practices to improve the lives of people with IDD and their families. This mission is met by leveraging our outstanding institutional resources and support, partnering with disability communities, and capitalizing on synergistic interactions across the VKC’s federally-designated centers: the VKC IDDRC, a University Center of Excellence in Developmental Disabilities and a Leadership Education in Neurodevelopmental Disabilities program. The IDDRC as the centerpiece of the VKC is the foundational organizing structure that creates a “Center culture” wherein research and discovery permeates the VKC’s broader training and service activities, thus enhancing the translational research goals of the IDDRC. Demonstrable IDDRC success includes 976 investigator- authored publications and robust NIH funding to Vanderbilt to support IDD-related research ($52.6M in FY20). Harnessing and leveraging this trans-institutional strength to focus on unique challenges in IDD, the overarching goal of the next phase of the IDDRC is to develop precision care for IDD by providing infrastructure and scientific leadership to enable rapid translation of basic discoveries into high- impact IDD interventions and treatments. Three global Aims guide the IDDRC’s work. Aim 1 provides core services to enable and disseminate impactful research on individualizing treatments based upon the causes, mechanisms, and contributing co-morbid sequelae of IDD; Aim 2 focuses on incorporating innovative methods and approaches to enhance multidisciplinary IDD research; and Aim 3 proposes to conduct a signature research project to improve the precision use of antipsychotic medication in people with autism. Across these Aims and five Cores supported by the IDDRC (Administrative, Clinical Translational, Translational Neuroscience, Behavioral Phenotyping, and Data Sciences), three themes permeate our work: (1) recruitment of highly-skilled researchers not currently conducting IDD research (non-traditional researchers); (2) inclusion of IDD participants into research studies that currently do not include IDD (non-traditional subjects); and (3) incorporation of novel scientific approaches and methods (non-traditional approaches). Our IDDRC is ideally posed to enable rapid discovery of precision care approaches by supporting 50 investigators leading 70 research projects (15 from NICHD) and, as highlighted by the Signature Research Project, to promote and implement generative, novel, and impactful research directions, thus meeting the NICHD’s vision of applying newly evolved technologies and approaches to rapidly accelerate the prevention and/or amelioration of IDDs. PUBLIC HEALTH RELEVANCE: As a group, intellectual and developmental disabilities, including Down syndrome and autism spectrum disorder, have dramatic effects on affected people’s and their caregiver’s lives. Unfortunately, there remains a lack of understanding about what causes these disabilities and, critically, how to treat them with targeted therapies. The Vanderbilt Kennedy Center’s Intellectual and Developmental Disabilities Research Center serves as the hub for Vanderbilt’s research efforts focusing on improving the lives of people with intellectual and developmental disabilities by understanding the causes of these disorders and developing and testing therapies tailored to each individual’s precise needs.",Overall: Eunice Kennedy Shriver Intellectual and Developmental Disabilities Research Center at Vanderbilt,10085550,P50HD103537,"['Academic Medical Centers', 'Affect', 'Antipsychotic Agents', 'Basic Science', 'Behavioral', 'Biomedical Research', 'Caregivers', 'Caring', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Communication', 'Communities', 'Computerized Medical Record', 'Data', 'Data Science', 'Development', 'Developmental Disabilities', 'Diagnosis', 'Disease', 'Disease model', 'Down Syndrome', 'Education', 'Evaluation', 'Faculty', 'Family', 'Foundations', 'Funding', 'Future', 'Gap Junctions', 'Genotype', 'Goals', 'Image', 'Individual', 'Infrastructure', 'Institutes', 'Intellectual and Developmental Disabilities Research Centers', 'Intellectual functioning disability', 'Interdisciplinary Study', 'Intervention', 'Leadership', 'Longevity', 'Machine Learning', 'Medical Records', 'Methods', 'Mission', 'Modeling', 'National Institute of Child Health and Human Development', 'Neurodevelopmental Disability', 'Obesity', 'Outcome', 'Participant', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Phase', 'Pilot Projects', 'Policy Research', 'Prevention', 'Problem behavior', 'Publications', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk', 'Sampling', 'Schools', 'Series', 'Services', 'Structure', 'Techniques', 'Technology', 'Testing', 'Training', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'Universities', 'Vision', 'Weight Gain', 'Work', 'autism spectrum disorder', 'base', 'behavioral phenotyping', 'clinical translation', 'comorbidity', 'cost effective', 'developmental disease', 'disability', 'drug-induced weight gain', 'experience', 'image processing', 'implementation science', 'improved', 'individualized medicine', 'innovation', 'large datasets', 'lectures', 'meetings', 'multidisciplinary', 'novel', 'personalized approach', 'personalized care', 'personalized medicine', 'population based', 'pragmatic trial', 'predictive modeling', 'programs', 'public health relevance', 'recruit', 'research study', 'success', 'targeted treatment', 'translational neuroscience', 'trial comparing']",NICHD,VANDERBILT UNIVERSITY MEDICAL CENTER,P50,2020,1387605,377931988,-0.020887036550584524
"High-Performance Compute Cluster for Comprehensive Cancer and Infectious Diseases Research Project Summary/Abstract Fred Hutch respectfully requests funds to upgrade the current 523-node, 3328-core high performance computing (HPC) cluster, which was created in 2004 and expanded in 2006, 2010 (S10 funds), 2013, 2015 (S10 funds), and 2018. 456 end of life nodes (1824 cores) will be replaced and 144 nodes (3456 cores) will be added, creating a new 211-node, 4996-core system with an overall 50% increase in core count, 60% increase in processing power, and more than 100% increase in memory capacity over the old system. The expanded capacity will enable deep and efficient analysis of our research studies and accommodate 20% annual growth in computing intensive research, much of which is not possible on the current cluster. The core user group for the new HPC cluster consists of at least 37 NIH-funded research groups participating in this proposal, however as much as 85 groups use the cluster regularly. Their biomedical research is aimed at eradicating cancer and other diseases and dependent on computationally intensive technical approaches such as development of novel statistical analysis or machine learning methods, for example for assessing immune correlates to facilitate vaccine development, analyzing large scale clinical trials or to develop software tools for the analysis of large-scale immunological datasets, DNA and RNA sequencing, modeling prostate cancer outcomes, studies of the human microbiome, modeling of cancers, mRNA, miRNA, and structural variant detection, structural biology with Cryo- EM, modeling of infectious agents and pandemics, computational modeling, prediction and design of macromolecular structures and interactions, identifying drivers of neoplasia and an international consortium improving colorectal cancer detection using GWAS, whole genome sequencing and genome-wide gene- environment (GxE) studies as well as research in diabetes, mhealth and cardiovascular diseases. Several of the Major Users at Fred Hutch are currently experiencing substantial delays in accomplishing their work using the current cluster. Others have projects that cannot be done at all on the existing instrument. (see Research Projects section for details). The Scientific Computing department (SciComp) has operated the current HPC cluster for more than 10 years and has a staff with a combined experience of over 150 years. The proposed new HPC cluster will be installed in available space in a Fred Hutch datacenter. The expanded cluster will address both immediate and future needs of our user community, supporting NIH-funded research at Fred Hutch. Funded research at our Center will greatly benefit from the increased data-processing capacity and improved performance of the requested HPC cluster, including applications of machine learning to the study of clinical trial efficacy, comparison of immune system receptors to identify responses to specific pathogens/diseases, modeling of carcinogenesis. Besides multiple infectious diseases Fred Hutch researches all types of cancer and our computationally intensive investigators tend to focus on colon, prostate, brain, Barrett’s esophagus, lung and liquid tumors such as leukemia. Project Narrative (Public Health Relevance) We are requesting an expansion of our high-performance computing (HPC) cluster to provide capacity for the growing computational needs in a broad range of biomedical research studies at our Center. Access to fast and reliable computational power is critically important for analyzing the exploding amounts of data produced by large-scale clinical and epidemiological studies, as well as scientific instruments such as genome sequencers and electron microscopes. The prevention, detection, and treatment of cancer, HIV, and other life-threatening diseases are major areas of NIH-funded research at our Center that will greatly benefit from the improved performance of the requested HPC cluster, including large-scale clinical and epidemiologic studies of various cancers, modeling of vaccine efficacy, and infectious disease transmission and proteomic-based biomarker discovery.",High-Performance Compute Cluster for Comprehensive Cancer and Infectious Diseases Research,9940345,S10OD028685,"['Address', 'Barrett Esophagus', 'Biomedical Research', 'Brain', 'Cancer Model', 'Cardiovascular Diseases', 'Clinical Trials', 'Colon', 'Communicable Diseases', 'Communities', 'Computer Models', 'Cryoelectron Microscopy', 'DNA sequencing', 'Data Set', 'Development', 'Diabetes Mellitus', 'Disease', 'Disease model', 'Environment', 'Funding', 'Future', 'Genes', 'Growth', 'High Performance Computing', 'Human Microbiome', 'Immune', 'Immune system', 'Immunologics', 'Infectious Agent', 'Infectious Diseases Research', 'International', 'Liquid substance', 'Lung', 'Machine Learning', 'Malignant Neoplasms', 'Memory', 'Messenger RNA', 'MicroRNAs', 'Modeling', 'Molecular Structure', 'Neoplasms', 'Performance', 'Prostate', 'Prostate Cancer Outcomes Study', 'Research', 'Research Personnel', 'Research Project Grants', 'Software Tools', 'Statistical Data Interpretation', 'Structure', 'System', 'United States National Institutes of Health', 'Work', 'cancer type', 'carcinogenesis', 'cluster computing', 'colorectal cancer screening', 'computerized data processing', 'design', 'efficacy clinical trial', 'end of life', 'experience', 'genome sequencing', 'genome wide association study', 'genome-wide', 'improved', 'instrument', 'leukemia', 'mHealth', 'machine learning method', 'microbiome research', 'novel', 'pandemic disease', 'pathogen', 'prostate cancer model', 'receptor', 'research study', 'response', 'scientific computing', 'software development', 'structural biology', 'transcriptome sequencing', 'tumor', 'vaccine development', 'variant detection', 'whole genome']",OD,FRED HUTCHINSON CANCER RESEARCH CENTER,S10,2020,2000000,758431960,-0.04106482521511584
"High-throughput discovery of protein energy landscapes in natural and designed proteomes Project Summary All folded proteins continuously fluctuate between their low-energy native structures and higher-energy “hidden” conformations that can be partially or fully unfolded. Although each protein molecule passes through its high- energy conformations only a small fraction of the time, these states have major physiological consequences. Partially-folded states of natural proteins can lead to protein aggregation, organ failure, and death. Partially- folded states of therapeutic proteins can induce dangerous anti-drug antibodies. The energetic balance between the native, folded state and these diverse higher-energy states – in other words, the overall “energy landscape” – is thus critically important in protein aggregation and immunogenicity, as well as in allostery, signaling, off- target drug interactions and numerous other phenomena. Despite decades of research into energy landscapes our overall understanding is very limited: few proteins have been characterized in depth, accurate computational predictions are very challenging, and experimental measurements are expensive, slow, and labor-intensive. We propose a transformational approach to understand protein energy landscapes by integrating a new massively parallel experimental method, machine learning, and protein design. First, we are developing a new high-throughput assay using hydrogen exchange mass spectrometry to measure energy landscapes for thousands of proteins in parallel. This method finally brings the study of protein energy landscapes into the “omics” age. Critically, these experiments reveal both the overall folding stability and the energies of conformational fluctuations in each protein. For a subset of proteins (tens to hundreds), these parallel experiments reveal the specific sites of conformational fluctuations as well. Using this approach, we will measure the energy landscapes of thousands of natural proteins and tens of thousands of computationally designed proteins custom-built to systematically probe how specific properties affect energy landscapes. We will then train machine learning models to predict energy landscapes from sequence and structure, as well as optimize physical force fields to accurately model high-energy protein states. We will also catalyze advances in modeling throughout the community by organizing large-scale competitions at blind prediction of energy landscapes. Finally, with these new predictive models in hand, we will pursue a unique application: the development of energetically-optimized screening libraries for therapeutic protein and biological probe discovery. This overcomes a major challenge in drug and probe development. In sum, this study provides the experimental and computational tools to bring hidden protein states to light quantitatively on a massive scale. This fundamentally shifts our perspective: instead of examining energy landscapes only when they cause problems, we can make energy landscape analysis a central tool in biology and bioengineering. Project Narrative All proteins continuously move between their folded state and “hidden” higher-energy states that can be partially or fully unfolded. These hidden states have an important impact on how proteins interact, how they become activated, and whether they aggregate. Here, we combine protein design with a totally new high-throughput experimental approach to reveal these high-energy states for thousands of proteins in parallel, and apply these advances to build a new generation of computational models of protein energetics.",High-throughput discovery of protein energy landscapes in natural and designed proteomes,10002881,DP2GM140927,"['Affect', 'Age', 'Antibodies', 'Biological', 'Biology', 'Biomedical Engineering', 'Cessation of life', 'Communities', 'Computer Models', 'Custom', 'Dangerousness', 'Development', 'Drug Interactions', 'Drug Targeting', 'Equilibrium', 'Generations', 'Hand', 'Hydrogen', 'Lead', 'Libraries', 'Light', 'Machine Learning', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Molecular Conformation', 'Organ failure', 'Pharmaceutical Preparations', 'Physiological', 'Property', 'Protein Engineering', 'Proteins', 'Proteome', 'Research', 'Signal Transduction', 'Site', 'Structure', 'Sum', 'Time', 'Training', 'blind', 'computerized tools', 'design', 'experimental study', 'high throughput screening', 'immunogenicity', 'predictive modeling', 'protein aggregation', 'protein folding', 'screening', 'therapeutic protein', 'tool']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,DP2,2020,2347906,367414121,0.008344004258390927
"Application of Advanced Quantitative Methods to Schizophrenia Research in Macedonia PROJECT SUMMARY  Abnormalities of white matter are important in schizophrenia. A preponderance of studies have found decreased levels of transcripts for myelin-related proteins in autopsy brains. Some have found decrease in the proteins themselves, and some have not. Hundreds of diffusion tensor imaging (DTI) studies have found reduced fractional anisotropy (FA) in the brains of many people with schizophrenia (SCH). Decreased FA is interpreted as disruption of normal architecture. However, postmortem examination has failed to identify characteristic abnormalities. This suggests that abnormalities are subtle, and perhaps postmortem examinations have not used the right tools to find them. We have therefore been developing, as part of a collaboration supported by our concluding Fogarty project, two new methods to characterize white matter at high resolution. The first is a machine learning protocol to measure axonal diameters and myelin sheath thickness in electron microscope (EM) images of prefrontal white matter, recognizing and avoiding artifacts in EM of autopsy tissue. This will enable us to measure thousands of fibers in EM images produced as part of our concluding Fogarty project, from individuals with SCH, major depressive disorder (MDD), or no psychiatric illness (NPI). The second method, suggested by the DTI findings, is to analyze the arrangement of the axons themselves. We will use 3-dimensional (3D) reconstructions of high-resolution images of the axons themselves, identified by Bielschowsky silver stain or immunohistochemistry for phosphorylated neurofilament protein. To obtain high-resolution images of Bielschowsy stains, we will take advantage of the recent observation by Dr. Mark Sonders, co-investigator on this project, that these and other heavy metal stains luminesce under 2-photon infrared excitation. This yields clear and measurable images of individual axons. We will perform these procedures on sections from existing paraffin blocks that comprise a complete left prefrontal coronal section from 36 triads containing 1 case each of SCH, MDD, or NPI, matched for sex and age. These brains were included in earlier studies that yielded data on protein composition, mRNA for myelin-related proteins, DNA methylation, microglial activation, and semiquantitative myelin histology. In a third, exploratory aim, we will employ graphical models to combine these various types of data with known properties of CNS white matter and myelin to build a model of what is disturbed in schizophrenia. We expect that novel techniques for data fusion will reveal associations based on multidimensional correlations that could not be detected by modeling the single-domain datasets separately. In the process of completing these scientific aims, we will pursue the pedagogic goals of training the first two professional biostatisticians in Macedonia, and an academic pathologist. We will also hold a seminar course for biological researchers to build awareness and understanding of the power of biostatistical and other computational methods to enrich their research. NARRATIVE Our ongoing Fogarty/NIMH research project in Macedonia (R01 MH060877, “Building Schizophrenia Research in Macedonia”), has demonstrated biochemical abnormalities of white matter in schizophrenia that are not present in major depressive disorder. However, we have not seen anatomical abnormalities of white matter, which MRI studies of schizophrenia tell us should exist, and as the biochemistry also suggests. To explore white matter in novel ways, we are developing new methods of microscopy, image analysis and statistical inference, which we now propose to employ on a large scale.",Application of Advanced Quantitative Methods to Schizophrenia Research in Macedonia,9953486,R56MH117769,"['3-Dimensional', 'Academy', 'Age', 'Anisotropy', 'Architecture', 'Autopsy', 'Awareness', 'Axon', 'Biochemical', 'Biochemistry', 'Biological', 'Biological Assay', 'Biometry', 'Brain', 'Caliber', 'Cerebrum', 'Characteristics', 'Charge', 'Collaborations', 'Complex', 'Computer-Assisted Diagnosis', 'Computers', 'Computing Methodologies', 'Confocal Microscopy', 'DNA Methylation', 'Data', 'Data Set', 'Deformity', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electron Microscope', 'Electrons', 'Fiber', 'Goals', 'Heavy Metals', 'Histologic', 'Histology', 'Image', 'Image Analysis', 'Immunohistochemistry', 'Individual', 'Informatics', 'International', 'Knowledge', 'Learning', 'Left', 'Macedonia', 'Machine Learning', 'Magnetic Resonance Imaging', 'Major Depressive Disorder', 'Measurable', 'Measurement', 'Measures', 'Medical', 'Mental disorders', 'Messenger RNA', 'Methodology', 'Methods', 'Microscopy', 'Modeling', 'Modernization', 'Morphologic artifacts', 'Morphology', 'Multiomic Data', 'Myelin', 'Myelin Sheath', 'National Institute of Mental Health', 'Neurofilament Proteins', 'Paraffin', 'Pathologist', 'Pathology', 'Positioning Attribute', 'Procedures', 'Process', 'Property', 'Proteins', 'Proteomics', 'Protocols documentation', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Schizophrenia', 'Scientist', 'Silver Staining', 'Stains', 'Statistical Data Interpretation', 'Statistical Methods', 'Structural Models', 'Students', 'Techniques', 'Thick', 'Time', 'Tissues', 'Training', 'Transcript', 'Translational Research', 'Triad Acrylic Resin', 'base', 'cognitive function', 'computerized', 'computerized tools', 'data modeling', 'deep neural network', 'diffusion anisotropy', 'high resolution imaging', 'histological image', 'histological studies', 'imaging study', 'innovation', 'interest', 'low and middle-income countries', 'microscopic imaging', 'multidimensional data', 'multimodality', 'network models', 'novel', 'pedagogy', 'reconstruction', 'sex', 'tool', 'two photon microscopy', 'two-photon', 'water diffusion', 'white matter']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R56,2019,10000,68331629,0.003381688566867911
"Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)? PROJECT SUMMARY The North Coast Conference on Precision Medicine is a national annual mid-sized conference series held in Cleveland, Ohio. The conference series aims to serve as a venue for the continuing education and exchange of scientific ideas related to the rapidly evolving and highly interdisciplinary landscape that is precision medicine research. The topics for each conference coincide with the national conversation and research agenda set by national research programs focused on precision medicine. The 2018 conference is a symposium that will focus on issues related to return of genomic results both in clinical and research settings with an emphasis on diverse populations. The conference will be organized as a traditional format with invited speakers from among national experts for topics ranging from issues returning research results to culturally diverse participants and family members, inclusion of diverse patient and participant populations in the Clinical Sequencing Evidence- Generating Research (CSER) consortium and the Trans-Omics for Precision Medicine (TOPMed) Program, pharmacogenomics-guided dosing and race/ethnicity, strategies used to return results, among others. 2019 and beyond conference topics are being considered from previous symposia attendees and trends in precision medicine research. Odd-numbered year conferences include a workshop component that has previously covered outcome and exposure variable extraction from electronic health records. Future workshop topics being considered include integration of multiple ‘omics, drug response in different populations, pharmacogenomics clinical implementation, precision medicine in cancer, data sharing and informed consent, and the use of apps for recruitment, diagnosis, follow-up, and treatment. Our second major objective of this conference series is the promotion of diversity in the biomedical workforce. It is well-known that the pipeline from training to full professor for women in biomedical research is leaky whereas the pipeline for under-represented minorities is practically non-existent. Drawing from national and local sources, we vet women and under-represented minorities for every invited speaker opportunity, thereby providing valuable career currency and networking opportunities. We will also encourage women and under-represented minorities, particularly at the trainee level, to attend and participate in this conference series to spur interest in pursuing precision medicine research as a career. Overall, the North Coast Conference on Precision Medicine series is a valuable addition to the national conference landscape, and with its unique location and low cost to participants, will serve as an important educational opportunity as precision medicine research accelerates in earnest. PROJECT NARRATIVE The North Coast Conference on Precision Medicine is a yearly fall conference series in Cleveland, Ohio designed as a continuing education forum in the burgeoning area of precision medicine research. The conference brings together national experts on a host of topics ranging from bioethics to bioinformatics to biomedical informatics to speak and lead workshops on timely challenges posed in translating complex genomic and health data into clinical practice. The conference series also serves to promote diversity in the biomedical workforce. This year’s symposium will focus issues related to return of genomic results in both clinical and research settings with an emphasis on diverse populations.",Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)?,9762963,R13HG010286,"['Academic Medical Centers', 'Acceleration', 'African American', 'Area', 'Back', 'Big Data', 'Bioethics', 'Bioinformatics', 'Biomedical Research', 'Clinic', 'Clinical', 'Clinical Research', 'Complex', 'Computational Biology', 'Computer Simulation', 'Continuing Education', 'Custom', 'Data', 'Databases', 'Diagnosis', 'Dose', 'Educational workshop', 'Electronic Health Record', 'Ensure', 'Ethnic Origin', 'Family member', 'Funding', 'Future', 'Generations', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health system', 'Healthcare Systems', 'Hospitals', 'Incidental Findings', 'Informed Consent', 'Infrastructure', 'Institution', 'Knowledge', 'Lead', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mining', 'Names', 'Ohio', 'Outcome', 'PMI cohort', 'Participant', 'Pathogenicity', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Population', 'Population Heterogeneity', 'Prevention', 'Process', 'Race', 'Research', 'Research Personnel', 'Resources', 'Schedule', 'Science', 'Series', 'Source', 'Surveys', 'Technology', 'Time', 'Training', 'Trans-Omics for Precision Medicine', 'Translating', 'Travel', 'Underrepresented Groups', 'Underrepresented Minority', 'United States', 'United States Centers for Medicare and Medicaid Services', 'Variant', 'Veterans', 'Woman', 'base', 'big biomedical data', 'biomedical informatics', 'career', 'clinical care', 'clinical implementation', 'clinical practice', 'clinical sequencing', 'clinically relevant', 'cost', 'cost effective', 'data sharing', 'design', 'falls', 'follow-up', 'forging', 'frontier', 'genome-wide', 'genomic data', 'health data', 'health disparity', 'health information technology', 'incentive program', 'individual patient', 'interest', 'medical specialties', 'multiple omics', 'patient population', 'point of care', 'posters', 'precision medicine', 'programs', 'recruit', 'response', 'science education', 'senior faculty', 'symposium', 'trend']",NHGRI,CASE WESTERN RESERVE UNIVERSITY,R13,2019,10000,197030888,0.003619473303932831
"Proteomics of ciliopathy protein complexes Summary Ciliopathies are a diverse class of developmental diseases that can manifest as defects in kidney, skeletal, eye, heart, reproductive, or mental function. Many ciliopathies, including Bardet-Biedl, Joubert, and Meckel Gruber Syndromes, are caused by defects in the major protein complexes that characterize the ciliary compartment. At least 1,000 proteins are associated with cilia biogenesis and function; uncovering the principles of ciliary protein organization thus requires a large scale, systematic investigation. Our current human protein complex maps, while extensive, have only moderate coverage of ciliary proteins, warranting the collection of targeted experimental datasets. I will use comparative proteomic analysis of cilia from multiple organisms to determine deeply conserved protein-protein interactions likely to be critical to ciliary function in humans. Examining human biological systems in the context of their level of conservation across species is a productive route to distinguish biological signal from noise. My preliminary data on three ciliated species confirm that this approach is capable of identifying conserved ciliary protein complexes. If a pair of proteins are found in physical contact in diverse species, having survived speciation events and gene loss, it can be predicted that this physical interaction is important. First, using mass spectrometry, we will experimentally detect protein complexes within the cilia across a spectrum of eukaryotic organisms, broadly defining conserved ciliary proteins. Second, we will integrate our own experimental data, some of which we have already collected, with outside data to construct a system-wide map of conserved ciliary protein complexes. These conserved ciliary protein complexes are strongly predicted to be functionally important in humans, and potentially involved in human birth defects. Finally, we will test a targeted subset of proteins predicted to associate with known birth defect proteins in vivo using Xenopus laevis embryos as a model system. We have four candidate genes in hand, and will consider candidates from the prior aims as appropriate. This project will lead to a map of critical conserved ciliary protein complexes, a future primary resource for research into diverse ciliopathies. Project Narrative! Ciliopathies are a class of birth defects caused by defective cilia. This project will determine conserved ciliary protein complexes in a series of targeted proteomics experiments, systematically mapping ciliary protein organization as a guide for research into ciliopathies and cilia function.",Proteomics of ciliopathy protein complexes,9588803,F31GM123683,"['Animals', 'Biochemical', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Candidate Disease Gene', 'Cilia', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collection', 'Comparative Biology', 'Complex', 'Congenital Abnormality', 'Core Protein', 'Data', 'Data Set', 'Databases', 'Defect', 'Development', 'Disease', 'Embryo', 'Epithelium', 'Event', 'Eye', 'Fractionation', 'Future', 'Genes', 'Genetic Diseases', 'Genetic screening method', 'Hand', 'Health', 'Human', 'Human Genetics', 'Investigation', 'Joubert syndrome', 'Kidney', 'Knock-out', 'Knockout Mice', 'Link', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Meckel-Gruber syndrome', 'Methods', 'Modeling', 'Molecular Biology', 'Mus', 'Mutation', 'Noise', 'Organism', 'Outcome', 'Pattern', 'Phenotype', 'Plant Proteins', 'Plants', 'Process', 'Protein-Protein Interaction Map', 'Proteins', 'Proteomics', 'Research', 'Resources', 'Route', 'Sampling', 'Series', 'Signal Transduction', 'System', 'Techniques', 'Testing', 'Whole Organism', 'Xenopus', 'Xenopus laevis', 'base', 'biological systems', 'ciliopathy', 'cilium biogenesis', 'comparative', 'developmental disease', 'exome', 'experimental study', 'heart function', 'in vivo', 'link protein', 'mental function', 'novel', 'protein complex', 'protein protein interaction', 'reproductive function', 'skeletal']",NIGMS,"UNIVERSITY OF TEXAS, AUSTIN",F31,2019,35616,91740242,0.03185310945620639
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9747977,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'deep learning algorithm', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2019,115051,685608202,0.006598087424920724
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. n/a",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9882672,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2019,149810,340417756,-0.022306365766466156
"Classifying addictions using machine learning analysis of multidimensional data ABSTRACT This Independent Scientist Award will significantly enhance my research capabilities, enabling me to become a leading quantitative investigator in the field of substance use disorders (SUDs). Specifically, it will allow me to increase my knowledge in the areas of SUD phenotypes, treatment and genetics. SUDs are clinically and etiologically heterogeneous and their classification has been difficult. This application reflects my ongoing commitment to developing an innovative and interdisciplinary research program on the classification of SUDs through quantitative analysis of multidimensional data. My extensive training in computational science and prior research on biomedical informatics have provided me with the skills to design, implement and evaluate advanced algorithms and sophisticated analyses to solve challenging problems in classifying SUDs. My ongoing NIDA-funded R01 employs a large (n=~12,000) sample aggregated from multiple genetic studies of cocaine, opioid, and alcohol dependence to develop and evaluate novel statistical models to generate clinical SUD subtypes that are optimized for gene finding. This K02 proposal extends that work to evaluate treatment outcome in refined subgroups of SUD populations using data from treatment studies for cocaine, opioid, alcohol and multiple substance dependence. This project will integrate data from diagnostic behavioral variables and genotypes, as well as biological/neurobiological features of the disorders and repeated measures of treatment outcome. The primary career development goals of this application are to: (1) understand the reliability, validity and functional mechanisms of various phenotyping methods; (2) to continue training in the genetics of addictions; and (3) to gain greater knowledge of different treatment approaches and their efficacy. A solid foundation in these areas will enhance my ability to realize the full potential of the data collected and aggregated from multiple dimensions, and to use the data to design the most clinically useful analysis and generate innovative solutions to diagnostic and predictive challenges in SUD research. Through formal coursework, directed readings, individual tutoring and intensive multidisciplinary collaboration with a diverse team of world-renowned researchers, I will receive training and collect pilot data for future R01 projects by examining (Aim I): whether clinically-defined highly heritable subtypes derived in my current R01 project predict differential treatment response; (Aim II) whether new statistical models that directly combine treatment data with behavioral, biological, and genomic data identify refined subtypes with confirmatory multilevel evidence; and (Aim III) whether there are genetic and social moderators of treatment outcome by subtype. The overall goal of this proposal is to further my independent and multidisciplinary research program in the development of statistical methods for refined classification of SUDs. The K02 award will provide me with the protected time necessary to fully engage in the training activities described that will enhance my knowledge and skills to enable me to make important, novel contributions to the genetics and treatment of SUD. PROJECT NARRATIVE This project will develop novel statistical and quantitative tools to identify homogeneous subtypes of substance use disorders (SUDs) and other complex diseases to enhance gene finding and treatment matching. The proposed project will perform secondary analyses of existing data from treatment studies of cocaine, opioid, alcohol, and mixed SUDs. The proposed novel approaches are expected to advance precision medicine approaches to SUDs by enabling treatment matching and a more refined SUD classification to gene finding.",Classifying addictions using machine learning analysis of multidimensional data,9625118,K02DA043063,"['Adherence', 'Aftercare', 'Alcohol dependence', 'Alcohols', 'Algorithms', 'Area', 'Behavioral', 'Biological', 'Biological Markers', 'Biosensor', 'Characteristics', 'Classification', 'Clinical', 'Cluster Analysis', 'Cocaine', 'Cocaine Dependence', 'Collaborations', 'Combined Modality Therapy', 'Complex', 'Computational Science', 'DSM-IV', 'DSM-V', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic and Statistical Manual of Mental Disorders', 'Dimensions', 'Disease', 'Drug Use Disorder', 'Electroencephalography', 'Etiology', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Heritability', 'Heterogeneity', 'Independent Scientist Award', 'Individual', 'Interdisciplinary Study', 'Investigation', 'Joints', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Neurobiology', 'Opiate Addiction', 'Opioid', 'Patients', 'Pattern', 'Pharmacogenetics', 'Pharmacotherapy', 'Phenotype', 'Population', 'Reading', 'Recording of previous events', 'Research', 'Research Personnel', 'Risk Factors', 'Sampling', 'Scientist', 'Signs and Symptoms', 'Solid', 'Statistical Methods', 'Statistical Models', 'Subgroup', 'Substance Addiction', 'Substance Use Disorder', 'Surveys', 'Symptoms', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Work', 'addiction', 'alcohol use disorder', 'biomarker performance', 'biomedical informatics', 'career development', 'cocaine use', 'contingency management', 'design', 'disease classification', 'disorder subtype', 'endophenotype', 'genetic association', 'genomic data', 'imaging genetics', 'improved', 'innovation', 'multidimensional data', 'neural correlate', 'novel', 'novel strategies', 'opioid use disorder', 'outcome prediction', 'personalized medicine', 'precision medicine', 'programs', 'recruit', 'secondary analysis', 'skills', 'social', 'tool', 'treatment planning', 'treatment response', 'tutoring']",NIDA,UNIVERSITY OF CONNECTICUT STORRS,K02,2019,162123,36067938,-0.027878295650114515
"Automated, model-guided phenotyping to identify metabolite/gene/microbe interactions Project Summary/Abstract DNA sequencing has spawned the “microbiome revolution” -- thousands of microbes and a dizzying number of microbial interactions that are associated with human health and disease. Unfortunately, most species in the microbiome are known only by a (partial) genome. The limited phenotypic data on newly discovered bacteria reveal species that behave unlike any of our model organisms. While genome-scale modeling plays an important role in understanding the microbiome, the paucity of phenotypic data for most species prevents detailed simulation of the microbial communities that affect our health. This project will develop an automated system for profiling, synthesizing, and modeling microbial communities. The center of our approach is Deep Phenotyping, an automated robotic platform that performs complex growth experiments on demand. Data from Deep Phenotyping will be used to train metabolic and statistical models of the oral pathogens Streptococcus mutans and Candida albicans to predict conditions that keep both microbes in a nonpathogenic state. Project Narrative The microbiome revolution has uncovered thousands of species of bacteria with roles in health and disease. This project automates the identification of interactions between environments, genes, and the microbes that live in and around us. Understanding these interactions is a critical step in re-engineering the microbiome to improve human health.","Automated, model-guided phenotyping to identify metabolite/gene/microbe interactions",9651956,R21EB027396,"['Affect', 'Animal Model', 'Antibiotics', 'Area', 'Bacteria', 'Biochemical Pathway', 'Bioinformatics', 'Biological', 'Biological Assay', 'Candida albicans', 'Carbon', 'Coculture Techniques', 'Collaborations', 'Combinatorics', 'Communities', 'Complex', 'Computer software', 'DNA sequencing', 'Data', 'Data Set', 'Development', 'Disease', 'Engineering', 'Environment', 'Environmental Risk Factor', 'Exposure to', 'Future', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Growth', 'Health', 'Human', 'Human Microbiome', 'Hybrids', 'Image', 'Individual', 'Knock-out', 'Knowledge', 'Link', 'Liquid substance', 'Machine Learning', 'Maps', 'Metabolic', 'Methods', 'Microbe', 'Microbiology', 'Modeling', 'Oral', 'Oral candidiasis', 'Pathway interactions', 'Phenotype', 'Play', 'Positioning Attribute', 'Regulator Genes', 'Research Personnel', 'Robotics', 'Role', 'Shapes', 'Source', 'Statistical Models', 'Streptococcus mutans', 'Structure', 'System', 'Technology', 'Time', 'Training', 'Work', 'base', 'combinatorial', 'cost', 'design', 'dysbiosis', 'experience', 'experimental study', 'fitness', 'fungus', 'genome-wide', 'improved', 'instrumentation', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microorganism interaction', 'network models', 'open source', 'oral pathogen', 'pathogenic fungus', 'phenotypic data', 'preference', 'prevent', 'screening', 'simulation', 'transcription factor', 'transcriptome sequencing', 'transposon sequencing']",NIBIB,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,R21,2019,219161,76545728,-0.005399863681654778
"Integrated Microbial Screening and Antimicrobial Susceptibility Test on Microfluidic Digital Array for Diagnosis of Urinary Tract Infections Abstract COMBiNATi will work with Stanford University to bring the world’s first integrated ID+AST UTI diagnostic platform to the market by combining COMBiNATi’s “one-click” cost-effective dPCR platform with Stanford’s pathogen identification (ID) HRMA algorithm for broad detection, deep characterization and absolute quantification of UTI pathogens. By the end of the Phase 1 project, we will deliver the prototype instrument for ID and AST, scale-ready consumables for dPCR + HRM and culture + qPCR, machine-learning software algorithm for melt curve analysis, and prove the feasibility by identifying and quantifying the top two urinary tract infection bacteria. We will first develop two consumables: an 100k partition ID chip and a 32-lane, 10k partition per lane AST plate based on the micro-injection molding design rules established by COMBiNATi. An integrated system with thermal control and imaging capability will then be developed for HRM and qPCR processes. The prototype system will then be transferred to Stanford University where mock samples including both isolated bacteria and contrived urine samples will be tested on the platform for integrated pathogen ID and AST determination. Narrative Urinary Tract Infection (UTI) is one of the most common community-acquired bacterial infection. However, like other bacterial infections, standard culture-based diagnosis of UTI requires at least 2-3 days from sample acquisition to result reporting. Despite technological advancements, the process remains time- consuming and requires significant technical expertise. Automated instruments remain bulky and still require clonal isolation of the pathogens from the body fluid samples prior to AST. Additionally, the lack of definitive microbiological diagnosis that is rapid enough to achieve evidence-based treatment has driven the over- and misuse of broad-spectrum antibiotics. We believe the proposed integrated ID + AST platform has the potential to enable deep genetic analyses of clinical samples to provide rapid precision UTI triage and MIC determination in a timely and cost-efficient manner to positively impact patient care as well as promote the use of narrow spectrum antibiotics to favorably impact antibiotic resistance profiles.",Integrated Microbial Screening and Antimicrobial Susceptibility Test on Microfluidic Digital Array for Diagnosis of Urinary Tract Infections,9777415,R41AI145604,"['Algorithmic Analysis', 'Algorithmic Software', 'Algorithms', 'Ampicillin', 'Antibiotic Resistance', 'Antibiotic susceptibility', 'Antibiotics', 'Antimicrobial susceptibility', 'Bacteria', 'Bacterial Infections', 'Biological Assay', 'Body Fluids', 'Ceftriaxone', 'Cell Separation', 'Cells', 'Ciprofloxacin', 'Clinical', 'Communities', 'Computer software', 'Consumption', 'Data', 'Databases', 'Detection', 'Devices', 'Diagnosis', 'Diagnostic', 'Escherichia coli', 'Evidence based treatment', 'Genomic DNA', 'Gentamicins', 'Growth', 'Individual', 'Injections', 'Klebsiella pneumonia bacterium', 'Laboratories', 'Liquid substance', 'Machine Learning', 'Methods', 'Microbiology', 'Microfluidics', 'Minimum Inhibitory Concentration measurement', 'Modality', 'Molds', 'Molecular', 'Patient Care', 'Performance', 'Perfusion', 'Phase', 'Phenotype', 'Process', 'Protocols documentation', 'Reagent', 'Reporting', 'Resistance profile', 'Resolution', 'Sampling', 'Small Business Technology Transfer Research', 'System', 'Technical Expertise', 'Testing', 'Time', 'Triage', 'Trimethoprim-Sulfamethoxazole', 'Universities', 'Urinary tract infection', 'Urine', 'Uropathogen', 'Work', 'base', 'cost effective', 'cost efficient', 'design', 'digital', 'genetic analysis', 'imaging capabilities', 'instrument', 'melting', 'microbial', 'operation', 'pathogen', 'prototype', 'response', 'scale up', 'screening', 'success']",NIAID,"COMBINATI, INC.",R41,2019,224658,599695,-0.0037069301681626966
"Adaptive & Individualized AAC The heterogeneity of the more than 1.3% of Americans who suffer from severe physical impairments (SPIs) preclude the use of common augmentative or alternative communication (AAC) solutions such as manual signs, gestures or dexterous interaction with a touchscreen for communication. While efforts to develop alternative access methods through eye or head tracking have provided some communication advancements for these individuals, all current technologies suffer from the same fundamental limitation: existing AAC devices require patients to conform to generic communication access methods and interfaces rather than the device conforming to the user. Consequently, AAC users are forced to settle for interventions that require excessive training and cognitive workload only to deliver extremely slow information transfer rates (ITRs) and recurrent communication errors that ultimately deprive them of the fundamental human right of communication. To meet this health need, we propose the first smart-AAC system designed using individually adaptive access methods and AAC interfaces to accommodate the unique manifestations of motor impairments specific to each user. Preliminary research by our team of speech researchers at Madonna Rehabilitation Hospital (Communication Center Lab) and Boston University (STEPP Lab), utilizing wearable sensors developed by our group (Altec, Inc) have already demonstrated that metrics based on surface electromyographic (sEMG) and accelerometer measures of muscle activity and movement for head-mediated control can be combined with optimizable AAC interfaces to improve ITRs when compared with traditional unoptimized AAC devices. Leveraging this pilot work, our team is now proposing a Phase I project to demonstrate the proof-of-concept that a single sEMG/IMU hybrid sensor worn on the forehead can provide improvements in ITR and communication accuracy when integrated with an AAC interface that is optimized through machine learning algorithms. The prototype system will be tested and compared to a conventional (non-adaptable) interface in subjects with SPI at a collaborative clinical site. Assistance by our speech and expert-AAC collaborators will ensure that all phases of technology development are patient-centric and usable in the context of clinical care. In Phase II we will build upon this proof-of-concept to design a smart-AAC system with automated optimization software that achieves dynamic learning which adapts to intra-individual changes in function through disease progression or training as well as inter-individual differences in motor impairments for a diverse set of users with spinal cord injury, traumatic brain injury, cerebral palsy, ALS, and other SPIs. The innovation is the first and only AAC technology that combines advancements in wearable-sensor access with interfaces that are autonomously optimized to the user, thereby reducing the resources and training needed to achieve effective person-centric communication in SPI, through improved HMI performance and reduced workload. This project addresses the fundamental mission of NIDCD (National Institute for Deafness and Communication Disorders) to provide a direct means of assisting communication for people with severe physical impairments caused by stroke, high level spinal cord injury, neural degeneration, or neuromuscular disease. Leveraging wearable access technology (which has barely been explored for AAC users), we will develop a first-of-its-kind adaptive tablet interface tailored to individual users through advanced movement classification algorithms. Through these efforts, we aim to provide an improved Human Machine Interface (HMI) that is able to accommodate varying degrees of inter- and intra-subject residual motor function and context dependent impairments to provide individuals with SPI the opportunity for improved societal integration and quality of life.",Adaptive & Individualized AAC,9907832,R43DC018437,"['Accelerometer', 'Address', 'American', 'Boston', 'Cerebral Palsy', 'Child', 'Cognitive', 'Communication', 'Communication Methods', 'Communication impairment', 'Computer software', 'Custom', 'Development', 'Devices', 'Diagnosis', 'Disease Progression', 'Ensure', 'Eye', 'Facial Muscles', 'Fatigue', 'Forehead', 'Gestures', 'Goals', 'Head', 'Head Movements', 'Health', 'Heterogeneity', 'Hospitals', 'Human Rights', 'Hybrids', 'Image', 'Imaging problem', 'Impairment', 'Individual', 'Individual Differences', 'Institutes', 'Intervention', 'Intuition', 'Learning', 'Linguistics', 'Manuals', 'Measures', 'Mediating', 'Methods', 'Mission', 'Motor', 'Motor Manifestations', 'Movement', 'Muscle', 'National Institute on Deafness and Other Communication Disorders', 'Nerve Degeneration', 'Neuromuscular Diseases', 'Patients', 'Pattern', 'Performance', 'Persons', 'Phase', 'Population Heterogeneity', 'Quality of life', 'Recurrence', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Residual state', 'Resources', 'Series', 'Signal Transduction', 'Speech', 'Spinal cord injury', 'Stroke', 'Surface', 'System', 'Tablets', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'Traumatic Brain Injury', 'United States National Aeronautics and Space Administration', 'Universities', 'User-Computer Interface', 'Variant', 'Work', 'Workload', 'alternative communication', 'base', 'classification algorithm', 'clinical care', 'clinical research site', 'communication device', 'deafness', 'design', 'experimental study', 'improved', 'innovation', 'kinematics', 'machine learning algorithm', 'mathematical model', 'motor impairment', 'novel', 'prototype', 'rehabilitation engineering', 'rehabilitation science', 'sensor', 'sensor technology', 'signal processing', 'technology development', 'touchscreen', 'two-dimensional', 'wearable device']",NIDCD,"ALTEC, INC.",R43,2019,224701,1532918,-0.07557236889211762
"Development of an adaptive machine learning platform for automated analysis of biomarkers in biomedical images ABSTRACT Manual analysis of biomedical images by researchers and pathologists has the potential to introduce bias and error that compromise the reliability of research and clinical findings. These problems are significant barriers to delivering the most beneficial evidence-based medicine, developing effective medical treatments, and promoting confidence in scientific inquiry. Identification of biomarkers and cellular targets following microscopy requires manual analysis of biomedical images, which is time intensive, difficult, and prone to bias and errors. Unintentional bias and attentional limitations during analysis of biomarkers can underlie poor reproducibility of findings in biomedical research and potentially introduce error in clinical diagnostics. We recently developed a “beta” software package designed to improve automation and standardization of image analysis, called “PIPSQUEAK” (Perineuronal net Intensity Program for the Standardization and Quantification of Extracellular matrix Analysis Kit). Since its publication in 2016, PIPSQUEAK beta has amassed approximately 1,300 users worldwide who use it to quantify the intensity and number of perineuronal nets and other neural markers in the brain. This technology significantly increases data reliability between image raters and decreases the time required for analysis by more than 100-fold. However, PIPSQUEAK beta currently uses target detection algorithms that require high-contrast images to automatically identify neurons as clusters of bright pixels on dark backgrounds. A significant current limitation to PIPSQUEAK beta, and other available imaging programs, is that detection of biomarkers can be difficult unless image conditions are ideal. Suboptimal conditions, like high background staining, off-target structures, overlapping or clustered biomarkers, and atypical morphologies, can lead to artifacts and consequently to inaccurate results and erroneous conclusions. Here, we propose to develop a user-friendly artificial intelligence (AI) platform for the automated detection of targeted biomarkers in digital microscopy that reduces this error by learning to distinguish between true cellular biomarkers and artifacts. We propose to integrate AI capabilities into our PIPSQUEAK technology to produce an adaptive, high-throughput, biomedical image analysis platform that quickly and accurately identifies biomarker targets from bench to bedside. A key advantage is that this AI program will be user friendly and available online, making it highly accessible to basic researchers and to technicians and clinicians identifying human pathologies. Thus, successful development of our AI program has a high translational potential. The goal of this proposal is 1) to develop and validate a machine learning model that is capable of detecting common histological marker morphologies in digital microscopy, and 2) to test the feasibility of adapting our AI platform to new biomarker datasets with minimal additional supervised training. Our end goal is to advance the reliability and speed of research findings and clinical diagnoses by making this technology widely available to researchers and clinicians. PROJECT NARRATIVE Manual analysis of biomedical images by researchers and pathologists has the potential to introduces bias and error that compromise the reliability of research and clinical findings; problems which are significant barriers to delivering the most beneficial evidence-based medicine and developing effective medical treatments. Application of artificial intelligence for the detection of disease or cellular targets has the potential to improve the reliability of research findings and clinical diagnoses, while reducing waste, time, and expense. We propose a method to improve the quality of biomedical research reproducibility and clinical diagnoses by developing a high-throughput, adaptive artificial intelligence platform for automated analysis of cellular and disease targets in digital microscopy images, which will be made available to scientists and clinicians as a user-friendly analysis platform.",Development of an adaptive machine learning platform for automated analysis of biomarkers in biomedical images,9845994,R43GM134789,"['Abbreviations', 'Algorithms', 'Artificial Intelligence', 'Attention', 'Automation', 'Biological Markers', 'Biomedical Research', 'Brain', 'Cell Line', 'Cell model', 'Cellular Morphology', 'Clinical', 'Computer software', 'Confocal Microscopy', 'Coupled', 'Custom', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Evidence Based Medicine', 'Extracellular Matrix', 'FOS gene', 'Fluorescence', 'Future', 'Glial Fibrillary Acidic Protein', 'Goals', 'Histologic', 'Histology', 'Human Pathology', 'Image', 'Image Analysis', 'Immunoassay', 'Immunohistochemistry', 'Lead', 'Learning', 'Location', 'Machine Learning', 'Manuals', 'Measurement', 'Medical', 'Methods', 'Microscopy', 'Modeling', 'Morphologic artifacts', 'Morphology', 'Neurons', 'Nuclear', 'Pathologist', 'Performance', 'Procedures', 'Psychological Transfer', 'Publications', 'Rattus', 'Reproducibility', 'Reproducibility of Results', 'Research', 'Research Personnel', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Shapes', 'Speed', 'Stains', 'Standardization', 'Structure', 'Supervision', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissue Banks', 'Tissue Model', 'Tissue imaging', 'Tissues', 'Training', 'Zebrafish', 'automated analysis', 'base', 'bench to bedside', 'bioimaging', 'biomarker identification', 'cell type', 'cellular targeting', 'clinical Diagnosis', 'clinical diagnostics', 'contrast imaging', 'design', 'digital', 'digital imaging', 'extracellular', 'histological specimens', 'histological stains', 'imaging biomarker', 'imaging program', 'improved', 'interest', 'lateral line', 'microscopic imaging', 'predictive marker', 'programs', 'relating to nervous system', 'software as a service', 'statistics', 'targeted biomarker', 'tool', 'user-friendly', 'wasting']",NIGMS,"REWIRE NEUROSCIENCE, LLC",R43,2019,224915,0,-0.006293666468610421
"Quantifying molecular consequences of human missense variants with large-scale interactome perturbation studies Almost all proteins function through interacting with other proteins. Previous studies have shown that the vast majority of damaging single amino acid mutations in proteins disrupt only a subset of specific protein-protein interactions, and that mutations in the same protein that disrupt different interactions tend to cause clinically distinct disorders. Therefore, it is of great importance to determine interaction-specific disruptions caused by each mutation. Furthermore, rapid advances in sequencing technologies have enabled the identification of tens of millions of single nucleotide variants (SNVs) in the human population, driving an urgent need to understand the impact of each SNV on the human interactome network. Unfortunately, there is currently no method that is capable of predicting the specific impact of a large fraction of these SNVs on individual protein-protein interactions. To address this issue, we propose to leverage our massively-parallel site-directed mutagenesis pipeline, Clone-seq, to generate clones for ~6,000 coding SNVs in the human population: ~4,000 from gnomAD and ~2,000 to be submitted by the international human genetics community. We will then experimentally examine the impact on protein stability and individual protein-protein interactions for every variant using high-throughput DUAL-FLUO and InPOINT (integrating PCA, LUMIER, Y2H, and wNAPPA) assays. This proposal brings together three groups with complementary expertise in high-throughput interactome experiments and network analysis from the Yu lab, in genomic and population genetic studies from the Clark lab, and in comprehensive biophysical and structural modeling of mutation’s impact on binding free energy of protein interactions from the Alexov lab. Out of the ~6,000 SNVs, we expect to identify ~1,200 disruptive SNVs and ~4,000 different SNV-interaction pairs where the SNV disrupt that specific interaction. The data produced by our project will increase the available experimental information by >140× in number of human proteins and >500× in number of interactions, allowing us for the first time to comprehensively assess the relationships between the impact of SNVs on interactions and their various population genetic attributes (including, but not limited to, allele frequency and flanking haplotype, inter-population differentiation, local rate of recombination, allele age, modes of selection). Finally, we will establish a computational-experimental- integrated iterative learning scheme to build a multi-layer random-forest-based framework, SIMPACT, which can accurately predict specific impacts on all individual protein-protein interactions for all missense SNVs. Our proposed work will fuel hypothesis-driven research, will significantly improve our functional understanding of variants, and will likely fundamentally change the experimental design and data interpretation for whole genome/exome studies going forward. The dramatic increase of DNA variants discovered through advances in sequencing technologies has been inadequately translated into therapeutic successes. Although many of these variants are related to human disorders, the overwhelming number of non-functional variants makes the assessment of functional significance a steep challenge. In this study, we aim to develop a high-throughput pipeline to quickly clone and directly test a large number of coding variants for their impact on the human interactome network and use the results to build a machine learning pipeline to predict functional impact of all coding variants, in anticipation that both our experimental data and computational pipeline will lead to broad clinical and therapeutic applications.",Quantifying molecular consequences of human missense variants with large-scale interactome perturbation studies,9988741,R01GM125639,"['Address', 'Age', 'Alleles', 'Amino Acids', 'Automobile Driving', 'Binding', 'Biochemical', 'Biological Assay', 'Biophysics', 'Cells', 'Clinical', 'Code', 'Communities', 'Coupling', 'Crystallization', 'DNA', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease', 'Experimental Designs', 'Free Energy', 'Gene Frequency', 'Genes', 'Genetic Recombination', 'Genetic study', 'Genome', 'Genomics', 'Haplotypes', 'Homology Modeling', 'Human', 'Human Genetics', 'Individual', 'International', 'Learning', 'Letters', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Molecular', 'Mutation', 'Nucleotides', 'Output', 'Pathway Analysis', 'Population', 'Population Genetics', 'Property', 'Proteins', 'Proteomics', 'Research', 'Resolution', 'Resources', 'Scheme', 'Site-Directed Mutagenesis', 'Structural Models', 'Structure', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Translating', 'Variant', 'Work', 'base', 'biological research', 'biophysical model', 'biophysical techniques', 'exome', 'experimental study', 'human interactome', 'improved', 'interest', 'molecular phenotype', 'mutant', 'next generation sequencing', 'protein function', 'protein protein interaction', 'random forest', 'screening', 'success', 'web portal', 'whole genome']",NIGMS,CORNELL UNIVERSITY,R01,2019,238632,91477866,0.007632084166965168
"Algorithmic identification of binding specificity mechanisms in proteins Project Summary Variations in protein binding preferences are a critical barrier to the precision treatment of disease. When high resolution structures of a protein are available, and many isoforms of the protein have been connected to dif- fering binding preferences, it is possible in principle to model the structures of all isoforms and discover the mechanisms that cause variations in binding preferences. Unfortunately, this discovery process depends on human expertise for examining molecular structure, and given that hundreds of isoforms may exist, a human would be overwhelmed to objectively examine many similar isoforms. To fill this gap, this project will (A1) de- velop software that identifies structural mechanisms that cause differential binding preferences, categorizes similar structural mechanisms, and explains the mechanisms in English. The second aim of this project (A2) is to validate the software at a large scale on families of proteins that exhibit a variety of well-examined binding preferences, and through blind predictions with experimental collaborators. Our approach involves creating software that mimics the visual reasoning techniques employed by structural biologists when examining molecular structures. Not only are these techniques responsible for most major dis- coveries in structural biology, but they are also straightforward to understand by non-computational research- ers. This property will enable our software to immediately integrate into existing workflows at labs that do not focus on computational methods. This property also contrasts from existing methods, which generally output structural models, potential energies, p-values and structural scores which are difficult for non-experts to un- derstand or incorporate into their research. Often, an expert in biophysics is required to interpret the outputs so that they can be operationalized in laboratory environments. In preliminary results, our methods have already identified molecular mechanisms that govern specificity in several families of proteins. Verification against peer-reviewed experimentation has proven the preliminary results correct in almost all cases. Our methods have also been applied to make a blind prediction of binding mechanisms in the ricin toxin, which binds to and damages the human ribosome. With experimental collabo- rators, we showed that our methods correctly identified and predicted the roles of several amino acids with a hitherto unknown role in recognizing the ribosome. Using our methodological approach and our rigorous valida- tion strategy, this project will produce a highly validated, usable software package that will bridge a critical gap in the development of precision therapies and diagnostics. Variations in protein binding preferences are a critical barrier to precision medicine and precise diagnostics. We will develop software that will identify and categorize molecular mechanisms that cause these variations. The resulting insights will enable clinicians to more precisely select therapies to achieve superior outcomes.",Algorithmic identification of binding specificity mechanisms in proteins,9740715,R01GM123131,"['Address', 'Algorithms', 'Amino Acids', 'Artificial Intelligence', 'Benchmarking', 'Binding', 'Binding Proteins', 'Binding Sites', 'Biochemical', 'Biophysical Process', 'Biophysics', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Computing Methodologies', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Docking', 'Drug Targeting', 'Electrostatics', 'Elements', 'English Language', 'Environment', 'Evaluation', 'Exhibits', 'Feedback', 'HIV Protease', 'Hot Spot', 'Human', 'Hydrogen Bonding', 'Hydrophobicity', 'Immune', 'Individual', 'Influentials', 'Laboratories', 'Letters', 'Ligand Binding', 'Ligands', 'Link', 'Literature', 'Major Histocompatibility Complex', 'Maps', 'Measures', 'Mechanics', 'Methodology', 'Methods', 'Molecular', 'Molecular Conformation', 'Molecular Structure', 'Mutation', 'Nicotinic Receptors', 'Outcome', 'Output', 'Patients', 'Peer Review', 'Peptide Hydrolases', 'Population', 'Potential Energy', 'Precision therapeutics', 'Process', 'Property', 'Protein Family', 'Protein Isoforms', 'Proteins', 'Research', 'Resolution', 'Ribosomes', 'Ricin', 'Role', 'Serine Protease', 'Shapes', 'Site', 'Specificity', 'Structural Biologist', 'Structural Models', 'Structural Protein', 'Structure', 'Techniques', 'Testing', 'Text', 'Toxin', 'Tweens', 'Universities', 'Validation', 'Variant', 'Visual', 'base', 'blind', 'human-in-the-loop', 'hydropathy', 'inhibitor/antagonist', 'insight', 'mutant', 'novel', 'personalized diagnostics', 'precision medicine', 'preference', 'protein structure', 'prototype', 'receptor', 'simulation', 'software development', 'structural biology', 'therapy development', 'tool', 'tumor']",NIGMS,LEHIGH UNIVERSITY,R01,2019,244161,6676095,0.020769665611445658
"QuBBD: Geometric Time-Frequency Methods for Multi-modal Physiological Monitoring Modern health monitoring devices at hospitals and wearable sensors in households generate a large amount of time series data at high rate, capturing the physiological status of patients in a real-lime fashion. The premise is that these technology advances enable a data-driven healthcare system that starts making fast, accurate, objective and inexpensive decisions based upon data, in addition to an individual physician's experience and preference. However, there is a significant gap in the mathematical theory and computational tools to promptly extract actionable information from multi-modal non-stationary time series data in a robust and tractable manner, which has become a serious roadblock to further utilize bigger data for better healthcare monitoring. The goal of this research program is to develop a mathematical framework for extracting time-frequency and geometric representations of multi-modal physiological data, in an online and robust manner, and use them to design machine learning algorithms to improve real-lime health monitoring. Specifically, we hypothesize that the development of time-series and geometric methods for large streaming multi-modal monitoring data will lead to more accurate diagnosis on various physiological monitoring applications, including detection and prediction of rare events such as seizure and arrhythmia, classification of sleep stages for newborns and children, and real-time artifact removal of physiological data. To achieve our goal, we plan to develop novel theoretical and computational tools for analyzing non-stationary multi-modal time series data with noise, corruption and missing data as well as real-time algorithms for filtering and event detection from such data. The tools and algorithms will be applied on clinical tasks at the Nationwide Children's Hospital. In addition, the real-time workflow will be implemented on Hadoop clusters with a mission of public sharing of both data and software. The development from the interdisciplinary team composed of mathematicians, biomedical informaticians as well as the hospital will not only transform the frontiers of mathematics knowledge, but also significantly impact clinical applications, data science education, and the development of the $11 O billion emerging market of wireless health. The goal of this project is to develop a series of novel computational theory and software to extract physiological information from the large multi-modal data streams generated by modern health monitoring devices. The tools will be applied to various clinical tasks such as detection and prediction of seizure and arrhythmia and classification of sleep stages for newborns and children, aiming for more accurate diagnosis.",QuBBD: Geometric Time-Frequency Methods for Multi-modal Physiological Monitoring,9771321,R01EB025018,"['Address', 'Algorithms', 'Arrhythmia', 'Behavior', 'Big Data', 'Breathing', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Collaborations', 'Complex', 'Computer software', 'Data', 'Data Science', 'Detection', 'Development', 'Diagnostic', 'Education', 'Environment', 'Event', 'Excision', 'Frequencies', 'Goals', 'Health', 'Healthcare', 'Healthcare Systems', 'Hospitals', 'Household', 'Human body', 'Individual', 'Infant', 'Knowledge', 'Limes', 'Machine Learning', 'Mathematics', 'Measures', 'Methods', 'Mission', 'Modernization', 'Monitor', 'Morphologic artifacts', 'Nature', 'Newborn Infant', 'Noise', 'Outcome', 'Patients', 'Pattern', 'Pediatric Hospitals', 'Physicians', 'Physiologic Monitoring', 'Physiological', 'Property', 'Public Domains', 'Research', 'Resources', 'Seizures', 'Series', 'Sleep Stages', 'Stream', 'Techniques', 'Technology', 'Time', 'Universities', 'Validation', 'Wireless Technology', 'accurate diagnosis', 'base', 'biological systems', 'clinical application', 'clinical practice', 'computerized tools', 'design', 'diagnostic biomarker', 'education resources', 'experience', 'frontier', 'geometric methodologies', 'graduate student', 'heart rate variability', 'improved', 'insight', 'machine learning algorithm', 'mathematical theory', 'monitoring device', 'multimodal data', 'multimodality', 'novel', 'preference', 'programs', 'science education', 'signal processing', 'student training', 'theories', 'tool', 'wearable device']",NIBIB,CARNEGIE-MELLON UNIVERSITY,R01,2019,255543,30434536,3.076726386959261e-05
"PiNDA - Fully integrated software platform for Preimplantation Genetic Testing - Aneuploidy (PGT-A) PROJECT SUMMARY  Since its inception 40 years ago, in vitro fertilization (IVF) has resulted in the birth of more than 1 million babies in the United States, and has revolutionized the field of reproductive medicine. Unfortunately, the success rate of IVF is still exceedingly low, especially for women >40 years old, with only 15.5% of implanted embryos resulting in pregnancy. This is partly due to the cytological method used for pre-implantation screening, which cannot detect the most common genetic defect during IVF, aneuploidy (i.e. chromosomal copy-number variation). Aneuploidy is linked to higher rates of miscarriage, and occurs more often in women >40 years of age; thus, aneuploidy has been a frequent target for genetic screening to improve IVF outcomes.  Pre-implantation genetic testing for aneuploidy (PGT-A) refers to a variety of techniques aimed at detecting changes in chromosomal copy number, with the goal of identifying high-quality euploid embryos for implantation. Recent advances in next-generation sequencing (NGS) technologies have made it possible to screen embryos at higher levels of precision, and across a wider range of genetic defects, including mosaicism, triploidy and single nucleotide polymorphisms (SNPs). Despite these remarkable advances, there are still significant challenges with PGT-A sequencing. Indeed, the most commonly implemented software for PGT-A (i.e. BlueFuse® ) are bundled with specific sequencing platforms (i.e. VeriSeq®), and are only designed to test for aneuploidy. Furthermore, existing pipelines are not user-friendly or customizable, which is a serious obstacle prohibiting the use of NGS by clinicians / embryologists. A more accessible bioinformatics platform is desperately needed that will bridge the gap between PGT-A sequencing and IVF outcomes.  Basepair™ is an innovator in efficient, user-friendly, web-based NGS analysis systems, with fully automated ChIP-, RNA-, ATAC-, and DNA-Seq bioinformatics pipelines available online. Here, Basepair will deliver PiNDA™, the first fully integrated software solution for comprehensive PGT-A analysis. In Aim 1, we will develop modules to test for specific chromosomal abnormalities, including mosaicism and triploidy, and validate each model with training data derived from somatic cell lines with known chromosomal aberrations. In Aim 2, we will integrate our modules into the PiNDA software system, creating a user-friendly, web-based interface that will perform full data analysis (raw data to full summary report) in <15 minutes, with no manual input required. Final data will be accessible via Basepair’s online portal, facilitating rapid data transfer from embryologists to physicians, and supporting the integration of NGS tests in IVF. Our innovative bioinformatics platform will accelerate NGS analysis for IVF, improving rates of pregnancy and advancing research in the success of IVF procedures. PROJECT NARRATIVE  In vitro fertilization (IVF) methods have begun to leverage next-generation sequencing technologies for pre-implantation genetic testing of aneuploidy (PGT-A), expanding the array of chromosomal abnormalities that can be accurately detected. However, the vast majority of software can only distinguish one type of genetic defect (i.e. aneuploidy), are difficult to use, and are tied to distinct sequencing platforms, limiting the clinical utility of resulting analyses. Basepair™ Inc. is a pioneer in user-friendly, web-based bioinformatics pipelines, providing comprehensive services for a wide range of sequencing projects. Here, Basepair will develop an inclusive suite of software for PGT-A, compatible with sequencing data from multiple platforms. This product will be of high value to the field and will help bridge the gap between advances in DNA sequencing and IVF technology.",PiNDA - Fully integrated software platform for Preimplantation Genetic Testing - Aneuploidy (PGT-A),9846492,R43HD100280,"['ATAC-seq', 'Age-Years', 'Algorithms', 'Aneuploid Cells', 'Aneuploidy', 'Bioinformatics', 'Biopsy', 'Birth', 'Cell Line', 'Cell division', 'Centers for Disease Control and Prevention (U.S.)', 'ChIP-seq', 'Chromosome abnormality', 'Clinical', 'Complex', 'Computer software', 'Copy Number Polymorphism', 'Culture Media', 'Cytology', 'DNA sequencing', 'Data', 'Data Analyses', 'Embryo', 'Feedback', 'Fertility Agents', 'Fertilization in Vitro', 'Genetic Screening', 'Goals', 'Harvest', 'Implant', 'Letters', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Morphology', 'Mosaicism', 'Mutation', 'Online Systems', 'Outcome', 'Phase', 'Physicians', 'Polymorphism Analysis', 'Pregnancy', 'Pregnancy Rate', 'Preimplantation Diagnosis', 'Procedures', 'Reporting', 'Reproductive Medicine', 'Research', 'Role', 'Sampling', 'Services', 'Single Nucleotide Polymorphism', 'Somatic Cell', 'Specificity', 'Spontaneous abortion', 'Summary Reports', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Training', 'Triploidy', 'United States', 'Uterus', 'Woman', 'analysis pipeline', 'aneuploidy analysis', 'cell free DNA', 'design', 'early embryonic stage', 'egg', 'implantation', 'improved', 'innovation', 'natural Blastocyst Implantation', 'next generation sequencing', 'phase 1 study', 'preimplantation', 'screening', 'sequencing platform', 'software development', 'software systems', 'sperm cell', 'success', 'transcriptome sequencing', 'user-friendly', 'web based interface']",NICHD,"BASEPAIR, INC.",R43,2019,298717,348442,-0.024764052707145132
"Statistical Methods in Trans-Omics Chronic Disease Research Project Summary The broad, long-term objectives of this research are the development of novel and high-impact statistical methods for medical studies of chronic diseases, with a focus on trans-omics precision medicine research. The speciﬁc aims of this competing renewal application include: (1) derivation of efﬁcient and robust statistics for integrative association analysis of multiple omics platforms (DNA sequences, RNA expressions, methylation proﬁles, protein expressions, metabolomics proﬁles, etc.) with arbitrary patterns of missing data and with detection limits for quantitative measurements; (2) exploration of statistical learning approaches for handling multiple types of high- dimensional omics variables with structural associations and with substantial missing data; and (3) construction of a multivariate regression model of the effects of somatic mutations on gene expressions in cancer tumors for discovery of subject-speciﬁc driver mutations, leveraging gene interaction network information and accounting for inter-tumor heterogeneity in mutational effects. All these aims have been motivated by the investigators' applied research experience in trans-omics studies of cancer and cardiovascular diseases. The proposed solutions are based on likelihood and other sound statistical principles. The theoretical properties of the new statistical methods will be rigorously investigated through innovative use of advanced mathematical arguments. Computationally efﬁcient and numerically stable algorithms will be developed to implement the inference procedures. The new methods will be evaluated extensively with simulation studies that mimic real data and applied to several ongoing trans-omics precision medicine projects, most of which are carried out at the University of North Carolina at Chapel Hill. Their scientiﬁc merit and computational feasibility are demonstrated by preliminary simulation results and real examples. Efﬁcient, reliable, and user-friendly open-source software with detailed documentation will be produced and disseminated to the broad scientiﬁc community. The proposed work will advance the ﬁeld of statistical genomics and facilitate trans-omics precision medicine studies of chronic diseases. Project Narrative The proposed research intends to develop novel and high-impact statistical methods for integrative analysis of trans-omics data from ongoing precision medicine studies of chronic diseases. The goal is to facilitate the creation of a new era of medicine in which each patient receives individualized care that matches their genetic code.",Statistical Methods in Trans-Omics Chronic Disease Research,9658524,R01HG009974,"['Accounting', 'Address', 'Algorithms', 'Applied Research', 'Biological', 'Cardiovascular Diseases', 'Characteristics', 'Chronic Disease', 'Communities', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Derivation procedure', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Documentation', 'Equation', 'Formulation', 'Gene Expression', 'Genes', 'Genetic Code', 'Genetic Transcription', 'Genomics', 'Goals', 'Grant', 'Information Networks', 'Institution', 'Inter-tumoral heterogeneity', 'Joints', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Medicine', 'Mental disorders', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular', 'Molecular Abnormality', 'Molecular Profiling', 'Mutation', 'Mutation Analysis', 'National Human Genome Research Institute', 'North Carolina', 'Patients', 'Pattern', 'Precision Medicine Initiative', 'Prevention', 'Procedures', 'Process', 'Property', 'Public Health', 'Research', 'Research Personnel', 'Resources', 'Somatic Mutation', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Tail', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'actionable mutation', 'base', 'disease phenotype', 'experience', 'gene interaction', 'genome sequencing', 'high dimensionality', 'innovation', 'learning strategy', 'metabolomics', 'multidimensional data', 'multiple omics', 'novel', 'open source', 'outcome prediction', 'personalized care', 'precision medicine', 'programs', 'protein expression', 'research and development', 'semiparametric', 'simulation', 'sound', 'statistics', 'theories', 'tool', 'tumor', 'tumor heterogeneity', 'user-friendly']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2019,305167,511185245,0.002075909372766252
"Integrated Instrument for non-natural aptamer generation Project Summary DNA and RNA aptamers are a useful class of synthetic affinity reagents. However, their performance can be greatly improved through the site-specific incorporation of chemically modified, ‘non-natural’ nucleotides that provide a greater chemical repertoire to enable superior aptamer affinity and specificity. Because a broad spectrum of chemical functional groups can be incorporated, non-natural aptamers offer the exciting potential for targeting molecules for which the generation of monoclonal antibodies remains difficult, such as small- molecule drugs, metabolites and carbohydrates. Unfortunately, the access to non-natural aptamers is severely limited. This is because the process of generating non-natural aptamers is technically challenging and limited to a few specialized laboratories. The goal of this project is to develop an integrated instrument, the Non-Natural Aptamer Array (N2A2) that eliminates these bottlenecks and enable rapid and facile non-natural aptamer discovery at virtually any research laboratory. The N2A2 will be built on a modified version of a benchtop commercial sequencer (Illumina MiSeq), and will perform every stage of non-natural aptamer discovery— including sequencing, screening and binding measurements—as part of a single work-flow. There are three main innovative aspects of our N2A2 system. First, our approach will entirely eliminate the need for polymerase engineering, and thus allows us to incorporate virtually any chemical functional group through click chemistry. Second, N2A2 will enable us to directly obtain the binding affinity (Kd) of ~10^7 aptamers directly in complex samples (e.g. cell lysate or serum), thereby resulting in aptamers with high-specificity. Finally, we will develop a machine-learning (ML) approach to identify key motifs (“k-mers”) and predict novel sequences with potentially higher affinity and specificity that can be tested using the N2A2 instrument. We believe this powerful combination of massively parallel, sequence-linked binding measurements with ML-based predictions will allow us to explore sequence space that is currently inaccessible to traditional in vitro selection methods, and enable us to discover aptamers with superior performance. The success of this project will produce an integrated instrument that greatly streamlines and accelerates the discovery of non-natural aptamers for a wide range of targets in complex media. The instrument is based on a commercially available sequencer and we will make all software available to the public. In this way, we believe the N2A2 instrument could broadly expand access to robust, high quality, custom affinity reagents for biomedical research and clinical diagnostics. Project Narrative We will develop an integrated instrument that simplifies the discovery of non-natural aptamer reagents for a wide range of molecules that are difficult to target using conventional antibody reagents. The access to these custom reagents will accelerate biomedical research and clinical diagnostics.",Integrated Instrument for non-natural aptamer generation,9705993,R01GM129313,"['Affinity', 'Algorithms', 'Antibodies', 'Binding', 'Biomedical Research', 'Carbohydrates', 'Cells', 'Chemicals', 'Chemistry', 'Chinese Hamster Ovary Cell', 'Complex', 'Computer software', 'Custom', 'DNA', 'Data', 'Data Analyses', 'Directed Molecular Evolution', 'Engineering', 'Generations', 'Goals', 'Graph', 'In Vitro', 'Label', 'Laboratories', 'Laboratory Research', 'Link', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Monoclonal Antibodies', 'Nucleotides', 'Opioid', 'Performance', 'Pharmaceutical Preparations', 'Polymerase', 'Process', 'Proteins', 'RNA', 'Reagent', 'Reproducibility', 'SLEB2 gene', 'Sampling', 'Serum', 'Site', 'Specificity', 'System', 'Testing', 'Tyrosine', 'Work', 'analysis pipeline', 'aptamer', 'base', 'clinical diagnostics', 'functional group', 'improved', 'innovation', 'instrument', 'machine learning algorithm', 'novel', 'scaffold', 'screening', 'small molecule', 'success', 'virtual']",NIGMS,STANFORD UNIVERSITY,R01,2019,314000,560644462,0.0016641053416738133
"The Transporter Classification Database (TCDB) ABSTRACT  Transporters catalyze entry and exit of molecules into and out of cells and organelles. They achieve cellular homeostasis, are responsible for multidrug resistance in pathogens and tumors, and, when defective, cause dozens of important human genetic diseases. Our laboratory maintains, updates and improves the Transporter Classification Database, TCDB, which houses the Transporter Classification (TC) system, adopted officially by the International Union of Biochemistry and Molecular Biology (IUBMB). TCDB is the internationally acclaimed, carefully annotated, universal standard for classifying and providing information about transporters and transport-related proteins in all major domains of life. It presents sequence, biochemical, physiological, pathological, structural and evolutionary data about these proteins and the transport systems they comprise. It uses a successful system of classification based on transporter class, subclass, family, subfamily, individual transport system and constituent proteins. It also includes a superfamily hyperlink.  In this competitive renewal of GM0077402, we propose to continue to expand, update, and semi-automate TCDB. Our specific aims are to (1) upgrade TCDB by characterizing and categorizing protein domains and their topologies, motifs, repeat units, functional interactions, alternative splicing and post-translational modifications, (2) expand TCDB by implementing novel pipelines for data entry that will increase the coverage of transport diversity in TCDB while describing more effectively the complexity of multicomponent transport systems, (3) enter into TCDB transporter modulators such as activators, inhibitors, drugs and xenobiotics as well as internal and external conditions that influence transporter activities, while generating an ontology to describe the effects of chemical modulators that will complement our substrate ontology, (4) incorporate into TCDB synthetic pores/channels (TC subclass 1.D), and carriers (TC subclass 2.B), (5) introduce into TCDB connections between transport and metabolism and (6) expand our plans for long-term TCDB sustainability. PROJECT NARRATIVE  TCDB is the only IUBMB-approved database providing the worldwide scientific community with systematic information about proteins that catalyze transmembrane transport. Transport proteins play critical roles in health-related issues such as personalized medicine, cancer, drug development, bacterial pathogenesis and antimicrobial resistance. Funding of this proposal will allow us to provide research results, as well as high-quality data and software for the identification of transporter proteins useful to scientists whose investigations focus on health issues.",The Transporter Classification Database (TCDB),9738997,R01GM077402,"['Activator Appliances', 'Adopted', 'Affect', 'Alternative Splicing', 'Antimicrobial Resistance', 'Antineoplastic Agents', 'Biochemical', 'Biochemistry', 'Biological Phenomena', 'Biology', 'Biotechnology', 'Carrier Proteins', 'Cells', 'Chemicals', 'Classification', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Data', 'Data Base Management', 'Data Quality', 'Databases', 'Development', 'Ecosystem', 'Ensure', 'Eukaryota', 'Family', 'Funding', 'Genetic Diseases', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Homeostasis', 'Human Genetics', 'Individual', 'Information Resources', 'International', 'Investigation', 'Island', 'Knowledge', 'Laboratories', 'Life', 'Ligands', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Membrane Proteins', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Multi-Drug Resistance', 'Ontology', 'Organelles', 'Pathogenesis', 'Pathologic', 'Pharmaceutical Preparations', 'Phenotype', 'Physiological', 'Play', 'Post-Translational Protein Processing', 'Production', 'Protein Isoforms', 'Proteins', 'Pump', 'RNA Splicing', 'Research', 'Research Personnel', 'Role', 'Scientist', 'Stimulus', 'Structure', 'System', 'Systems Biology', 'Tertiary Protein Structure', 'Time', 'Tissues', 'Training', 'Transmembrane Transport', 'Update', 'Variant', 'Vertebral column', 'Work', 'Xenobiotics', 'base', 'data pipeline', 'drug development', 'improved', 'inhibitor/antagonist', 'insight', 'member', 'metagenome', 'novel', 'pathogen', 'personalized medicine', 'protein transport', 'screening', 'text searching', 'transmission process', 'tumor', 'whole genome', 'willingness']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2019,314750,524978793,0.013487446458726203
"Inferential methods for functional data from wearable devices Project Summary/Abstract This is a project to develop new statistical methods for comparing groups of subjects in terms of health outcomes that are assessed using data from wearable devices. Inexpensive wearable sensors for health monitoring are now capable of generating massive amounts of data collected longitudinally, up to months at a time. The project will develop inferential methods that can deal with the complexity of such data. A serious challenge is the presence of unmeasured time-dependent confounders (e.g., circadian and dietary patterns), making direct comparisons or borrowing strength across subjects untenable unless the studies are carried out in controlled experimental con- ditions. Generic data mining and machine learning tools have been widely used to provide predictions of health status from such data. However, such tools cannot be used for signiﬁcance testing of covariate effects, which is necessary for designing precision medicine interventions, for example, without taking the inherent model selection or the presence of the unmeasured confounders into account. To overcome these difﬁculties, a systematic de- velopment of inferential methods for functional outcome data obtained from wearable devices will be carried out. There are three speciﬁc aims: 1) Develop metrics for functional outcome data from wearable devices, 2) Develop nonparametric estimation and testing methods for activity proﬁles and a screening method for predictors of activity proﬁles, 3) Implement the methods in an R package and carry out two case studies using accelerometer data. For Aim 1, the approach is to reduce the sensor data to occupation time proﬁles (e.g., as a function of activity level), and formulate the statistical modeling in terms of these proﬁles using survival and functional data analytic meth- ods. This will have a number of advantages, the principal one being that time-dependent confounders become less problematic because the effect of differences in temporal alignment across subjects is mitigated. In addition, survival analysis methods can be applied by viewing the occupation time as a time-to-event outcome indexed by activity level. For Aim 2, nonparametric methods will be used to compare and order occupation time distributions between groups of subjects that are speciﬁed in terms of baseline covariate levels or treatment groups. Further, a new method of post-selection inference based on marginal screening for function-on-scalar regression will be developed to identify and formally test whether covariates are signiﬁcantly associated with activity proﬁles. Aim 3 will develop an R-package implementation, and as a test-bed for the proposed methods they will be applied to two Columbia-based clinical studies: to the study of physical activity in children enrolled in New York City Head Start, and to the study of experimental drugs for the treatment of mitochondrial depletion syndrome. Project Narrative The relevance of the project to public health is that it will develop statistical methods for the physiological eval- uation of patients on the basis of data collected by inexpensive wearable sensors (e.g., accelerometers). By introducing methods for the rigorous comparison of healthcare status among groups of patients observed longi- tudinally over time using such devices, treatment decisions that can beneﬁt targeted populations of patients in terms of continuously-assessed health outcomes will become possible.",Inferential methods for functional data from wearable devices,9658873,R01AG062401,"['Acceleration', 'Accelerometer', 'Beds', 'Bypass', 'Case Study', 'Characteristics', 'Child', 'Clinical Research', 'Computer software', 'Data', 'Data Analytics', 'Development', 'Devices', 'Dietary Practices', 'Drug Combinations', 'Enrollment', 'Evaluation', 'Event', 'Grant', 'Head Start Program', 'Health', 'Health Status', 'Healthcare', 'Intervention', 'Lead', 'Machine Learning', 'Measures', 'Methods', 'Mitochondria', 'Modeling', 'Molecular', 'Monitor', 'Motivation', 'Nature', 'New York City', 'Obesity', 'Occupations', 'Outcome', 'Outcome Measure', 'Patients', 'Pharmacotherapy', 'Physical activity', 'Physiological', 'Preschool Child', 'Process', 'Proxy', 'Public Health', 'Recording of previous events', 'Regimen', 'Signal Transduction', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Stochastic Processes', 'Survival Analysis', 'Syndrome', 'Target Populations', 'Techniques', 'Testing', 'Time', 'Work', 'analytical method', 'base', 'circadian', 'data mining', 'design', 'experimental study', 'functional outcomes', 'indexing', 'interest', 'lower income families', 'novel', 'patient population', 'precision medicine', 'screening', 'sensor', 'theories', 'time use', 'tool', 'treatment group', 'wearable device']",NIA,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2019,317858,558628098,0.0017333610305145734
"Synergistic integration of topology and machine learning for the predictions of protein-ligand binding affinities and mutation impacts Project Summary Fundamental challenges that hinder the current understanding of biomolecular systems are their tremendous complexity, high dimensionality and excessively large data sets associated with their geometric modeling and simulations. These challenges call for innovative strategies for handling massive biomolecular datasets. Topology, in contrast to geometry, provides a unique tool for dimensionality reduction and data simplification. However, traditional topology typically incurs with excessive reduction in geometric information. Persistent homology is a new branch of topology that is able to bridge traditional topology and geometry, but suffers from neglecting biological information. Built upon PI’s recent work in the topological data analysis of biomolecules, this project will explore how to integrate topological data analysis and machine learning to significantly improve the current state-of-the-art predictions of protein-ligand binding and mutation impact established in the PI’s preliminary studies. These improvements will be achieved through developing physics-embedded topological methodologies and advanced deep learning architectures for tackling heterogeneous biomolecular data sets arising from a variety of physical and biological considerations. Finally, the PI will establish robust databases and online servers for the proposed predictions. Project Narrative The project concerns the integration of topological data analysis and machine learning architectures for the predictions of protein-ligand binding affinities and mutation induced protein stability changes from massive data sets. This new data approach has considerable impact for future generation methods in computational biophysics and drug design.",Synergistic integration of topology and machine learning for the predictions of protein-ligand binding affinities and mutation impacts,9756427,R01GM126189,"['3-Dimensional', 'Address', 'Affinity', 'Architecture', 'Big Data', 'Binding', 'Binding Proteins', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Biophysics', 'Characteristics', 'Chemicals', 'Classification', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Dimensions', 'Drug Design', 'Electrostatics', 'Elements', 'Free Energy', 'Freedom', 'Future Generations', 'Geometry', 'Handwriting', 'Image Analysis', 'Induced Mutation', 'Ions', 'Learning', 'Ligand Binding', 'Ligands', 'Lipids', 'Machine Learning', 'Medical', 'Membrane', 'Membrane Proteins', 'Metals', 'Methodology', 'Methods', 'Mutation', 'Physics', 'Plant Roots', 'Proteins', 'Psychological Transfer', 'Site', 'Speech', 'System', 'Techniques', 'Thermodynamics', 'Work', 'algebraic topology', 'base', 'cofactor', 'data warehouse', 'deep learning', 'deep learning algorithm', 'direct application', 'high dimensionality', 'improved', 'innovation', 'language processing', 'learning algorithm', 'learning strategy', 'machine learning algorithm', 'metallicity', 'models and simulation', 'multi-task learning', 'multitask', 'mutant', 'neglect', 'next generation', 'search engine', 'tool', 'trend', 'user-friendly']",NIGMS,MICHIGAN STATE UNIVERSITY,R01,2019,319267,89938253,0.015889504472531187
"Common Fund Data Supplement: Integration of KOMP2 (IMPC) and PHAROS into MARRVEL 2.0 for machine learning-assisted rare variant prioritization Project Summary  This application is being submitted in response to NOT-RM-19-009 as a supplement to the parent award U54NS093793.  The Common Fund supports a number of resources that can significantly enhance gene and variant prioritization for study in the Model Organisms Screening Center of the Undiagnosed Diseases Network and beyond. To facilitate the use of these resources, we propose to create a tool that can be easily accessed by clinical geneticists and model organism scientists alike.  MARRVEL (Model organism Aggregated Resources for Rare Variant ExpLoration) was created two years ago because important data that is necessary for rare variant analysis for personalized medicine is spread throughout the internet in tens of different locations. To improve efficiency and streamline access to these data sources, we created a web-tool that allows users to query tens of data sources at once, including GTEx, and links to IMPC, the display portal for KOMP2.  In this proposal, our goal is to develop version 2 of MARRVEL to promote the use of Common Fund resources in the rare disease research community for manual and automated data analysis. This goal will be accomplished by developing MARRVEL 2.0 by integrating KOMP2 (IMPC) and PHAROS data and using the aggregated dataset to develop a machine-assisted gene and variant prioritization for diagnosis and animal model generation.  Our goals align with those of the NIH Common Fund to increase the utility of resources for broader use in the biomedical community. Project Narrative  We aim to promote the use of Common Fund resources and facilitate the diagnosis of rare diseases and the subsequent generation of animal models for the Undiagnosed Diseases Network and beyond. This goal will be accomplished by developing the web resource, MARRVEL 2.0.",Common Fund Data Supplement: Integration of KOMP2 (IMPC) and PHAROS into MARRVEL 2.0 for machine learning-assisted rare variant prioritization,9984757,U54NS093793,"['Affect', 'Animal Model', 'Artificial Intelligence', 'Award', 'Clinical', 'Collaborations', 'Communities', 'Country', 'Data', 'Data Analyses', 'Data Display', 'Data Set', 'Data Sources', 'Development', 'Diagnosis', 'Discipline', 'Disease', 'Disease model', 'Drosophila genus', 'Drug Targeting', 'Expert Systems', 'Family', 'Funding', 'Generations', 'Genes', 'Genetic Diseases', 'Genotype-Tissue Expression Project', 'Goals', 'Growth', 'Healthcare Systems', 'Human Genetics', 'Individual', 'Internet', 'Investigation', 'Knowledge', 'Link', 'Location', 'Machine Learning', 'Manuals', 'Medical', 'Medical Genetics', 'Modeling', 'Mus', 'Parents', 'Pathogenicity', 'Pharmaceutical Preparations', 'Phenotype', 'Process', 'Proteins', 'Rare Diseases', 'Research', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Suggestion', 'Symptoms', 'System', 'Testing', 'Therapeutic', 'Therapeutic Studies', 'Time', 'Training', 'United States National Institutes of Health', 'Variant', 'Visit', 'Yeasts', 'Zebrafish', 'base', 'data wrangling', 'design', 'experimental study', 'feeding', 'fly', 'genetic disorder diagnosis', 'genetic variant', 'human data', 'improved', 'interest', 'learning community', 'machine learning algorithm', 'model organisms databases', 'online resource', 'personalized medicine', 'phenotypic data', 'rare genetic disorder', 'rare variant', 'response', 'screening', 'supervised learning', 'tool', 'web-based tool']",NINDS,BAYLOR COLLEGE OF MEDICINE,U54,2019,320000,323604360,-0.010796012419701965
"Big Flow Cytometry Data: Data Standards, Integration and Analysis PROJECT SUMMARY Flow cytometry is a single-cell measurement technology that is data-rich and plays a critical role in basic research and clinical diagnostics. The volume and dimensionality of data sets currently produced with modern instrumentation is orders of magnitude greater than in the past. Automated analysis methods in the field have made great progress in the past five years. The tools are available to perform automated cell population identification, but the infrastructure, methods and data standards do not yet exist to integrate and compare non-standardized big flow cytometry data sets available in public repositories. This proposal will develop the data standards, software infrastructure and computational methods to enable researchers to leverage the large amount of public cytometry data in order to integrate, re-analyze, and draw novel biological insights from these data sets. The impact of this project will be to provide researchers with tools that can be used to bridge the gap between inference from isolated single experiments or studies, to insights drawn from large data sets from cross-study analysis and multi-center trials. PROJECT NARRATIVE The aims of this project are to develop standards, software and methods for integrating and analyzing big and diverse flow cytometry data sets. The project will enable users of cytometry to directly compare diverse and non-standardized cytometry data to each other and make biological inferences about them. The domain of application spans all disease areas where cytometry is utilized.","Big Flow Cytometry Data: Data Standards, Integration and Analysis",9731544,R01GM118417,"['Address', 'Adoption', 'Advisory Committees', 'Archives', 'Area', 'Basic Science', 'Bioconductor', 'Biological', 'Biological Assay', 'Cells', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Data Files', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Environment', 'Flow Cytometry', 'Foundations', 'Genes', 'Goals', 'Heterogeneity', 'Immune System Diseases', 'Immunologic Monitoring', 'Industry', 'Informatics', 'Infrastructure', 'International', 'Knock-out', 'Knowledge', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Meta-Analysis', 'Metadata', 'Methods', 'Modernization', 'Mouse Strains', 'Multicenter Trials', 'Mus', 'Output', 'Phenotype', 'Play', 'Population', 'Procedures', 'Protocols documentation', 'Reagent', 'Research', 'Research Personnel', 'Retrieval', 'Role', 'Societies', 'Software Tools', 'Standardization', 'Technology', 'Testing', 'Validation', 'Work', 'automated analysis', 'base', 'bioinformatics tool', 'body system', 'cancer diagnosis', 'clinical diagnostics', 'community based evaluation', 'computerized tools', 'data exchange', 'data integration', 'data submission', 'data warehouse', 'experimental study', 'human disease', 'insight', 'instrument', 'instrumentation', 'mammalian genome', 'multidimensional data', 'novel', 'operation', 'phenotypic data', 'repository', 'research and development', 'software development', 'statistics', 'supervised learning', 'tool', 'vaccine development']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2019,350620,758431960,0.005968832321221359
"Advanced Computational Approaches for NMR Data-mining ABSTRACT Nuclear magnetic resonance spectroscopy (NMR)-based metabolomics is a powerful method for identifying metabolic perturbations that report on different biological states and sample types. Compared to mass spectrometry, NMR provides robust and highly reproducible quantitative data in a matter of minutes, which makes it very suitable for first-line clinical diagnostics. Although the metabolome is known to provide an instantaneous snap-shot of the biological status of a cell, tissue, and organism, the utilization of NMR in clinical practice is hindered by cumbersome data analysis. Major challenges include high-dimensionality of the data, overlapping signals, variability of resonance frequencies (chemical shift), non-ideal shapes of signals, and low signal-to-noise ratio (SNR) for low concentration metabolites. Existing approaches fail to address these challenges and sample analysis is time-consuming, manually done, and requires considerable knowledge of NMR spectroscopy. Recent developments in the field of sparse methods for machine learning and accelerated convex optimization for high dimensional problems, as well as kernel-based spatial clustering show promise at enabling us to overcome these challenges and achieve fully automated, operator-independent analysis. We are developing two novel, powerful, and automated algorithms that capitalize on these recent developments in machine learning. In Aim 1, we describe ‘NMRQuant’ for automated identification and quantification of annotated metabolites irrespective of the chemical shift, low SNR, and signal shape variability. In Aim 2, we describe ‘SPA-STOCSY’ for automated de-novo identification of molecular fragments of unknown, non- annotated metabolites. Based on substantial preliminary data, we propose to evaluate these algorithms' sensitivity, specificity, stability, and resistance to noise on phantom, biological, and clinical samples, comparing them to current methods. We will validate the accuracy of analyses by experimental 2D NMR, spike-in, and mass spectrometry. The proposed efforts will produce new NMR analytical software for discovery of both annotated and non-annotated metabolites, substantially improving accuracy and reproducibility of NMR analysis. Such analytical ability would change the existing paradigm of NMR-based metabolomics and provide an even stronger complement to current mass spectrometry-based methods. This approach, once thoroughly validated, will enable NMR to reach wide network of applications in biomedical, pharmaceutical, and nutritional research and clinical medicine. NARRATIVE This project seeks to develop an advanced and automated platform for identifying NMR metabolomics biomarkers of diseases and for fundamental studies of biological systems. When fully developed, these approaches could be used to detect small molecules in the blood or urine, indicative of the onset of various diseases, drug toxicity, or environmental effects on the organism.",Advanced Computational Approaches for NMR Data-mining,9608754,R01GM120033,"['Address', 'Algorithms', 'Animal Disease Models', 'Biological', 'Biological Markers', 'Blood', 'Cardiovascular Diseases', 'Cells', 'Chemicals', 'Clinic', 'Clinical', 'Clinical Medicine', 'Complement', 'Computer software', 'Consumption', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Drug toxicity', 'Early Diagnosis', 'Frequencies', 'Health', 'Human', 'Knowledge', 'Left', 'Libraries', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Mass Spectrum Analysis', 'Measures', 'Medical', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'NMR Spectroscopy', 'Nature', 'Neurodegenerative Disorders', 'Noise', 'Nuclear Magnetic Resonance', 'Nutritional', 'Obesity', 'Organism', 'Outcome', 'Patients', 'Pharmacologic Substance', 'Phenotype', 'Plague', 'Process', 'Regulation', 'Relaxation', 'Reporting', 'Reproducibility', 'Research', 'Residual state', 'Resistance', 'Sampling', 'Sensitivity and Specificity', 'Shapes', 'Signal Transduction', 'Societies', 'Sodium Chloride', 'Spectrum Analysis', 'Statistical Algorithm', 'Structure', 'Temperature', 'Time', 'Tissues', 'Treatment outcome', 'Urine', 'Variant', 'automated analysis', 'base', 'biological systems', 'biomarker discovery', 'clinical diagnostics', 'clinical implementation', 'clinical practice', 'computational suite', 'data mining', 'experimental analysis', 'experimental study', 'high dimensionality', 'improved', 'infancy', 'metabolome', 'metabolomics', 'multidimensional data', 'novel', 'personalized medicine', 'phenotypic biomarker', 'small molecule', 'stem']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2019,356625,323604360,0.01289135021324345
"A platform for mining, visualization and design of microbial interaction networks Project Summary One of the burning questions in the study of the human microbiome is whether and how it is possible to design specific strategies for rebalancing the taxonomic and functional properties of human-associated microbial communities, triggering the transition from “disease states” to “healthy states”. While empirical studies provide strong support for the idea that we may be able to cure, or at least  treat, a number of diseases by simply transplanting microbiomes, or inducing changes through taxonomic or environmental perturbations, to date little mechanistic understanding exists on how microbial communities work, and on how to extend microbiome research from an empirical science to a systematic, quantitative field of biomedicine. We propose here to establish a computational platform--   a database (Aim 1) with fully integrated analytical software (Aims 2 and 3) --- developed for and with the cooperation of the scientific community. The resource goes beyond cataloguing microbial abundances under different condition; its aim is to enable an understanding of networks of interacting species and their condition-dependence, with the goal of eventually facilitating disease diagnosis and prognosis, and designing therapeutic strategies for microbiome intervention. Our project is centered around three key aims: 1.	The creation of a Microbial Interaction Network Database (MIND), a public resource that will collect data on inter-species interactions from metagenomic sequencing projects, computer simulations and direct experiments. This database will be accessed through a web-based platform complemented with tools for microbial interaction network analysis and visualization, akin to highly fruitful tools previously developed for the study of genetic networks; the database will also serve as the public repository of microbial networks associated with human diseases; 2.	The implementation of an integrated tool for simulation of interspecies interactions under different environments, based on genomic data and whole-cell models of metabolism; 3.	The implementation of new algorithms for microbial community analysis and engineering. These algorithms, including stoichiometric, machine-learning and statistical approaches will facilitate a “synthetic ecology” approach to help design strategies (e.g. microbial transplants or probiotic mixtures) for preventing and targeting microbiome-associated diseases. Our work will fill a major gap in current microbiome research, creating the first platform for global microbial interaction data integration, mining and computation. Project Narrative Among the major developments of the genomic revolution has been the ability to identify thousands of microbial species and strains living in communities in 5 major habitats in the human body, and the recognition that the relative abundances of these populations is strongly correlated with environment: disease state, diet, treatment protocol and so on. A major challenge in utilizing the deluge of health relevant data is structuring it into a database that facilitates understanding inter-microbial interactions in these communities. The aim of this proposal is to create a database and integrated computational platform, open to and contributed to by the research community, which will greatly accelerate the conversion of data into health related actionable knowledge.","A platform for mining, visualization and design of microbial interaction networks",9638561,R01GM121950,"['Affect', 'Algorithms', 'Cataloging', 'Catalogs', 'Cell model', 'Clinical', 'Communities', 'Complement', 'Complex', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Dependence', 'Development', 'Diet', 'Discipline', 'Disease', 'Ecology', 'Ecosystem', 'Empirical Research', 'Engineering', 'Environment', 'Evolution', 'Future', 'Genetic', 'Genetic study', 'Genome', 'Genomics', 'Goals', 'Habitats', 'Health', 'Human Biology', 'Human Microbiome', 'Human body', 'Imagery', 'Individual', 'Intervention', 'Knowledge', 'Laboratories', 'Machine Learning', 'Measurable', 'Mediating', 'Metabolic', 'Metabolism', 'Metadata', 'Methods', 'Microbe', 'Mining', 'Nature', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pattern', 'Population', 'Preventive Medicine', 'Probiotics', 'Property', 'Research', 'Resources', 'Science', 'Scientist', 'Structure', 'Taxonomy', 'Technology', 'Therapeutic', 'Time', 'Transplantation', 'Treatment Protocols', 'Work', 'base', 'computational platform', 'computer framework', 'data integration', 'data to knowledge', 'design', 'disease diagnosis', 'experimental study', 'feeding', 'genome-wide', 'genomic data', 'human disease', 'human microbiota', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microbiome research', 'microbiota transplantation', 'microorganism interaction', 'novel diagnostics', 'novel therapeutics', 'open source', 'outcome forecast', 'prevent', 'repository', 'simulation', 'tool', 'user-friendly']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2019,364865,61050884,0.006708013419382738
"Lagrangian computational modeling for biomedical data science The goal of the project is to develop a new mathematical and computational modeling framework for from biomedical data extracted from biomedical experiments such as voltages, spectra (e.g. mass, magnetic resonance, impedance, optical absorption, …), microscopy or radiology images, gene expression, and many others. Scientists who are looking to understand relationships between different molecular and cellular measurements are often faced with questions involving deciphering differences between different cell or organ measurements. Current approaches (e.g. feature engineering and classification, end-to-end neural networks) are often viewed as “black boxes,” given their lack of connection to any biological mechanistic effects. The approach we propose builds from the “ground up” an entirely new modeling framework build based on recently developed invertible transformation. As such, it allows for any machine learning model to be represented in original data space, allowing for not only increased accuracy in prediction, but also direct visualization and interpretation. Preliminary data including drug screening, modeling morphological changes in cancer, cardiac image reconstruction, modeling subcellular organization, and others are discussed. Mathematical data analysis algorithms have enabled great advances in technology for building predictive models from biological data which have been useful for learning about cells and organs, as well as for stratifying patient subgroups in different diseases, and other applications. Given their lack to fundamental biophysics properties, the modeling approaches in current existence (e.g. numerical feature engineering, artificial neural networks) have significant short-comings when applied to biological data analysis problems. The project describes a new mathematical data analysis approach, rooted on transport and related phenomena, which is aimed at greatly enhance our ability to extract meaning from diverse biomedical datasets, while augmenting the accuracy of predictions.",Lagrangian computational modeling for biomedical data science,9642618,R01GM130825,"['3-Dimensional', 'Accountability', 'Address', 'Algorithmic Analysis', 'Area', 'Biological', 'Biological Models', 'Biology', 'Biophysics', 'Brain', 'Cancer Detection', 'Cartilage', 'Cell model', 'Cells', 'Classification', 'Collaborations', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Analyses', 'Data Reporting', 'Data Science', 'Data Scientist', 'Data Set', 'Development', 'Disease', 'Drug Screening', 'Effectiveness', 'Engineering', 'Flow Cytometry', 'Fluorescence', 'Gene Expression', 'Generations', 'Goals', 'Heart', 'Image', 'Imagery', 'Knee', 'Laboratories', 'Learning', 'Letters', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Medical Imaging', 'Methodology', 'Modeling', 'Molecular', 'Morphology', 'Optics', 'Organ', 'Performance', 'Plant Roots', 'Population', 'Pythons', 'Research', 'Scientist', 'Signal Transduction', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Universities', 'Virginia', 'absorption', 'artificial neural network', 'base', 'biophysical properties', 'brain morphology', 'cellular imaging', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data space', 'deep learning', 'deep neural network', 'electric impedance', 'experimental study', 'graphical user interface', 'gray matter', 'heart imaging', 'image reconstruction', 'learning strategy', 'mathematical algorithm', 'mathematical model', 'mathematical theory', 'microscopic imaging', 'models and simulation', 'neural network', 'patient stratification', 'patient subsets', 'predictive modeling', 'radiological imaging', 'technology research and development', 'tool', 'voltage']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2019,375602,169622494,0.003453196574271495
"Interpretable and extendable deep learning model for biological sequence analysis and prediction Project Abstract Bioinformatics and computational biology have become the core of biomedical research. The PI Dr. Dong Xu's work in this area focuses on development of novel computational algorithms, software and information systems, as well as on broad applications of these tools and other informatics resources for diverse biological and medical problems. He works on many research problems in protein structure prediction, post-translational modification prediction, high-throughput biological data analyses, in silico studies of plants, microbes and cancers, biological information systems, and mobile App development for healthcare. He has published more than 300 papers, with about 12,000 citations and H-index of 55. In this project, the PI proposes to develop deep-learning algorithms, tools, web resources for analyses and predictions of biological sequences, including DNA, RNA, and protein sequences. The availability of these data provides emerging opportunities for precision medicine and other areas, while deep learning as a cutting-edge technology in machine learning, presents a new powerful method for analyses and predictions of biological sequences. With rapidly accumulating sequence data and fast development of deep-learning methods, there is an urgent need to systematically investigate how to best apply deep learning in sequence analyses and predictions. For this purpose, the PI will develop cutting-edge deep-learning methods with the following goals for the next five years:  (1) Develop a series of novel deep-learning methods and models to specifically target biological sequence analyses and predictions in: (a) general unsupervised representations of DNA/RNA, protein and SNP/mutation sequences that capture both local and global features for various applications; (b) methods to make deep-learning models interpretable for understanding biological mechanisms and generating hypotheses; (c) “rule learning”, which abstracts the underlying “rules” by combining unsupervised learning of large unlabeled data and supervised learning of small labeled data so that it can classify new unlabeled data.  (2) Apply the proposed deep-learning model to DNA/RNA sequence annotation, genotype-phenotype analyses, cancer mutation analyses, protein function/structure prediction, protein localization prediction, and protein post-translational modification prediction. The PI will exploit particular properties associated with each of these problems to improve the deep-learning models. He will develop a set of related prediction and analysis tools, which will improve the state-of-art performance and shed some light on related biological mechanisms.  (3) Make the data, models, and tools freely accessible to the research community. The system will be designed modular and open-source, available through GitHub. They will be available like integrated circuit modules, which are universal and ready to plug in for different applications. The PI will develop a web resource for biological sequence representations, analyses, and predictions, as well as tutorials to help biologists with no computational knowledge to apply deep learning to their specific research problems. Relevance to Public Health Biological sequences, including DNA, RNA and protein sequences, represent the largest sources of growing big data in current biology and medicine, which provide tremendous opportunities for precision medicine, synthetic biology, and other areas. Deep learning as an emerging machine-learning method has a great potential in utilizing these data in biomedical research. This project will develop and apply cutting-edge deep- learning methods to deliver various sequence-based computational tools for gaining new knowledge, accelerating drug development, and improving personalized diagnosis and treatment.",Interpretable and extendable deep learning model for biological sequence analysis and prediction,9691995,R35GM126985,"['Algorithmic Software', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Big Data', 'Bioinformatics', 'Biological', 'Biological Models', 'Biology', 'Biomedical Research', 'Communities', 'Computational Biology', 'Computational algorithm', 'Computer Simulation', 'DNA', 'DNA Sequence', 'Data', 'Data Analyses', 'Development', 'Genotype', 'Goals', 'Healthcare', 'Information Systems', 'Knowledge', 'Label', 'Learning', 'Light', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Medicine', 'Methods', 'Microbe', 'Modeling', 'Mutation', 'Mutation Analysis', 'Paper', 'Performance', 'Phenotype', 'Plants', 'Plug-in', 'Post-Translational Protein Processing', 'Property', 'Proteins', 'Public Health', 'Publishing', 'RNA', 'RNA Sequences', 'Research', 'Resource Informatics', 'Sequence Analysis', 'Series', 'Source', 'System', 'Technology', 'Work', 'computerized tools', 'deep learning', 'deep learning algorithm', 'design', 'drug development', 'improved', 'indexing', 'learning strategy', 'mobile application', 'novel', 'online resource', 'open source', 'personalized diagnostics', 'personalized medicine', 'precision medicine', 'protein structure function', 'protein structure prediction', 'software systems', 'supervised learning', 'synthetic biology', 'tool', 'unsupervised learning']",NIGMS,UNIVERSITY OF MISSOURI-COLUMBIA,R35,2019,378183,63611576,0.025952459790816354
"Data-driven analysis of protein structure, function, and regulation PROJECT SUMMARY Proteins are capable of performing a wide variety of complex molecular functions and play a central role in all biological processes. A detailed and quantitative understanding of the relationship between a protein's sequence and it's biochemical properties would have a profound impact across all areas of biology, medicine, and biotechnology. We are developing data-driven approaches for dissecting the molecular basis of protein function. Our general framework involves designing informative libraries of protein sequences, experimentally mapping the relationship between sequence and function, and extracting detailed functional information from large sequence-function data sets. This work leverages emerging technologies and methods in DNA sequencing and synthesis, microfluidic screening, large-scale statistical learning, and optimization. We will develop generalizable platforms that can be applied to study a wide variety of enzymes and membrane transport proteins. PROJECT NARRATIVE A detailed and quantitative understanding of the relationship between a protein's sequence and it's biochemical properties would have a profound impact across all areas of biology, medicine, and biotechnology. This important capability would allow us to design new therapeutic and diagnostic proteins, and diagnose genetic diseases before they manifest symptoms. The goal of this proposal is to develop a general framework for dissecting the molecular basis of protein structure, function, and regulation.","Data-driven analysis of protein structure, function, and regulation",9744733,R35GM119854,"['Amino Acid Sequence', 'Area', 'Biochemical', 'Biological Process', 'Biology', 'Biotechnology', 'Complex', 'DNA biosynthesis', 'DNA sequencing', 'Data', 'Data Set', 'Emerging Technologies', 'Enzymes', 'Genetic Diseases', 'Goals', 'Libraries', 'Machine Learning', 'Medicine', 'Membrane Transport Proteins', 'Methods', 'Microfluidics', 'Molecular', 'Play', 'Property', 'Protein Analysis', 'Proteins', 'Regulation', 'Role', 'Symptoms', 'Work', 'design', 'genetic disorder diagnosis', 'novel diagnostics', 'novel therapeutics', 'protein function', 'protein structure function', 'screening']",NIGMS,UNIVERSITY OF WISCONSIN-MADISON,R35,2019,382500,338121506,0.03237762551668807
"Towards a comprehensive multiscale 3D human interactome network PROJECT SUMMARY/ABSTRACT Almost all proteins function through interacting with other proteins. On average, a protein interacts with ~5 other protein partners in the current human interactome. Therefore, it is of great importance to accurately determine the interface of each interaction, in order to understand how each protein works with different partners to carry out different functions. In our previous Nature Biotechnology study, we implemented a proteome-scale homology modeling approach to generate the first 3D human structural interactome: the interface for each interaction in this network was determined at atomic resolution through co-crystal structures and homology models. Using our 3D interactome, we found that, among >1,800 known disease genes associated with two or more clinically distinctly disorders, pairs of mutations on the same gene but in different interfaces with different partners are significantly more likely to cause distinct diseases. However, only 4,150 human protein interactions have co-crystal structures and 2,921 have high-quality homology models. ~50,000 interactions (87% of the current human interactome) are not amenable to current structural modeling methods. Here, we propose to develop a big-data-driven machine-learning approach integrating biophysiochemical, evolutionary, structural, and population genetic features to identify interaction- specific interfaces for the whole human interactome. Because several key features are unavailable for many proteins and interactions, we propose an innovative approach to use an ensemble of random forest classifiers, named Ensemble Protein Interface Classifier (EPIC), to address this large-scale non-random missing data problem (Aim 1). The high throughput of our massively parallel Clone-seq and INtegrated PrOtein INteractome perTurbation screening (InPOINT) pipeline! uniquely enables us to perform real-time experimental parameter optimization (in Years 2-4 we will clone ~1,500 mutations and examine their impact on ~2,500 interactions every year to iteratively evaluate and refine EPIC; Aim 2). Finally, we will construct a comprehensive multiscale 3D interactome for all known human protein-protein interactions: we will collect/generate atomic- resolution structural models for interactions whenever possible (co-crystal structures and homology models); we will accurately determine interaction-specific interface residues and domains for the whole human interactome. We will deploy an interactive web portal to disseminate our results and allow functional genomic inference in the context of our structural interactome (Aim 3). Our comprehensive multiscale 3D human interactome and the accompanying web portal will greatly reduce the barrier-to-entry for performing systematic structural analysis on a large number of proteins and their interactions, and open the flood gates for such analyses in genomic studies. NARRATIVE Almost all proteins function through interacting with other proteins and the structural details of these interaction interfaces are key in understanding protein function. However, the interfaces for vast majority of human protein interactions are currently unknown. Here, we propose to establish an innovative ensemble classifier approach and implement an unprecedented large-scale computational-experimental iterative learning scheme to predict interfaces for the whole human interactome, in anticipation that our predicted interfaces will help dissect functional sites of disease mutations and be useful for rational drug design to target these sites.",Towards a comprehensive multiscale 3D human interactome network,9731549,R01GM124559,"['3-Dimensional', 'Address', 'Big Data', 'Binding', 'Biotechnology', 'Cells', 'Clinical', 'Crystallization', 'Data', 'Disease', 'Docking', 'Drug Design', 'Evolution', 'Floods', 'Genes', 'Genomics', 'Goals', 'Graph', 'Homology Modeling', 'Human', 'Individual', 'Learning', 'Letters', 'Machine Learning', 'Methods', 'Missense Mutation', 'Modeling', 'Molecular', 'Mutation', 'Names', 'Nature', 'Population Genetics', 'Proteins', 'Proteome', 'Publishing', 'Resolution', 'Resources', 'Scheme', 'Site', 'Structural Models', 'Structure', 'Study models', 'Techniques', 'Testing', 'Time', 'Variant', 'Work', 'biological research', 'functional genomics', 'human interactome', 'improved', 'innovation', 'learning strategy', 'protein function', 'protein protein interaction', 'protein structure', 'random forest', 'screening', 'three dimensional structure', 'user-friendly', 'web portal', 'web-accessible']",NIGMS,CORNELL UNIVERSITY,R01,2019,384166,91477866,0.0197534589109463
"Defining the multi-dimensional code of zinc finger specificity-Resubmission-1 Project Summary The Cys2His2 zinc finger DNA-binding domain is the most common domain in human yet the DNA-binding specificities for the great majority of these proteins remain undefined. Mutations in many of these domains, both with and without known DNA-binding data, have been linked to a host of diseases from Alzheimers (REST) to Cancer (e.g. Slug, WT1, CTCF). Therefore, the characterization of these proteins holds great value. Unfortunately common methodologies used to determine the DNA-binding specificity of transcription factors have failed to address the zinc finger, at least in part because of an inability to fully define the large target specificities required of the average mammalian zinc finger protein. Even when ChIP-Seq data exists it is limited because the size of the genome does not allow us to capture the full binding potential of a factor that could offer a ≥21bp target sequence. As a result, without a comprehensive understanding of a protein’s binding potential, SNPs across the genome will continue to represent potential binding sites that we are unable to predict. In sum, decades of research have enlightened our understanding of this domain but we are still in the dark when it comes to its function as a transcription factors. Recently we have taken an alternative approach to define this domain, demonstrating that a synthetic, one-by-one screen of individual zinc fingers allows us to predict the specificity of multi-fingered proteins with similar or greater accuracy than all prior prediction algorithms. However, this approach fails to take into consideration the influences that adjacent fingers have on one another. We have produced the equivalent of a comprehensive snapshot of what a zinc finger is capable of in just one of many potential contextual environments. Here we propose to scale this approach and screen the zinc finger under an inclusive set of contextual environments. We will consider the most common direct and indirect influences on adjacent finger binding as well as factors that impact the geometry with which the zinc fingers engage the DNA. We will use these results to provide a complete picture of how adjacent zinc fingers determine their specificity and by scaffolding these two-fingered models, predict and design the specificity of large, multi-fingered proteins. In this way, we will define a multi-dimensional code of zinc finger specificity that allows us to predict all zinc finger DNA-binding specificities, how any neighbor finger context would modify this specificity, and the factors that result in adjacent finger incompatibility and loss of DNA-binding function. We will apply this model to predict the specificity of all human zinc finger proteins, validate these predictions through in vivo characterization of an informed set of transcription factors, and test predicted mechanisms of multi-fingered binding with designer, artificial factors. Project Narrative The proposed research is relevant to public health because the ZF domain is the most common in human yet it remains largely uncharacterized. A holistic understanding of ZF function will provide insight into how ZF mutations are related to disease and allow us to predict harmful binding sites due to SNPs across the genome.",Defining the multi-dimensional code of zinc finger specificity-Resubmission-1,9627995,R01GM118851,"['Achievement', 'Address', 'Alzheimer&apos', 's Disease', 'Amino Acids', 'Binding', 'Binding Proteins', 'Binding Sites', 'ChIP-seq', 'Charge', 'Code', 'Collection', 'Communities', 'Comprehension', 'DNA', 'DNA Binding', 'DNA Binding Domain', 'Data', 'Disease', 'Distal', 'Environment', 'Exposure to', 'Fingers', 'Genetic Transcription', 'Genome', 'Geometry', 'Goals', 'Human', 'Hybrids', 'Individual', 'Lead', 'Libraries', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Mammals', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Nucleotides', 'Proteins', 'Public Health', 'Reporter', 'Research', 'Sampling', 'Schizophrenia', 'Series', 'Specificity', 'Structure', 'Sum', 'System', 'Systems Biology', 'Testing', 'WT1 gene', 'Work', 'Zinc Fingers', 'base', 'design', 'exhaustion', 'experimental study', 'in vivo', 'insight', 'loss of function', 'model design', 'prediction algorithm', 'predictive modeling', 'predictive test', 'scaffold', 'screening', 'slug', 'transcription factor', 'user-friendly', 'web site']",NIGMS,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,R01,2019,393339,329565273,0.003535696619737962
"Joint Analysis of Microbiome and Other Genomic Data Types PROJECT SUMMARY In the same way that the human genome project created invaluable genomic maps, the objective of this project is to develop methods for eventual construction of comprehensive genetic and metabolomic by microbome relationship maps. Such maps would be an invaluable resource for improving our understanding as to the underlying mechanisms by which microbes and –omics features influence human diseases and conditions, potentially leading to identification of novel therapeutic targets. To these ends, this proposal seeks to develop statistical and computational tools for mapping associations and interactions between microbes and other – omic features and for further utilizing other –omics to improve microbiome based prediction models. Specifically, motivated by studies examining the role of the vaginal microbiome and other –omics in birth outcomes and menopause, we aim to develop statistical methodology for (1) mapping genetic variants that influence microbiome composition so as to understand the innate component of the microbiome as well as learn mechanisms by which genetics influence outcomes; (2) creating global metabolic maps integrating both microbes and metabolites which will enable understanding of how perturbations might influence the system and identify key pathways for therapeutic target; (3) exploiting other –omics in constructing more accurate microbiome based prediction models for preterm birth; (4) developing, distributing and supporting software packages for the proposed methods. The methods are based on frameworks in which we have considerable experience, but novel technical contributions are made to accommodate features of the data such as population stratification and relatedness in genetics, phylogenetic structure, and compositionality, as well as practical considerations such as availability of samples and other –omics data. Consequently, these new methods have the potential for accelerating mechanistic and translational microbiome studies, developing vital resources for enabling systematic achievement of many biological, clinical, and public health problems that have eluded researchers for decades. PROJECT NARRATIVE The methods developed in this proposal will enable improved understanding of the interactions between microbes and other –omics, thus aiding in elucidation of the mechanisms by which microbes and –omic features influence health outcomes and aiding in identification of potential molecular targets. Further emphasis is placed on utilization of other –omics to develop microbiome based prediction models in pregnancy outcomes, improving early detection of women who are at risk of preterm delivery.",Joint Analysis of Microbiome and Other Genomic Data Types,9763572,R01GM129512,"['Achievement', 'Address', 'Area', 'Automobile Driving', 'Biological', 'Birth', 'Chromosome Mapping', 'Clinical', 'Complex', 'Computer software', 'Data', 'Data Set', 'Development', 'Early Diagnosis', 'Future', 'Genes', 'Genetic', 'Genomics', 'Grouping', 'Health', 'Human Genome Project', 'Joints', 'Lasso', 'Learning', 'Machine Learning', 'Maintenance', 'Maps', 'Medical', 'Menopause', 'Metabolic', 'Methodology', 'Methods', 'Microbe', 'Modeling', 'Modification', 'Molecular Target', 'Network-based', 'Outcome', 'Pathway interactions', 'Performance', 'Phylogenetic Analysis', 'Pregnancy Outcome', 'Premature Birth', 'Procedures', 'Public Health', 'Quantitative Trait Loci', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'Role', 'Sampling', 'Scientific Advances and Accomplishments', 'Structure', 'System', 'Taxonomy', 'Training', 'Woman', 'Work', 'base', 'biological systems', 'computerized tools', 'experience', 'genetic variant', 'genomic data', 'human disease', 'improved', 'interest', 'loss of function', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microbiome analysis', 'microbiome components', 'microbiome composition', 'microbiome research', 'new therapeutic target', 'novel', 'open source', 'population stratification', 'predictive modeling', 'simulation', 'software development', 'therapeutic target', 'tool', 'translational study', 'user friendly software', 'vaginal microbiome']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2019,396000,758431960,-0.008849231567807293
"Research Resource for Complex Physiologic Signals PhysioNet, established in 1999 as the NIH-sponsored Research Resource for Complex Physiologic Signals, has attained a preeminent status among biomedical data and software resources. Its data archive, PhysioBank, was the first, and remains the world's largest, most comprehensive and widely used repository of time-varying physiologic signals. PhysioToolkit, its software collection, supports exploration and quantitative analyses of PhysioBank and similar data with a wide range of well-documented, rigorously tested open-source software that can be run on any platform. PhysioNet's team of researchers leverages results of other funded projects to drive the creation and enrichment of: i) Data collections that provide increasingly comprehensive, multifaceted views of pathophysiology over long time intervals, such as the MIMIC III (Medical Information Mart for Intensive Care) Database of critical care patients; ii) Analytic methods that lead to more timely and accurate diagnoses of major public health problems (such as life-threatening cardiac arrhythmias, infant apneas, fall risk in older individuals and those with neurologic disease, and seizures), and iii) Elucidation of dynamical changes associated with a variety of pathophysiologic processes and aging (such as cardiopulmonary interactions during sleep disordered breathing syndromes); User interfaces, reference materials and services that add value and improve accessibility to PhysioNet's data and software (such as PhysioNetWorks, a virtual laboratory for data sharing). Impact: Cited in The White House Fact Sheet on Big Data Across the Federal Government (March 29, 2012), PhysioNet is a proven enabler and accelerator of innovative research by investigators with a diverse range of interests, working on projects made possible by data that are inaccessible otherwise. The creation and development of PhysioNet were recognized with the 2016 highest honor of the Association for the Advancement of Medical Instrumentation (AAMI). PhysioNet's world- wide, growing community of researchers, clinicians, educators, students, and medical instrument and software developers, retrieve about 380 GB of data per day. By providing free access to its unique and wide-ranging data and software collections, PhysioNet is invaluable to studies that currently result in an impressive average of nearly 250 new scholarly articles per month by academic, clinical, and industry-affiliated researchers worldwide. Over the next year we aim to sustain and enhance PhysioNet's impact with new technology and data; and complete the 2019 PhysioNet/Computing in Cardiology Challenge on sepsis. PhysioNet, the Research Resource for Complex Physiological Signals, maintains the world's largest, most comprehensive and most widely used repository of physiological data and data analysis software, making them freely available to the research community. PhysioNet is a proven enabler and accelerator of innovative biomedical research through its unique role in providing data and other resources that otherwise would be inaccessible.",Research Resource for Complex Physiologic Signals,9993811,R01GM104987,"['Aging', 'Algorithms', 'Apnea', 'Area', 'Arrhythmia', 'Big Data', 'Biomedical Research', 'Boston', 'Bypass', 'Cardiology', 'Cardiopulmonary', 'Categories', 'Clinical', 'Clinical Data', 'Cloud Service', 'Collection', 'Communities', 'Community Outreach', 'Complex', 'Computer software', 'Critical Care', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Databases', 'Dedications', 'Development', 'Diagnostic radiologic examination', 'Entropy', 'FAIR principles', 'Federal Government', 'Functional disorder', 'Funding', 'Grant', 'Imagery', 'Individual', 'Industry', 'Infant', 'Infrastructure', 'Intensive Care', 'Israel', 'Journals', 'Laboratories', 'Lead', 'Licensing', 'Life', 'Link', 'Machine Learning', 'Maintenance', 'Medical', 'Medical center', 'Methods', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phase Transition', 'Physiological', 'Process', 'Public Health', 'Publishing', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Roentgen Rays', 'Role', 'Running', 'Seizures', 'Sepsis', 'Services', 'Signal Transduction', 'Sleep Apnea Syndromes', 'Source Code', 'Students', 'Switzerland', 'Syndrome', 'Testing', 'Thoracic Radiography', 'Time', 'United States National Institutes of Health', 'University Hospitals', 'Visit', 'accurate diagnosis', 'analytical method', 'clinical application', 'computerized data processing', 'computing resources', 'data archive', 'data sharing', 'experience', 'fall risk', 'heart rate variability', 'improved', 'innovation', 'instrument', 'instrumentation', 'interest', 'member', 'nervous system disorder', 'new technology', 'open source', 'preservation', 'repository', 'signal processing', 'software repository', 'symposium', 'time interval', 'virtual laboratory']",NIGMS,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2019,409563,135941803,0.010254839283120093
"3D temperature control to study biological processes Project Summary Temperature control technology is necessary for a broad range of biologically relevant processes including organ-on-chip operation, biomolecular kinetics, cell growth, studying gene function with temperature-sensitive mutations, cancer cell resistance to hyperthermia treatments, protein crystallization, and DNA analysis. Most biosensing devices lack the needed temperature measurement accuracy and precise temperature control to understand the thermal mechanisms of these processes. For example, temperature variations of 0.2°C can activate heat shock proteins, increasing the resistance of cancer cells to thermal ablation treatment, but reported temperature accuracies are often near ±1°C. This proposal aims to revolutionize the biomedical temperature measurement and control ecosystem by developing technology, models, and validated devices capable of microscopic, spatially resolved temperature sensing and control at ±0.1°C accuracy (10x better than what is used in most biosensing systems). Microfluidics is a promising technology for an extremely broad range of biomedical applications that notably lacks the necessary temperature accuracies and spatial temperature control to effectively study biothermal mechanisms. This proposal intends to impact human health by developing disruptive temperature control tools to accelerate biomedical innovation in thermally sensitive processes. Our group recently demonstrated the capacity to measure temperature at a single point with fluorescent dyes, achieving a ±0.05°C noise floor by using machine learning techniques. We have also 3D printed a cell-based genotype and phenotype assay device with cell growth chambers, monoliths for mRNA capture & fluorescence measurement, and integrated pumps and valves in a volume of only 2.2 mm × 2.2 mm × 1 mm. Aims 1 and 2 of this proposal will build on these successes by developing 3D printing technologies that easily incorporate complex temperature sensing, heating, and cooling channels, coupled with multi-physics/CAD models to rapidly iterate through the prototype development cycle. These advances will be used in Aim 3 to construct a microscopically temperature-controlled chip to measure DNA melt curves to determine the zygosity of a Factor 5 Leiden. This will show that the technology can detect the subtle difference in melting temperature that is undetectable by most PCR machines, as a proof-of-concept before the technology can be applied to other biological process. The overall objective of these studies is to develop a suite of affordable technologies researchers can use to understand biothermal mechanisms to lay the foundation for advances in disease diagnosis, treatment, and prevention. Project Narrative The instruments we use to study the effects of temperature on biological processes are less accurate than humans’ own ability to perceive temperature changes. The proposed research will develop improved microscopic temperature sensing & control technologies and demonstrate them by performing DNA analysis in a 3D printed device. Because the technology is cheap and accurate, it will be widely accessible to any lab, increasing our ability to understand the fundamental role biothermal processes have in disease occurrence, diagnosis, and treatment.",3D temperature control to study biological processes,9732034,R15GM132868,"['3-Dimensional', '3D Print', 'Automation', 'Biochemistry', 'Biological', 'Biological Assay', 'Biological Process', 'Biomedical Research', 'Biosensing Techniques', 'Blood specimen', 'Cells', 'Complex', 'Coupled', 'Crystallization', 'Custom', 'DNA', 'DNA analysis', 'Development', 'Devices', 'Diagnosis', 'Discipline', 'Disease', 'Ecosystem', 'Electrical Engineering', 'Engineering', 'Environment', 'Factor Analysis', 'Factor V', 'Floor', 'Fluorescence', 'Fluorescent Dyes', 'Foundations', 'Future', 'General Population', 'Genotype', 'Geometry', 'Goals', 'Grant', 'Health', 'Heat Stress Disorders', 'Heat shock proteins', 'Heating', 'High temperature of physical object', 'Hour', 'Human', 'Hyperthermia', 'Institution', 'Kinetics', 'Machine Learning', 'Measurement', 'Measures', 'Mechanics', 'Messenger RNA', 'Microfluidic Analytical Techniques', 'Microfluidic Microchips', 'Microfluidics', 'Microscopic', 'Modeling', 'Mutation', 'Noise', 'Outcome', 'Performance', 'Persons', 'Phenotype', 'Physics', 'Plant Resins', 'Prevention', 'Printing', 'Process', 'Proteins', 'Pump', 'Quantum Dots', 'Reagent', 'Reporting', 'Research', 'Research Personnel', 'Resistance', 'Resolution', 'Risk', 'Role', 'Sampling', 'Shapes', 'Single Nucleotide Polymorphism', 'Source', 'Spatial Distribution', 'System', 'Techniques', 'Technology', 'Temperature', 'Temperature Sense', 'Testing', 'Thermal Ablation Therapy', 'Thermometry', 'Thermoreceptors', 'Thromboembolism', 'Training', 'Variant', 'Venous', 'Work', 'base', 'biological systems', 'cancer cell', 'career', 'cell growth', 'computer science', 'design', 'disease diagnosis', 'experience', 'gene function', 'graduate student', 'hyperthermia treatment', 'improved', 'improved outcome', 'innovation', 'innovative technologies', 'instrument', 'melting', 'new technology', 'operation', 'organ on a chip', 'prototype', 'sensor', 'sensor technology', 'success', 'tool', 'undergraduate student']",NIGMS,BRIGHAM YOUNG UNIVERSITY,R15,2019,439058,6579725,-0.016889291736402502
"Probing transcriptional activation at the molecular level Project abstract Much of the assembly process of the transcription machinery is governed by transient and dynamic protein‐ protein interactions (PPIs) that defy standard characterization strategies. Transcriptional coactivators are the hubs of this process, interacting with transcriptional activators, epigenetic modulators, the polymerase, and other coactivators to assemble the transcriptional machine. Coactivators also represent a central molecular recognition conundrum, as the mechanisms by which they interact with such a diverse array with binding partners and the influence of local binding interactions on longer‐range structural and functional trajectories. In the previous funding period, we developed a molecular recognition model that addresses the first part of the conundrum. Here will build on this model to discover allosteric modulators of coactivators previously labelled undruggable despite their central functional roles. Additionally, we will answer the latter half of the conundrum to develop a comprehensive model of molecular recognition in transcriptional coactivator function. Project narrative Transcription is dysregulated in every human disease as either a cause or an effect and as such represents a potentially powerful intervention point for therapeutics; a significant impediment to progress are the many questions regarding the complex network of protein‐protein interactions that regulates this process. In this research plan we implement a combination of chemical biology tools and approaches to define key binding interactions and conformational changes that produce function in vitro and in cells. In doing so, we will discover new opportunities for drug discovery, with a particular focus on transcriptional pathways integral to metabolic disease and cancer.",Probing transcriptional activation at the molecular level,9840248,R01GM065330,"['Acids', 'Address', 'Algorithms', 'Architecture', 'Ataxia', 'Binding', 'Biology', 'Biophysics', 'Cells', 'Chemicals', 'Complex', 'Cryoelectron Microscopy', 'Data', 'Deuterium', 'Disease', 'EP300 gene', 'Electron Microscopy', 'Epigenetic Process', 'Family', 'Funding', 'Future', 'Genetic Transcription', 'Homeostasis', 'Hydrogen', 'In Vitro', 'Individual', 'Intervention', 'Label', 'Length', 'Ligands', 'Lipids', 'MED25 gene', 'Malignant Neoplasms', 'Mediating', 'Metabolic Diseases', 'Modeling', 'Molecular', 'Molecular Chaperones', 'Molecular Conformation', 'Negative Staining', 'Nuclear Receptors', 'Organism', 'Outcome', 'Pathway interactions', 'Peptides', 'Play', 'Polymerase', 'Process', 'Regulation', 'Reporting', 'Research', 'Resolution', 'Role', 'Structural Models', 'Structure', 'TP53 gene', 'Testing', 'Therapeutic', 'Time', 'Transcription Coactivator', 'Transcription Process', 'Transcriptional Activation', 'Transcriptional Activation Domain', 'Up-Regulation', 'biophysical techniques', 'drug discovery', 'flexibility', 'human disease', 'inhibitor/antagonist', 'machine learning algorithm', 'molecular modeling', 'molecular recognition', 'peptidomimetics', 'prediction algorithm', 'protein protein interaction', 'screening', 'small molecule', 'tool']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2019,440078,641965656,-0.0018121541932367184
"Automated Phenotyping in Epilepsy There are 65 million people worldwide with epilepsy and 150,000 new cases of epilepsy are diagnosed in the US annually. However, treatment options for epilepsy remain inadequate, with many patients suffering from treatment-resistant seizures, cognitive comorbidities and the negative side effects of treatment. A major obstacle to progress towards the development of new therapies is the fact that preclinical epilepsy research typically requires labor-intensive and expensive 24/7 video-EEG monitoring of seizures that rests on the subjective scoring of seizure phenotypes by human observers (as exemplified by the widely used Racine scale of behavioral seizures). Recently, the Datta lab showed that complex animal behaviors are structured in stereotyped modules (“syllables”) at sub-second timescales and arranged according to specific rules (“grammar”). These syllables can be detected without observer bias using a method called motion sequencing (MoSeq) that employs video imaging with a 3D camera combined with artificial intelligence (AI)-assisted video analysis to characterize behavior. Through collaboration between the Soltesz and Datta labs, exciting data were obtained that demonstrated that MoSeq can be adapted for epilepsy research to perform objective, inexpensive and automated phenotyping of mice in a mouse model of chronic temporal lobe epilepsy. Here we propose to test and improve MoSeq further to address long-standing, fundamental challenges in epilepsy research. This includes the development of an objective alternative to the Racine scale, testing of MoSeq as an automated anti-epileptic drug (AED) screening method, and the development of human observer- independent behavioral biomarkers for seizures, epileptogenesis, and cognitive comorbidities. In addition, we plan to dramatically extend the epilepsy-related capabilities of MoSeq to include the automated tracking of finer-scale body parts (e.g., forelimb and facial clonus) that are not possible with the current approach. Finally, we propose to develop the analysis pipeline for MoSeq into a form that is intuitive, inexpensive, user-friendly and thus easily sharable with the research community. We anticipate that these results will have a potentially transformative effect on the field by demonstrating the feasibility and power of automated, objective, user- independent, inexpensive analysis of both acquired and genetic epilepsy phenotypes. There is an urgent need for new therapies for patients with uncontrolled epilepsy. The project will develop breakthrough technologies involving artificial intelligence (AI)-assisted analysis of 3-dimensional video data of mouse behavior to address long-standing, fundamental challenges in preclinical epilepsy research. If successful, this innovative approach is expected to have a significant and sustained impact on epilepsy research by enabling investigators to perform objective, automated, inexpensive, reproducible assessment of epilepsy in experimental animals to aid the testing of anti-seizure drugs and other novel therapies.",Automated Phenotyping in Epilepsy,9862231,R01NS114020,"['3-Dimensional', 'Address', 'Animal Behavior', 'Animal Model', 'Animals', 'Antiepileptic Agents', 'Artificial Intelligence', 'Behavior', 'Behavioral', 'Biological Markers', 'Body part', 'Cannabidiol', 'Carbamazepine', 'Cells', 'Chronic', 'Clonazepam', 'Clonus', 'Cognitive', 'Collaborations', 'Communities', 'Comorbidity', 'Complex', 'Data', 'Development', 'Diagnosis', 'Drug Screening', 'Electroencephalography', 'Epilepsy', 'Epileptogenesis', 'Face', 'Forelimb', 'Future', 'Genetic', 'Genetic Models', 'Head', 'Hippocampus (Brain)', 'Human', 'Image', 'Imaging Techniques', 'Intervention', 'Intuition', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Mus', 'Observer Variation', 'Patients', 'Pharmaceutical Preparations', 'Phenotype', 'Phenytoin', 'Pilocarpine', 'Probability', 'Reproducibility', 'Research', 'Research Personnel', 'Resistance', 'Resolution', 'Rest', 'Seizures', 'Specificity', 'Stereotyping', 'Structure', 'Tail', 'Technology', 'Temporal Lobe Epilepsy', 'Testing', 'Three-dimensional analysis', 'Time', 'Treatment Side Effects', 'Valproic Acid', 'analysis pipeline', 'base', 'behavior test', 'drug use screening', 'improved', 'innovation', 'kainate', 'method development', 'mouse model', 'novel therapeutics', 'optogenetics', 'pre-clinical', 'pre-clinical research', 'selective expression', 'user-friendly']",NINDS,STANFORD UNIVERSITY,R01,2019,450365,560644462,-5.320053283198475e-05
"Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions People with mobility restrictions and limited to no upper extremity use depend on others to position the objects that they rely upon for health, communication, productivity, and general well-being. The technology developed in this project directly increases users’ independence by responding to them without another person’s intervention. The independence resulting from the proposed technology allows a user to perform activities related to self-care and communication. Eye tracking and head tracking access to speech devices, tablets, or computers requires very specific positioning. If the user’s position relative to the device are not aligned precisely, within a narrow operating window, the device will no longer detect the eyes or head, rendering the device inoperable. Auto-positioning technology uses computer vision and robotic positioning to automatically move devices to a usable position. The system moves without assistance from others, ensuring the user has continual access to communication. In a generalized application, the technology can target any area of the user’s body to position relevant equipment, such as for hydration or a head array for power wheelchair control. Research and development of the automatic positioning product will be accomplished through user-centered, iterative design, and design for manufacturing. Input from people with disabilities, their family members, therapists, and assistive technology professionals define the functional requirements of the technology and guide its evolution. Throughout technical development, prototype iterations are demonstrated and user-tested, providing feedback to advance the design. Design for manufacturability influences the outcomes to optimize the product pipeline, ensure high quality and deliver a cost effective product. Phase 1 Aims demonstrate the feasibility of 1) movement and control algorithms 2) face tracking algorithms and logic to maintain device positioning and 3) integration with commercial eye tracking device software development kits (SDK). Phase 2 Aims include technical, usability, and manufacturability objectives leading to development of a product for commercialization. Software development advances computer vision capabilities to recognize facial expressions and gestures. A new sensor module serves as a gesture switch or face tracker. App development provides the user interface, remote monitoring and technical support. Design of scaled down actuators and an enclosed three degree of freedom alignment module reduces the form factor, allowing for smaller footprint applications. Rigorous user testing by people with different diagnoses and end uses will ensure the product addresses a range of needs and is easy to use. Testing involves professionals and family members to evaluate ease of set-up, functionality, and customization. Extended user testing will investigate and measure outcomes and effects on their independence, access and health. Prototype tooling and design for cost-effective manufacturing will produce units for user and regulatory testing, and eventual sale. People who are essentially quadriplegic, with significant mobility impairments and limited to no upper extremity use, are dependent on others to maintain proper positioning for access to devices essential for their health, mobility and communication, such as eye gaze speech devices, call systems, phones, tablets, wheelchair controls, and hydration and suctioning tubes. Auto-positioning technology uses computer vision to detect specific targets, such as facial features, to control a robotic mounting and positioning system; automatically repositioning devices for best access without assistance from others, even when their position changes. This technology enables continuous access to communication, maintains one’s ability to drive a wheelchair and allows a person to drink or suction themselves, providing good hydration, respiratory health and hygiene.",Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions,9750520,R44HD093467,"['Address', 'Advanced Development', 'Algorithms', 'Area', 'Articular Range of Motion', 'Beds', 'Caring', 'Cerebral Palsy', 'Communication', 'Computer Vision Systems', 'Computers', 'Custom', 'Data', 'Dependence', 'Development', 'Devices', 'Diagnosis', 'Disabled Persons', 'Duchenne muscular dystrophy', 'Ensure', 'Equipment', 'Evolution', 'Eye', 'Face', 'Facial Expression', 'Family', 'Family member', 'Feedback', 'Freedom', 'Gestures', 'Goals', 'Head', 'Head and neck structure', 'Health', 'Health Communication', 'Hydration status', 'Hygiene', 'Immunity', 'Impairment', 'Individual', 'Intervention', 'Intuition', 'Joints', 'Lighting', 'Logic', 'Methods', 'Monitor', 'Motor', 'Movement', 'Oral cavity', 'Outcome', 'Outcome Measure', 'Personal Satisfaction', 'Persons', 'Phase', 'Positioning Attribute', 'Powered wheelchair', 'Productivity', 'Publishing', 'Quadriplegia', 'Quality of life', 'Research Design', 'Robotics', 'Safety', 'Sales', 'Saliva', 'Self Care', 'Self-Help Devices', 'Services', 'Signal Transduction', 'Spastic Tetraplegia', 'Speech', 'Spinal Muscular Atrophy', 'Spinal cord injury', 'Suction', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Tube', 'Update', 'Upper Extremity', 'Variant', 'Wheelchairs', 'Work', 'application programming interface', 'base', 'body system', 'commercialization', 'communication device', 'cost', 'cost effective', 'cost effectiveness', 'design', 'experience', 'gaze', 'improved', 'iterative design', 'manufacturability', 'meetings', 'product development', 'prototype', 'psychosocial', 'research and development', 'respiratory health', 'sensor', 'software development', 'tool', 'usability', 'user centered design']",NICHD,"BLUE SKY DESIGNS, INC.",R44,2019,507856,0,-0.036411331318849036
"Quantifying molecular consequences of human missense variants with large-scale interactome perturbation studies Almost all proteins function through interacting with other proteins. Previous studies have shown that the vast majority of damaging single amino acid mutations in proteins disrupt only a subset of specific protein-protein interactions, and that mutations in the same protein that disrupt different interactions tend to cause clinically distinct disorders. Therefore, it is of great importance to determine interaction-specific disruptions caused by each mutation. Furthermore, rapid advances in sequencing technologies have enabled the identification of tens of millions of single nucleotide variants (SNVs) in the human population, driving an urgent need to understand the impact of each SNV on the human interactome network. Unfortunately, there is currently no method that is capable of predicting the specific impact of a large fraction of these SNVs on individual protein-protein interactions. To address this issue, we propose to leverage our massively-parallel site-directed mutagenesis pipeline, Clone-seq, to generate clones for ~6,000 coding SNVs in the human population: ~4,000 from gnomAD and ~2,000 to be submitted by the international human genetics community. We will then experimentally examine the impact on protein stability and individual protein-protein interactions for every variant using high-throughput DUAL-FLUO and InPOINT (integrating PCA, LUMIER, Y2H, and wNAPPA) assays. This proposal brings together three groups with complementary expertise in high-throughput interactome experiments and network analysis from the Yu lab, in genomic and population genetic studies from the Clark lab, and in comprehensive biophysical and structural modeling of mutation’s impact on binding free energy of protein interactions from the Alexov lab. Out of the ~6,000 SNVs, we expect to identify ~1,200 disruptive SNVs and ~4,000 different SNV-interaction pairs where the SNV disrupt that specific interaction. The data produced by our project will increase the available experimental information by >140× in number of human proteins and >500× in number of interactions, allowing us for the first time to comprehensively assess the relationships between the impact of SNVs on interactions and their various population genetic attributes (including, but not limited to, allele frequency and flanking haplotype, inter-population differentiation, local rate of recombination, allele age, modes of selection). Finally, we will establish a computational-experimental- integrated iterative learning scheme to build a multi-layer random-forest-based framework, SIMPACT, which can accurately predict specific impacts on all individual protein-protein interactions for all missense SNVs. Our proposed work will fuel hypothesis-driven research, will significantly improve our functional understanding of variants, and will likely fundamentally change the experimental design and data interpretation for whole genome/exome studies going forward. The dramatic increase of DNA variants discovered through advances in sequencing technologies has been inadequately translated into therapeutic successes. Although many of these variants are related to human disorders, the overwhelming number of non-functional variants makes the assessment of functional significance a steep challenge. In this study, we aim to develop a high-throughput pipeline to quickly clone and directly test a large number of coding variants for their impact on the human interactome network and use the results to build a machine learning pipeline to predict functional impact of all coding variants, in anticipation that both our experimental data and computational pipeline will lead to broad clinical and therapeutic applications.",Quantifying molecular consequences of human missense variants with large-scale interactome perturbation studies,9629970,R01GM125639,"['Address', 'Age', 'Alleles', 'Amino Acids', 'Automobile Driving', 'Binding', 'Biochemical', 'Biological Assay', 'Biophysics', 'Cells', 'Clinical', 'Code', 'Communities', 'Coupling', 'Crystallization', 'DNA', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease', 'Experimental Designs', 'Free Energy', 'Gene Frequency', 'Genes', 'Genetic Recombination', 'Genetic study', 'Genome', 'Genomics', 'Haplotypes', 'Homology Modeling', 'Human', 'Human Genetics', 'Individual', 'International', 'Learning', 'Letters', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Molecular', 'Mutation', 'Nucleotides', 'Output', 'Pathway Analysis', 'Population', 'Population Genetics', 'Property', 'Proteins', 'Proteomics', 'Research', 'Resolution', 'Resources', 'Scheme', 'Site-Directed Mutagenesis', 'Structural Models', 'Structure', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Translating', 'Variant', 'Work', 'base', 'biological research', 'biophysical model', 'biophysical techniques', 'exome', 'experimental study', 'human interactome', 'improved', 'interest', 'molecular phenotype', 'mutant', 'next generation sequencing', 'protein function', 'protein protein interaction', 'random forest', 'screening', 'success', 'web portal', 'whole genome']",NIGMS,CORNELL UNIVERSITY,R01,2019,563248,91477866,0.007632084166965168
"Integrative Predictors of Temporomandibular Osteoarthritis ABSTRACT This application proposes the development of efficient web-based data management, mining, and analytics, to integrate and analyze clinical, biological, and high dimensional imaging data from TMJ OA patients. Based on our published results, we hypothesize that patterns of condylar bone structure, clinical symptoms, and biological mediators are unrecognized indicators of the severity of progression of TMJ OA. Efficiently capturing, curating, managing, integrating and analyzing this data in a manner that maximizes its value and accessibility is critical for the scientific advances and benefits that such comprehensive TMJ OA patient information may enable. High dimensional databases are increasingly difficult to process using on-hand database management tools or traditional processing applications, creating a continuing demand for innovative approaches. Toward this end, the DCBIA at the Univ. of Michigan has partnered with the University of North Carolina, the University of Texas MD Anderson Cancer Center and Kitware Inc. Through high-dimensional quantitative characterization of individuals with TMJ OA, at molecular, clinical and imaging levels, we will identify phenotypes at risk for more severe prognosis, as well as targets for future therapies. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA. Due to its ubiquitous design in the web, DSCI software installation will no longer be required. Our long-term goal is to create software and data repository for Osteoarthritis of the TMJ. Such repository requires maintaining the data in a distributed computational environment to allow contributions to the database from multi-clinical centers and to share trained models for TMJ classification. In years 4 and 5 of the proposed work, the dissemination and training of clinicians at the Schools of Dentistry at the University of North Carol, Univ. of Minnesota and Oregon Health Sciences will allow expansion of the proposed studies. In Aim 1, we will test state-of-the-art neural network structures to develop a combined software module that will include the most efficient and accurate neural network architecture and advanced statistics to mine imaging, clinical and biological TMJ OA markers identified at baseline. In Aim 2, we propose to develop novel data analytics tools, evaluating the performance of various machine learning and statistical predictive models, including customized- Gaussian Process Regression, extreme boosted trees, Multivariate Varying Coefficient Model, Lasso, Ridge and Elastic net, Random Forest, pdfCluster, decision tree, and support vector machine. Such automated solutions will leverage emerging computing technologies to determine risk indicators for OA progression in longitudinal cohorts of TMJ health and disease. PROJECT NARRATIVE This application proposes the development of efficient web-based data management, mining, and analytics of clinical, biological, and high dimensional imaging data from TMJ OA patients. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA.",Integrative Predictors of Temporomandibular Osteoarthritis,9739919,R01DE024450,"['3-Dimensional', 'Age', 'Architecture', 'Arthritis', 'Benchmarking', 'Biological', 'Biological Markers', 'Blood', 'Bone remodeling', 'Bone structure', 'Cancer Center', 'Chronic', 'Classification', 'Clinical', 'Clinical Markers', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Diagnosis', 'Country', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Base Management', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Decision Trees', 'Degenerative polyarthritis', 'Dental', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Environment', 'Fibrocartilages', 'Future', 'Gaussian model', 'Goals', 'Hand', 'Health', 'Health Sciences', 'Image', 'Image Analysis', 'Individual', 'Inflammation Mediators', 'Inflammatory', 'Internet', 'Joints', 'Lasso', 'Longitudinal cohort', 'Machine Learning', 'Mandibular Condyle', 'Mediator of activation protein', 'Medicine', 'Methods', 'Michigan', 'Mining', 'Minnesota', 'Modeling', 'Molecular', 'Morphology', 'North Carolina', 'Online Systems', 'Oregon', 'Outcome', 'Pain', 'Paper', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Phenotype', 'Process', 'Property', 'Proteins', 'Publishing', 'Replacement Arthroplasty', 'Resolution', 'Risk', 'Saliva', 'School Dentistry', 'Scientific Advances and Accomplishments', 'Severities', 'Slice', 'Structure', 'Study models', 'Symptoms', 'System', 'Technology', 'Temporomandibular Joint', 'Temporomandibular joint osteoarthritis', 'Testing', 'Texas', 'Three-Dimensional Imaging', 'Training', 'Trees', 'Universities', 'University of Texas M D Anderson Cancer Center', 'Work', 'X-Ray Computed Tomography', 'analytical tool', 'base', 'bone', 'cadherin 5', 'cartilage degradation', 'clinical diagnostics', 'cone-beam computed tomography', 'craniofacial', 'craniomaxillofacial', 'data warehouse', 'deep learning', 'deep neural network', 'design', 'high dimensionality', 'imaging biomarker', 'improved', 'innovation', 'joint destruction', 'machine learning algorithm', 'neural network', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'outcome forecast', 'predictive modeling', 'prospective', 'quantitative imaging', 'random forest', 'repository', 'scale up', 'screening', 'serial imaging', 'software repository', 'statistics', 'subchondral bone', 'tool']",NIDCR,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2019,588354,641965656,0.0028683574205746794
"An integrated electrical impedance myography platform for neuromuscular disease classification and diagnosis Project Summary  Improved methods for the bedside diagnosis and evaluation of neuromuscular disorders are needed. One technology that is finding increasing use for this purpose is electrical impedance myography (EIM). In EIM, a very weak, high frequency electrical current is passed through a muscle of interest and the resulting surface voltages are measured. Disease associated alterations in the composition and microstructural features of the muscle produce characteristic changes that can be used to help classify specific conditions and grade disease severity. To date, most studies using EIM analysis have utilized a fairly limited data set for disease assessment. While effective, this approach ignores a great deal of information locked within the impedance data, including those values that can assist in predicting specific muscle features (such as myofiber diameter) and the presence of pathological change (e.g., fat or connective tissue deposition). In addition, as it stands, the data set is challenging for the clinician to understand without a detailed knowledge of impedance theory. Myolex, Inc is a small business concern located in Boston, MA has as its main focus the development of EIM technologies for clinical use. Myolex recently completed a Phase 1 SBIR that demonstrated the potential capability of machine learning based classification algorithms to effectively discriminate healthy muscle from diseased and to discriminate one disease from another. In this proposed work, we will greatly advance this concept by embodying classification algorithms into a powerful new software suite for Myolex’s current EIM system, the mView. Our underlying hypothesis is that EIM data analysis can be automated to the point that classification systems can provide data on disease diagnosis as well as disease severity for improved ease-of-use. We propose to study this hypothesis via 2 specific aims. In Specific Aim 1, we will design a software suite capable of assisting with artifact-free data collection to be incorporated into our current EIM system, the mViewTM. Then using classification paradigms based on a prodigious amount of previous collected data, we will develop an automated data analysis tool to help provide data on disease category as well as microscopic features, muscle based on the impedance data alone using Microsoft’s Azure Cloud platform. In Specific Aim 2, we will test this developed software suite in a total of180 adult and pediatric neuromuscular disease patients and healthy participants evaluated at Ohio State University Wexner Medical Center (adults) and Boston Children’s Hospital (children). During this data collection period, the Ohio State and Boston Children’s researchers will have real- time access to Myolex staff to provide feedback and have questions/problems answered and addressed. The user interface will continue to be refined and classification algorithms improved. At the conclusion of this work, a new diagnostic tool will be developed for potential 510(k) FDA approval. It will serve as the basis for a continuously self-refining system as additional data sets are collected by end-users employing them in regular clinical use. Project Narrative  Electrical impedance myography (EIM) is a valuable technique to assist with the evaluation of a variety of conditions affecting nerve and muscle. However, to date, only simplistic EIM outcomes have been utilized to assess muscle condition. In this proposed work, we will develop a software platform using machine learning to be incorporated into current EIM technology to allow for automated diseased classification and characterization using the entire large EIM data set collected with each muscle measurement. This will serve as the basis for a new, powerful and convenient tool for neuromuscular diagnosis that will continue to advance over time.",An integrated electrical impedance myography platform for neuromuscular disease classification and diagnosis,9846955,R44NS113756,"['Address', 'Adult', 'Affect', 'Algorithmic Software', 'Amyotrophic Lateral Sclerosis', 'Area', 'Back Pain', 'Boston', 'Businesses', 'Caliber', 'Categories', 'Characteristics', 'Child', 'Childhood', 'Classification', 'Clinical', 'Complex', 'Computer software', 'Connective Tissue', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Set', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Duchenne muscular dystrophy', 'Effectiveness', 'Electrodes', 'Ensure', 'Evaluation', 'Fatty acid glycerol esters', 'Feedback', 'Fiber', 'Frequencies', 'Functional disorder', 'Health', 'Inclusion Body Myositis', 'Individual', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Medical Records', 'Medical Technology', 'Medical center', 'Methods', 'Microscopic', 'Morphologic artifacts', 'Muscle', 'Muscular Dystrophies', 'Myography', 'Myopathy', 'Myositis', 'Nerve', 'Neuromuscular Diseases', 'Neuromuscular conditions', 'Ohio', 'Outcome', 'Participant', 'Pathologic', 'Patients', 'Pediatric Hospitals', 'Performance', 'Phase', 'Physicians', 'Play', 'Positioning Attribute', 'Provider', 'Radiculopathy', 'Research Personnel', 'Role', 'Severities', 'Severity of illness', 'Small Business Innovation Research Grant', 'Specific qualifier value', 'Spinal Muscular Atrophy', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Universities', 'Work', 'base', 'classification algorithm', 'cloud based', 'cloud platform', 'commercialization', 'data acquisition', 'design', 'diagnosis evaluation', 'disease classification', 'disease diagnosis', 'electric impedance', 'improved', 'indexing', 'interest', 'machine learning algorithm', 'method development', 'nerve injury', 'neuromuscular', 'novel diagnostics', 'pediatric patients', 'physical therapist', 'prototype', 'sarcopenia', 'software development', 'success', 'theories', 'tool', 'usability', 'user friendly software', 'user-friendly', 'voltage']",NINDS,"MYOLEX, INC.",R44,2019,621318,869698,0.006769855608420727
"Blending deep learning with probabilistic mechanistic models to predict and understand the evolution and function of adaptive immune receptors Project Summary Scientific understanding of adaptive immune receptors (i.e. antibodies and T cell receptors) has the potential to revolutionize prophylaxis, diagnosis, and treatment of disease. High‐throughput DNA sequencing and functional experiments have now brought the study of adaptive immune receptors into the big‐data era. To realize this potential of these data they must be matched with appropriately powerful analytical techniques. Existing probabilistic and mechanistic models are insufficient to capture the complexities of these data, while a naïve application of machine learning cannot leverage our profound existing knowledge of the immune system. The goal of this project is to blend deep learning with mechanistic modeling in order to predict and understand the evolution and function of adaptive immune receptors. Aim 1: Develop generative models of immune receptor sequences that capture the complexity of real adaptive immune receptor repertoires. These will combine deep learning along with our knowledge of VDJ recombination, and provide a rigorous platform for detailed repertoire comparison. Aim 2: Develop quantitative mechanistic models of antibody somatic hypermutation that incorporate the underlying biochemical processes. Estimate intractable likelihoods using deep learning to infer important latent variables, and validate models using knock‐out experiments in cell lines. Aim 3: Develop hybrid deep learning models to predict binding properties from sequence data, combining large experimentally‐derived binding data with even larger sets of immune sequences from human immune memory samples. Incorporate structural information via 3D convolution or distance‐based penalties. These tools will reveal the full power of immune repertoire data for medical applications. We will obtain more rigorous comparisons of repertoires via their distribution in a relevant space. These will reveal the effects of immune perturbations such as vaccination and disease, allowing us to pick out sequences that are impacted by these perturbations. We will have a greater quantitative understanding of somatic hypermutation in vivo, and statistical models that appropriately capture long‐range effects of collections of mutations. We will also have algorithms that will be able to combine repertoire data and sparse binding data to predict binding properties. Put together, these advances will enable rational vaccine design, treatment for autoimmune disease, and identification of T cells that are promising candidates for cancer immunotherapy. Project Narrative Adaptive immune receptors (i.e. antibodies and T cell receptors) enable our body to fight off disease, “remember” pathogens, and train the immune system through vaccination. Immunologists have learned via high‐throughput sequencing that adaptive immune receptors have a truly remarkable diversity. In this proposal, we develop machine‐learning methods for these sequence data, which will allow us to predict the maturation, statistical distribution, and binding properties of adaptive immune receptors, and thus to better design vaccinations, autoimmune disease treatment, and immunotherapy treatment for cancer.",Blending deep learning with probabilistic mechanistic models to predict and understand the evolution and function of adaptive immune receptors,9800752,R01AI146028,"['3-Dimensional', 'Algorithms', 'Animal Model', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Architecture', 'Autoimmune Diseases', 'Automobile Driving', 'Big Data', 'Binding', 'Biochemical', 'Biochemical Process', 'Categories', 'Cell Line', 'Characteristics', 'Collection', 'Complement', 'Complex', 'Data', 'Data Set', 'Dependence', 'Diagnosis', 'Disease', 'Entropy', 'Evolution', 'Exposure to', 'Foundations', 'Gene Conversion', 'Generations', 'Goals', 'High-Throughput DNA Sequencing', 'High-Throughput Nucleotide Sequencing', 'Human', 'Hybrids', 'Immune', 'Immune response', 'Immune system', 'Immunoglobulin Somatic Hypermutation', 'Immunologic Memory', 'Immunologic Receptors', 'Immunological Models', 'Immunologics', 'Immunologist', 'Immunology', 'Immunotherapy', 'In Vitro', 'Individual', 'Knock-out', 'Knowledge', 'Laboratories', 'Machine Learning', 'Medical', 'Methods', 'Modeling', 'Modification', 'Mutation', 'Pathway interactions', 'Population', 'Procedures', 'Process', 'Property', 'Prophylactic treatment', 'Resolution', 'Sampling', 'Science', 'Statistical Distributions', 'Statistical Methods', 'Statistical Models', 'Structure', 'T-Cell Receptor', 'T-Lymphocyte', 'T-cell receptor repertoire', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Update', 'V(D)J Recombination', 'Vaccination', 'Vaccine Design', 'Vaccines', 'Validation', 'Work', 'analytical tool', 'base', 'biochemical model', 'cancer immunotherapy', 'cancer therapy', 'deep learning', 'deep neural network', 'deep sequencing', 'design', 'experimental study', 'fighting', 'functional group', 'in vivo', 'insertion/deletion mutation', 'learning strategy', 'markov model', 'pathogen', 'progenitor', 'receptor', 'repaired', 'response', 'success', 'three dimensional structure', 'tool']",NIAID,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2019,632498,758431960,-0.010099482473733464
"Integrating Ethics into Machine Learning for Precision Medicine The application of new computerized methods of data analysis to vast collections of medical, biological, and other data is emerging as a central feature of a broad vision of precision medicine (PM) in which systems based on artificial intelligence (AI) assist clinicians in treatment, diagnosis, or prognosis. The use of AI to analyze big data for clinical decision-making opens up a new domain for ELSI inquiry to address a possible future when the implications of genetics and genomics become embedded into algorithms, pervasive yet implicit and difficult to identify. Thus, an important target of inquiry is the development and developers of these algorithms. There are three distinctive features of the application of AI, and in particular machine learning (ML), to the domain of PM that create the need for ELSI inquiry. First, the process of developing ML-based systems for PM goals is technically and organizationally complex. Thus, members of development teams will likely have different expertise and assumptions about norms, responsibilities, and regulation. Second, machine learning does not solely operate through predetermined rules, and is thus difficult to hold accountable for its conclusions or reasoning. Third, designers of ML systems for PM may be subject to diverse and divergent interests and needs of multiple stakeholders, yet unaware of the associated ethical and values implications for design. These distinctive features of ML in PM could lead to difficulties in detecting misalignment of design with values, and to breakdown in responsibility for realignment. Because machine learning in the context of precision medicine is such a new phenomenon, we have very little understanding of actual practices, work processes and the specific contexts in which design decisions are made. Importantly, we have little knowledge about the influences and constraints on these decisions, and how they intersect with values and ethical principles. Although the field of machine learning for precision medicine is still in its formative stage, there is growing recognition that designers of AI systems have responsibilities to ask such questions about values and ethics. In order to ask these questions, designers must first be aware that there are values expressed by design. Yet, there are few practical options for designers to learn how to increase awareness. Our specific aims are: Aim 1 To map the current state of ML in PM by identifying and cataloging existing US-based ML in PM  projects and by exploring a range of values expressed by stakeholders about the use of ML in PM through  a combination of multi-method review, and interviews of key informants and stakeholders. Aim 2 To characterize decisions and rationales that shape ML in PM and explore whether and how  developers perceive values as part of these rationales through interviews of ML developers and site visits. Aim 3 To explore the feasibility of using design rationale as a framework for increasing awareness of the  existence of values, and multiple sources of values, in decisions about ML in PM through group-based  exercises with ML developers from academic and commercial settings. The overall goal of this project is to understand how to encourage and enable people who are developing artificial intelligence for personalized health care to be aware of values in their daily practice. We will examine actual practices and contexts in which design decisions are made for precision medicine applications, and use this information to design group-based workshop exercises to increase awareness of values.",Integrating Ethics into Machine Learning for Precision Medicine,9713512,R01HG010476,"['Address', 'Algorithms', 'Artificial Intelligence', 'Awareness', 'Big Data', 'Biological', 'Cataloging', 'Catalogs', 'Clinical', 'Collection', 'Complex', 'Computers', 'Data', 'Data Analyses', 'Development', 'Diagnosis', 'Educational workshop', 'Electronic Health Record', 'Engineering', 'Ethics', 'Evolution', 'Exercise', 'Expert Systems', 'Foundations', 'Future', 'Genetic', 'Genomics', 'Goals', 'Healthcare', 'Interview', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Outcome', 'Process', 'Regulation', 'Research', 'Resources', 'Sampling', 'Scholarship', 'Scientist', 'Shapes', 'Site Visit', 'Source', 'System', 'Time', 'Vision', 'Work', 'base', 'biobank', 'clinical decision-making', 'computerized', 'design', 'ethical legal social implication', 'genomic data', 'informant', 'innovation', 'interest', 'member', 'new technology', 'outcome forecast', 'personalized health care', 'precision medicine']",NHGRI,STANFORD UNIVERSITY,R01,2019,647706,560644462,0.008701397099824997
"PREMIERE: A PREdictive Model Index and Exchange REpository The confluence of new machine learning (ML) data-driven approaches; increased computational power; and access to the wealth of electronic health records (EHRs) and other emergent types of data (e.g., omics, imaging, mHealth) are accelerating the development of biomedical predictive models. Such models range from traditional statistical approaches (e.g., regression) through to more advanced deep learning techniques (e.g., convolutional neural networks, CNNs), and span different tasks (e.g., biomarker/pathway discovery, diagnostic, prognostic). Two issues have become evident: 1) as there are no comprehensive standards to support the dissemination of these models, scientific reproducibility is problematic, given challenges in interpretation and implementation; and 2) as new models are put forth, methods to assess differences in performance, as well as insights into external validity (i.e., transportability), are necessary. Tools moving beyond the sharing of data and model “executables” are needed, capturing the (meta)data necessary to fully reproduce a model and its evaluation. The objective of this R01 is the development of an informatics standard supporting the requisite information for scientific reproducibility for statistical and ML-based biomedical predictive models; from this foundation, we then develop new computational approaches to compare models' performance. We begin by extending the current Predictive Model Markup Language (PMML) standard to fully characterize biomedical datasets and harmonize variable definitions; to elucidate the algorithms involved in model creation (e.g., data preprocessing, parameter estimation); and to explain the validation methodology. Importantly, models in this PMML format will become findable, accessible, interoperable, and reusable (i.e., following FAIR principles). We then propose novel meth- ods to compare and contrast predictive models, assessing transportability across datasets. While metrics exist for comparing models (e.g., c-statistics, calibration), often the required case-level information is not available to calculate these measures. We thus introduce an approach to simulate cases based on a model's reported da- taset statistics, enabling such calculations. Different levels of transportability are then assigned to the metrics, determining the extent to which a selected model is applicable to a given population/cohort (i.e., helping answer the question, can I use this published model with my own data?). We tie these efforts together in our proposed framework, the PREdictive Model Index & Exchange REpository (PREMIERE). We will develop an online portal and repository for model sharing around PREMIERE, and our efforts will include fostering a community of users to guide its development through workshops, model-thons, and other activities. To demonstrate these efforts, we will bootstrap PREMIERE with predictive models from a targeted domain (risk assessment in imaging-based lung cancer screening). Our efforts to evaluate these developments will engage a range of stakeholders (model developers, users) to inform the completeness of our standard; and biostatisticians and clinical experts to guide assessment of model transportability. PROGRAM NARRATIVE With growing access to information contained in the electronic health record and other data sources, the appli- cation of statistical and machine learning methods are generating more biomedical predictive models. However, there are significant challenges to reproducing these models for purposes of comparison and application in new environments/populations. This project develops informatics standards to facilitate the sharing and reproducibil- ity of these models, enabling a suite of comparative methods to evaluate model transportability.",PREMIERE: A PREdictive Model Index and Exchange REpository,9712304,R01EB027650,"['Access to Information', 'Address', 'Algorithms', 'Area', 'Attention', 'Bayesian Network', 'Big Data', 'Biological Markers', 'Calibration', 'Characteristics', 'Clinical', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Decision Making', 'Decision Trees', 'Dermatology', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Ecosystem', 'Educational workshop', 'Electronic Health Record', 'Environment', 'Evaluation', 'FAIR principles', 'Fostering', 'Foundations', 'Goals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Language', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Ophthalmology', 'Pathway interactions', 'Performance', 'Population', 'Publications', 'Publishing', 'Radiology Specialty', 'Receiver Operating Characteristics', 'Reporting', 'Reproducibility', 'Reproduction', 'Research Personnel', 'Risk Assessment', 'Source', 'Techniques', 'Testing', 'Training', 'Validation', 'Variant', 'Work', 'base', 'bioimaging', 'biomarker discovery', 'case-based', 'cohort', 'collaborative environment', 'comparative', 'computer aided detection', 'convolutional neural network', 'data sharing', 'deep learning', 'design', 'experience', 'indexing', 'innovation', 'insight', 'interest', 'interoperability', 'learning network', 'learning strategy', 'lung basal segment', 'lung cancer screening', 'mHealth', 'model development', 'novel', 'novel strategies', 'predictive modeling', 'prognostic', 'repository', 'software repository', 'statistics', 'stem', 'tool']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2019,657823,673201228,-0.007657162372478689
"Accelerating phage evolution and tools via synthetic biology and machine learning Summary Phages, which are the naturally evolved predators of bacteria, may hold the key to combating bacterial pathogens, including the looming threat of multidrug resistant bacteria. Phages are viruses which while harmless to humans and have been successfully engineered as tools to separate, concentrate, and detect their bacterial hosts. Additionally, phages have been used as therapeutic agents to treat patients infected with pathogens resistant to known antibiotics. While the potential benefits of phages are numerous, certain limitations must be addressed in order to fully employ them. The central hypothesis of this proposal is that both top-down and bottom-up approaches can be utilized to design and synthesize novel phages, through a combination of synthetic biology and machine learning. This will result in phage-based tools with increased functionality and customizable host ranges. The rationale for the proposed research is that as the threat of bacterial infections including those with multi-drug resistance continues to grow, phages, which have evolved to efficiently recognize and kill bacteria, will become indispensable tools. Therefore, the ability to rapidly design and engineer new phages for biosensing and therapeutics will be a critical advantage to human health. The proposal contains three specific aims which are supported by preliminary data and cited literature. Aim 1: Site-directed conjugation for advanced phage-based biosensors and therapeutics. Under this aim, phages will be modified with alkyne-containing unnatural amino acids allowing their direct conjugation to 1) azide decorated magnetic nanoparticles, and 2) azide terminated polyethylene glycol. The modifications will allow the development of magnetic phages for bacteria separation and detection, and phages that are more effective therapeutics due to their ability to avoid a patient’s innate immune response, respectively. Aim 2: Decoding phage biorecognition elements using machine learning. In this aim, machine learning will be used to model the binding of phages and their bacterial hosts. The model will enable the prediction of host interactions as well as allow the design and synthesis of novel phage tail fibers which can target specific bacterial isolates. Aim 3: Repurposing phage biorecognition for a broader host ranges. Under the final aim, phage-binding proteins will be replaced with those known to recognize conserved regions of the bacterial LPS, resulting in a phage with a much broader host range. This approach is innovative because it uses top-down characterizations for bottom-up design and synthesis of novel phages. Traditional phage screening methods will be replaced with the rapid synthesis of phages, which are optimized for a particular bacterial isolate. Following the successful completion of the specific aims, the expected outcome is the design and synthesis of phages that can be used to target a selected group of bacteria within Enterobacteriaceae for advanced biosensing and therapeutics. A publically available computer model will allow rapid design of custom phage biorecognition elements which can be added to functionalized phages. These technologies will allow researchers to tip the scales of the co-evolutionary arms race between phage and bacteria. Narrative The project is relevant to public health because it accelerates the development of phage-based tools for the rapid detection of bacterial pathogens in human, food, and environmental samples, and the treatment of diseases from multidrug resistant bacteria by integrating machine learning and synthetic biology. Thus, it is specifically relevant to part of NIH's mission that pertains to the diagnosis, prevention, and cure of human diseases.",Accelerating phage evolution and tools via synthetic biology and machine learning,9714883,R01EB027895,"['Acinetobacter baumannii', 'Address', 'Alkynes', 'Amino Acid Sequence', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotics', 'Azides', 'Bacteria', 'Bacterial Genome', 'Bacterial Infections', 'Bacteriophage T4', 'Bacteriophages', 'Binding', 'Binding Proteins', 'Biosensing Techniques', 'Biosensor', 'CRISPR/Cas technology', 'Capsid', 'Chemistry', 'Clinical', 'Computer Simulation', 'Consumption', 'Custom', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Drug resistance', 'Elements', 'Engineering', 'Enterobacteriaceae', 'Environment', 'Escherichia coli', 'Evolution', 'Family', 'Fiber', 'Food', 'Future', 'Genes', 'Genome', 'Goals', 'Health', 'Human', 'Infection', 'Innate Immune Response', 'Innate Immune System', 'Intervention', 'Life', 'Literature', 'Machine Learning', 'Magnetic nanoparticles', 'Magnetism', 'Methods', 'Mission', 'Modeling', 'Modification', 'Multi-Drug Resistance', 'Multidrug-resistant Acinetobacter', 'Multiple Bacterial Drug Resistance', 'Natural Immunity', 'Outcome', 'Patients', 'Phenotype', 'Polyethylene Glycols', 'Prevention', 'Process', 'Property', 'Public Health', 'Race', 'Reporting', 'Research', 'Research Personnel', 'Resistance', 'Sampling', 'Site', 'Specificity', 'Surface', 'System', 'Tail', 'Technology', 'Therapeutic', 'Therapeutic Agents', 'Time', 'Training', 'Treatment Efficacy', 'United States National Institutes of Health', 'Viral', 'Virus', 'arm', 'base', 'combat', 'design', 'human disease', 'innovation', 'next generation', 'novel', 'pathogen', 'pathogenic bacteria', 'rapid detection', 'receptor', 'resistance mechanism', 'screening', 'synthetic biology', 'tool', 'unnatural amino acids']",NIBIB,CORNELL UNIVERSITY,R01,2019,666637,91477866,-0.009763284384976027
"Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Efforts to include behavioral measures in large-scale studies as envisioned by precision medicine are hampered by the time and expertise required. Paper-and-pencil tests currently dominating clinical assessment and neuropsychological testing are plainly unfeasible. The NIH Toolbox contains many computerized tests and clinical assessment tools varying in feasibility. Unique in the Toolbox is the Penn Computerized Neurocognitive Battery (CNB), which contains 14 tests that take one hour to administer. CNB has been validated with functional neuroimaging and in multiple normative and clinical populations across the lifespan worldwide, and is freely available for research. Clinical assessment tools are usually devoted to specific disorders, and scales vary in their concentration on symptoms that are disorder specific. We have developed a broad assessment tool (GOASSESS), which currently takes about one hour to administer. These instruments were constructed, optimized and validated with classical psychometric test theory (CTT), and are efficient as CTT allows. However, genomic studies require even more time-efficient tools that can be applied massively.  Novel approaches, based on item response theory (IRT) can vastly enhance efficiency of testing and clinical assessment. IRT shifts the emphasis from the test to the items composing it by estimating item parameters such as “difficulty” and “discrimination” within ranges of general trait levels. IRT helps shorten the length of administration without compromising data quality, and for many domains leads to computer adaptive testing (CAT) that further optimizes tests to individual abilities. We propose to develop and validate adaptive versions of the CNB and GOASSESS, resulting in a neurocognitive and clinical screener that, using machine learning tools, will be continually optimized, becoming shorter and more precise as it is deployed. The tool will be in the Toolbox available in the public domain. We have item-level information to perform IRT analyses on existing data and use this information to develop CAT implementations and generate item pools for adaptive testing. Our Specific Aims are: 1. Use available itemwise data on the Penn CNB and the GOASSESS and add new tests and items to generate item pools for extending scope while abbreviating tests using IRT-CAT and other methods. The current item pool will be augmented to allow large selection of items during CAT administration and add clinical items to GOASSESS. New items will be calibrated through crowdsourcing. 2. Produce a modular CAT version of a neurocognitive and clinical assessment battery that covers major RDoC domains and a full range of psychiatric symptoms. We have implemented this procedure on some CNB tests and clinical scales and will apply similar procedures to remaining and new tests as appropriate. 3. Validate the CAT version in 100 individuals with psychosis spectrum disorders (PS), 100 with depression/anxiety disorders (DA), and 100 healthy controls (HC). We will use this dataset to implement and test data mining algorithms that optimize prediction of specific outcomes. All tests, algorithms and normative data will be in the toolbox. Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Narrative Large scale genomic studies are done in the context of precision medicine, and for this effort to benefit neuropsychiatric disorders such studies should include behavioral measures of clinical symptoms and neurocognitive performance. Current tools are based on classical psychometric theory, and we propose to apply novel approaches of item response theory to develop a time-efficient adaptive tool for assessing broad neurocognitive functioning and psychopathology. The tool will be available in the public domain (NIH Toolbox) and will facilitate incorporation of psychiatric disorders into the precision medicine initiative.",Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan,9737676,R01MH117014,"['Algorithms', 'Anxiety', 'Anxiety Disorders', 'Assessment tool', 'Behavior', 'Biological Markers', 'Calibration', 'Characteristics', 'Classification', 'Clinical', 'Clinical Assessment Tool', 'Clinical assessments', 'Cognitive', 'Collection', 'Complex', 'Computers', 'Data', 'Data Compromising', 'Data Quality', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Discrimination', 'Disease', 'Environmental Risk Factor', 'Feedback', 'Female', 'Genomics', 'Hour', 'Individual', 'Internet', 'Internet of Things', 'Intervention Studies', 'Length', 'Link', 'Longevity', 'Machine Learning', 'Measures', 'Medicine', 'Mental Depression', 'Mental disorders', 'Methods', 'Molecular Genetics', 'Moods', 'Neurocognitive', 'Neurocognitive Deficit', 'Neuropsychological Tests', 'Neurosciences', 'Outcome', 'Paper', 'Pathway interactions', 'Performance', 'Phenotype', 'Population', 'Precision Medicine Initiative', 'Preparation', 'Preventive Intervention', 'Procedures', 'Psychiatry', 'Psychometrics', 'Psychopathology', 'Psychotic Disorders', 'Public Domains', 'Research', 'Research Domain Criteria', 'Sampling', 'Screening procedure', 'Sensitivity and Specificity', 'Severities', 'Speed', 'Structure', 'Symptoms', 'Tablets', 'Testing', 'Time', 'Translational Research', 'United States National Institutes of Health', 'Validation', 'base', 'behavior measurement', 'cognitive performance', 'computerized', 'crowdsourcing', 'data mining', 'digital', 'genomic variation', 'improved', 'individualized prevention', 'instrument', 'male', 'mobile computing', 'neuroimaging', 'neuropsychiatric disorder', 'novel', 'novel strategies', 'open source', 'precision medicine', 'protective factors', 'psychiatric symptom', 'response', 'symptom cluster', 'theories', 'tool', 'trait', 'validation studies']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2019,804907,593605914,-0.008867055270126819
"Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine The overall goal of this project is to develop a safe, broadly effective, and affordable vaccine to prevent group A streptococcal infections. Antibodies against the N-terminal hypervariable region (HVR) of surface M (Emm) proteins of GAS are opsonic and are associated with protection against infection. Immunity has classically been described as “type-specific”, leading to the assumption that natural immunity confers protection against only one of the more than 200 different emm types of GAS. We now have new information that calls into question this classic view and serves as the basis for an entirely different approach to GAS vaccine design and development. A recent comprehensive sequence analysis of M proteins from a global collection of 175 emm types of GAS resulted in a new emm cluster typing system that classified 96.2% of all contemporary GAS isolates into 48 emm clusters containing structurally and functionally related M proteins. Moreover, 117 emm types contained in 16 clusters accounted for 94.4% of GAS infections in the world. Indeed, preclinical studies indicated that a multivalent vaccine containing N-terminal peptides from 30 prevalent M types cross-opsonized a significant number of non-vaccine emm types of GAS that co-localized in clusters with vaccine emm types. The frequency of cross-opsonic antibodies, combined with the emm cluster data, prompted us to conclude that there is a need for a paradigm shift away from the concept of “type-specific” immunity against GAS infections to one of “cluster-specific” immunity. Our overall hypothesis is that immunity to GAS infections is the result of both type-specific and cross-reactive antibodies against the N-terminal regions of M proteins and that a new approach employing computational predictions of peptide structures will result in a multivalent vaccine that will induce broadly protective immunity in populations throughout the world. Our preliminary results indicate the feasibility of using structure-based design to predict the antigenic relatedness of M peptides within a cluster. The specific aims of this proposal are to: 1) Apply computational structure-based design in an iterative process with immunological data from Aim 2 to predict the minimal number of M peptide sequences that are most representative of the structural and physicochemical properties of the peptides in one emm cluster containing 17 GAS emm types, 2) determine the cross-reactive immunogenicity of the selected peptides with all seventeen emm types of GAS in the cluster, and apply the results to refine the computational design predictions in Aim 1, 3) apply the refined computational parameters from Aims 1 and 2 to analyze the remaining epidemiologically important emm clusters, select a comprehensive panel of peptides representing all emm types, construct four multivalent recombinant vaccine proteins, and assess potential cross-protective immunogenicity using in vitro bactericidal assays against all 117 emm types of GAS, and 4) determine the protective immunogenicity of the final multivalent vaccine in unique transgenic mice expressing human C4BP and factor H that will be immunized and then challenged with multiple emm types of GAS. The world needs an effective, safe and affordable vaccine to prevent group A streptococcal (GAS) infections. Although most GAS infections are mild, there are more than 18 million people with a chronic complication of a severe GAS disease worldwide, over 15 million of whom have rheumatic heart disease, another 2 million cases of severe disease occur each year and a total of 517,000 deaths annually are estimated to be due to this organism. Vaccine prevention of even a fraction of these life-threatening diseases could have a significant impact on the health of people around the world.",Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine,9724344,R01AI132117,"['Animals', 'Antibodies', 'Bacteria', 'Base Sequence', 'Binding', 'Biological Assay', 'Cell surface', 'Cells', 'Cessation of life', 'Chronic', 'Collection', 'Complement Factor H', 'Complementarity Determining Regions', 'Complication', 'Computer Analysis', 'Data', 'Development', 'Disease', 'Ensure', 'Enzyme-Linked Immunosorbent Assay', 'Epidemiology', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Human', 'Immune', 'Immune Sera', 'Immunity', 'Immunize', 'Immunologics', 'In Vitro', 'Infection', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Mus', 'N-terminal', 'Natural Immunity', 'Organism', 'Oryctolagus cuniculus', 'Peptide Vaccines', 'Peptide antibodies', 'Peptides', 'Population', 'Prevention', 'Process', 'Property', 'Proteins', 'Recombinant Vaccines', 'Recombinants', 'Rheumatic Heart Disease', 'Sequence Analysis', 'Streptococcal Infections', 'Streptococcal Vaccines', 'Streptococcus pyogenes', 'Streptococcus vaccine', 'Structure', 'Surface', 'System', 'Testing', 'Transgenic Mice', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'bactericide', 'base', 'cross reactivity', 'design', 'experimental study', 'flexibility', 'hybrid protein', 'immunogenic', 'immunogenicity', 'innovation', 'molecular dynamics', 'multiple myeloma M Protein', 'novel', 'novel strategies', 'peptide structure', 'preclinical study', 'prevent', 'protein aminoacid sequence', 'protein structure', 'retinal S antigen peptide M', 'synthetic peptide', 'tool', 'vaccine development', 'vaccine evaluation']",NIAID,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2019,872121,46216755,0.013057258446658905
"Advancing Protein Engineering Using Artificial Intelligence and the ProtaBank Mutation Database PROJECT SUMMARY Therapeutic antibodies, specialized enzymes for drug manufacturing, small molecule drug screening agents, and other proteins have been instrumental in advancing biotechnology and medicine. Protein therapeutics alone represents a rapidly growing $100+ billion market with broad applications in the treatment of cancer, inflammatory and metabolic diseases, and numerous other disorders. Most of the antibodies and other protein therapeutics developed in the last several years have been engineered, leading to improvements in important properties such as efficacy, binding affinity, expression, stability, and immunogenicity. However, improving protein properties through sequence modification remains a challenging task. Artificial intelligence (AI), which has been enormously successful in several fields (e.g., image recognition, self-driving cars, natural language processing), is now being applied to protein engineering and has the potential to transform this field as well. AI and machine learning (ML) can take advantage of large and diverse datasets to identify correlations, predict beneficial mutations, and explore novel protein sequences in ways that are not possible using other techniques. Other advantages include the ability to simultaneously optimize multiple protein properties and explore sequence space more efficiently. In Phases I and II of this project, we developed the ProtaBank database as a central repository to store, organize, and annotate protein mutation data spanning a broad range of properties. ProtaBank is the largest and only database actively collecting such a comprehensive set of sequence mutation data and is growing rapidly due to the wealth of data being generated with advanced automation and next-generation sequencing techniques. ProtaBank's depth and breadth makes it an ideal data source to train ML models. This proposal aims to create the ProtaBank AI Platform to enable the use of AI and ML tools to apply the data in ProtaBank to engineer proteins. The platform will provide fully customizable computational tools and will invoke protein-specific knowledge to properly prepare data for use with ML models. An interface to popular ML frameworks will be provided so that scientists can use these techniques to discover new predictive algorithms and enhance their ability to design proteins with the desired properties. Specific aims include: (1) integrating peer validated ML methods and proprietary technology for protein engineering into the ProtaBank AI Platform, (2) developing dynamic ML dataset creation tools, (3) expanding and improving the ProtaBank database by reaching out to scientists to contribute data, (4) enhancing our data deposition tools, and (5) integrating ProtaBank with the Protein Data Bank structure database and other databases. ! Project Narrative Protein engineering has enabled significant advances in health care by playing a key role in the development of antibodies and other protein therapeutics (e.g., for the treatment of cancer, inflammatory and metabolic diseases, and other disorders), highly selective enzymes for drug manufacturing, and novel proteins for use in diagnostics and the identification of new small molecule drugs. This project will enable the power of artificial intelligence (AI) to be applied to accelerate the engineering of proteins with new and improved properties. AI approaches can capitalize on the large amounts of protein mutation data being generated and stored in our recently developed ProtaBank protein mutation database to transform the way in which protein therapeutics and reagents are discovered and developed.!",Advancing Protein Engineering Using Artificial Intelligence and the ProtaBank Mutation Database,9847841,R44GM117961,"['Affinity', 'Amino Acid Sequence', 'Antibodies', 'Artificial Intelligence', 'Automation', 'Automobile Driving', 'Base Sequence', 'Binding', 'Biological Assay', 'Biotechnology', 'Collection', 'Communities', 'Coupled', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Databases', 'Deposition', 'Development', 'Diagnostic', 'Disease', 'Drug Screening', 'Engineering', 'Enzymes', 'Generations', 'Healthcare', 'Image', 'Inflammatory', 'Knowledge', 'Link', 'Machine Learning', 'Medicine', 'Metabolic Diseases', 'Meteor', 'Modeling', 'Modification', 'Mutation', 'Natural Language Processing', 'Pharmaceutical Preparations', 'Phase', 'Play', 'Preparation', 'Property', 'Protein Engineering', 'Proteins', 'Protocols documentation', 'Reagent', 'Research Personnel', 'Sampling', 'Scientist', 'Standardization', 'Structural Protein', 'Structure', 'Techniques', 'Technology', 'Therapeutic antibodies', 'Training', 'base', 'cancer therapy', 'computerized tools', 'data submission', 'data warehouse', 'database structure', 'deep learning', 'design', 'flexibility', 'immunogenicity', 'improved', 'insight', 'learning strategy', 'multitask', 'next generation sequencing', 'novel', 'peer', 'prediction algorithm', 'protein structure', 'repository', 'small molecule', 'therapeutic protein', 'tool']",NIGMS,"PROTABIT, LLC",R44,2019,990318,495160,0.01681773266472744
"IGF::OT::IGF  BIOINFORMATICS SUPPORT FOR THE NIEHS IN DIR & DNTP The purpose of this contract is to provide bioinformatic support to researchers in the Divisions of National Toxicology Program (DNTP) and Intramural Research (DIR) at the National Institute of Environmental Health Sciences (NIEHS). NIEHS researchers conduct studies that produce large amounts of data, varying in size and complexity. Fields of scientific study are diverse and include toxicology, genomics, transcriptomics, high throughput screening (HTS) data and data extraction from diverse text resources. The variety and complexity of NIEHS scientific studies dictates the need for innovative analytical techniques and the development of new software tools. Bioinformatic data analyses are required to support accurate and precise interpretation of study results. Specific bioinformatics needs include data analysis, data mining, creating bioinformatics pipelines for gene expression and pathway analysis and computational support for the vast amount of data collected through studies conducted at NIEHS and NIEHS contract laboratories. n/a",IGF::OT::IGF  BIOINFORMATICS SUPPORT FOR THE NIEHS IN DIR & DNTP,9915697,73201700001C,"['Artificial Intelligence', 'Bioinformatics', 'Biological Assay', 'ChIP-seq', 'Chemical Exposure', 'Chemicals', 'Contractor', 'Contracts', 'DNA Methylation', 'DNA Sequence', 'DNA sequencing', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Epigenetic Process', 'Evaluation', 'Exons', 'Gene Expression', 'Genes', 'Genomics', 'Informatics', 'Intramural Research', 'Knowledge', 'Laboratories', 'Literature', 'Measures', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Output', 'Pathway Analysis', 'Peer Review', 'Privatization', 'Programming Languages', 'Proteomics', 'Publications', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Sampling', 'Scientific Evaluation', 'Scientist', 'Series', 'Software Tools', 'Specific qualifier value', 'Technology', 'Text', 'Toxicogenomics', 'Toxicology', 'analysis pipeline', 'bioinformatics tool', 'bisulfite sequencing', 'cheminformatics', 'computational intelligence', 'data integration', 'data mining', 'differential expression', 'high throughput screening', 'innovation', 'meetings', 'metabolomics', 'method development', 'next generation sequencing', 'physical property', 'programs', 'screening', 'technique development', 'transcriptomics', 'whole genome']",NIEHS,"SCIOME, LLC",N01,2019,2464037,2510992,-0.006089627525340591
"Systems-level functional proteomics analysis assemblies in Alzheimer's disease and mouse models of tauopathy The goal of this proposal is to comprehensively map and identify the subnetworks of synaptic protein complexes that are central players in the synaptic dysfunction occurring with neurodegeneration. We will use the emerging power of quantitative network proteomics in the Emili laboratory to systematically characterize the major protein assemblies present at normal and diseased synapses on a proteome scale. This research will be propelled by discoveries from the Wolozin laboratory demonstrating that a dynamic network of protein interactions drives tau biology and changes with the course of disease. Interpreting these perturbed assembly networks, though, demands knowledge of the localization and compositional specificity of such complexes. The unbiased interactome screening technology developed by the Emili laboratory is uniquely suited for unbiased interrogations of synaptic protein networks. We hypothesize that selective disruption of specific synaptic protein assemblies mediates the functional degeneration associated with tauopathy. Aim 1 will determine how synaptic protein complexes differ between general cortical and cholinergic neurons. We will isolate and biochemical separate synaptic assemblies from total cortical and cholinergic (ChAT::GFP) synapses, using FACS to further purify ChAT:GFP synapses. Separated assemblies will be characterized by precision mass spectrometry and integrative data mining (machine learning) procedures to determine their composition and post- translational modification states, and to map dynamically changing interactions implicated in altered synaptic function during normal aging. Aim 2 will determine how the macromolecular structures of synaptic protein assemblies change with aging and AD. We will analyze cortical synaptosomal complexes from P301S tau and P301S tau x TIA1+/- mice, the latter which exhibited delayed degeneration. Key drivers in synaptic dysfunction will be identified and verified in AD and control human samples by co-immunoprecipitation. Aim 3 will identify complexes that are critical drivers of synaptic function by disrupting prioritized assemblies using genetic and opticogenetic tools. This work will determine key regulators of synaptic function in health and disease, and will also produce expanded genetic tools and outstanding targets for future approaches using bio-engineered regulation. TAhlzihs epirmoepro’ssadliwseiallsied, eanntdi fdy etvheelompoilne cnuolveastti vhea tt ocoolnst rt oolcdoengteronlearna tdi opne rohf anpesrvree vceerlslse i n the relentless brain degeneration that causes dementia.",Systems-level functional proteomics analysis assemblies in Alzheimer's disease and mouse models of tauopathy,9655120,RF1AG061706,"['Acute', 'Aftercare', 'Aging', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Architecture', 'Atlases', 'Biochemical', 'Biology', 'Biomedical Engineering', 'Brain', 'Chimeric Proteins', 'Co-Immunoprecipitations', 'Complex', 'Coupled', 'Cytoplasmic Granules', 'Data', 'Dementia', 'Dimerization', 'Disease', 'Disease Progression', 'Ectopic Expression', 'Exhibits', 'Fractionation', 'Functional disorder', 'Future', 'Genetic', 'Glutamates', 'Goals', 'Health', 'Human', 'Impairment', 'Ion-Exchange Chromatography Procedure', 'Isoelectric Focusing', 'Knowledge', 'Laboratories', 'Lead', 'Light', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'Mediating', 'Membrane', 'Messenger RNA', 'Molecular', 'Molecular Structure', 'Mus', 'Nerve Degeneration', 'Neurons', 'Neurophysiology - biologic function', 'Organelles', 'Physostigmine', 'Pilocarpine', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteome', 'Proteomics', 'RNA', 'RNA-Binding Proteins', 'Regional Disease', 'Regulation', 'Research', 'Role', 'Sampling', 'Specificity', 'Synapses', 'Synaptic Transmission', 'Synaptosomes', 'System', 'Tauopathies', 'Technology', 'Testing', 'Variant', 'Work', 'brain tissue', 'cell type', 'cholinergic', 'cholinergic neuron', 'data mining', 'disorder control', 'insight', 'knock-down', 'macromolecule', 'mouse model', 'normal aging', 'overexpression', 'protein complex', 'screening', 'synaptic function', 'tau Proteins', 'tool']",NIA,BOSTON UNIVERSITY MEDICAL CAMPUS,RF1,2019,3945190,164685352,-0.03965890250891926
"Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)? PROJECT SUMMARY The North Coast Conference on Precision Medicine is a national annual mid-sized conference series held in Cleveland, Ohio. The conference series aims to serve as a venue for the continuing education and exchange of scientific ideas related to the rapidly evolving and highly interdisciplinary landscape that is precision medicine research. The topics for each conference coincide with the national conversation and research agenda set by national research programs focused on precision medicine. The 2018 conference is a symposium that will focus on issues related to return of genomic results both in clinical and research settings with an emphasis on diverse populations. The conference will be organized as a traditional format with invited speakers from among national experts for topics ranging from issues returning research results to culturally diverse participants and family members, inclusion of diverse patient and participant populations in the Clinical Sequencing Evidence- Generating Research (CSER) consortium and the Trans-Omics for Precision Medicine (TOPMed) Program, pharmacogenomics-guided dosing and race/ethnicity, strategies used to return results, among others. 2019 and beyond conference topics are being considered from previous symposia attendees and trends in precision medicine research. Odd-numbered year conferences include a workshop component that has previously covered outcome and exposure variable extraction from electronic health records. Future workshop topics being considered include integration of multiple ‘omics, drug response in different populations, pharmacogenomics clinical implementation, precision medicine in cancer, data sharing and informed consent, and the use of apps for recruitment, diagnosis, follow-up, and treatment. Our second major objective of this conference series is the promotion of diversity in the biomedical workforce. It is well-known that the pipeline from training to full professor for women in biomedical research is leaky whereas the pipeline for under-represented minorities is practically non-existent. Drawing from national and local sources, we vet women and under-represented minorities for every invited speaker opportunity, thereby providing valuable career currency and networking opportunities. We will also encourage women and under-represented minorities, particularly at the trainee level, to attend and participate in this conference series to spur interest in pursuing precision medicine research as a career. Overall, the North Coast Conference on Precision Medicine series is a valuable addition to the national conference landscape, and with its unique location and low cost to participants, will serve as an important educational opportunity as precision medicine research accelerates in earnest. PROJECT NARRATIVE The North Coast Conference on Precision Medicine is a yearly fall conference series in Cleveland, Ohio designed as a continuing education forum in the burgeoning area of precision medicine research. The conference brings together national experts on a host of topics ranging from bioethics to bioinformatics to biomedical informatics to speak and lead workshops on timely challenges posed in translating complex genomic and health data into clinical practice. The conference series also serves to promote diversity in the biomedical workforce. This year’s symposium will focus issues related to return of genomic results in both clinical and research settings with an emphasis on diverse populations.",Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)?,9612854,R13HG010286,"['Academic Medical Centers', 'Acceleration', 'African American', 'Area', 'Back', 'Big Data', 'Bioethics', 'Bioinformatics', 'Biomedical Research', 'Clinic', 'Clinical', 'Clinical Research', 'Complex', 'Computational Biology', 'Computer Simulation', 'Continuing Education', 'Custom', 'Data', 'Databases', 'Diagnosis', 'Dose', 'Educational workshop', 'Electronic Health Record', 'Ensure', 'Ethnic Origin', 'Family member', 'Funding', 'Future', 'Generations', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health system', 'Healthcare Systems', 'Hospitals', 'Incidental Findings', 'Informed Consent', 'Institution', 'Knowledge', 'Lead', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mining', 'Names', 'Ohio', 'Outcome', 'PMI cohort', 'Participant', 'Pathogenicity', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Population', 'Population Heterogeneity', 'Prevention', 'Process', 'Race', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Schedule', 'Science', 'Series', 'Source', 'Surveys', 'Technology', 'Time', 'Training', 'Trans-Omics for Precision Medicine', 'Translating', 'Travel', 'Underrepresented Groups', 'Underrepresented Minority', 'United States', 'United States Centers for Medicare and Medicaid Services', 'Variant', 'Veterans', 'Woman', 'base', 'big biomedical data', 'biomedical informatics', 'career', 'clinical care', 'clinical implementation', 'clinical practice', 'clinical sequencing', 'clinically relevant', 'cost', 'cost effective', 'data sharing', 'design', 'falls', 'follow-up', 'forging', 'frontier', 'genome-wide', 'genomic data', 'health data', 'health disparity', 'health information technology', 'incentive program', 'individual patient', 'interest', 'medical specialties', 'multiple omics', 'patient population', 'point of care', 'posters', 'precision medicine', 'programs', 'recruit', 'response', 'science education', 'senior faculty', 'symposium', 'trend']",NHGRI,CASE WESTERN RESERVE UNIVERSITY,R13,2018,10000,197030888,0.003619473303932831
"Computational Design of Crystal Lattice Interactions to Determine Recalcitrant Protein Structures PROJECT SUMMARY Proteins are macromolecules that carry out innumerable biological functions ranging from intracellular signaling to immune response. The specific function of a protein is determined by its three-dimensional structure. Therefore, an essential step towards understanding protein function is elucidating protein structure. However, the premier method for structure determination, X-ray crystallography, can be limited by many potential factors including the quality of the protein crystal. The proposed research will investigate the effects of protein– protein interactions in the crystal lattice on crystal quality and resolution and develop a design approach to stabilize lattice interactions, which should result in higher-quality crystals.  First, the molecular determinants of high-quality protein crystal structures will be identified by (1) curating a set of representative structures from the Protein Data Bank, (2) extracting relevant features, such as interaction energy, buried solvent accessible surface area, interfacial packing quality, and residue usage, and (3) analyzing the features’ relationship to crystal resolution (a proxy for quality). Based on these findings, a design strategy and score function for ranking designs within the Rosetta framework will be developed.  Second, the design strategy will be applied to SNase, a model protein that is easy to purify and well- behaved in crystallization and diffraction experiments. The designed proteins will be crystallized and the crystal structures will be solved, testing for improved resolution. Analysis of the resultant crystal structures will drive development of the design strategy.  Third, the design strategy will be applied on a bacterial gyrase, an antibiotic-target protein for which there are several drug-bound structures at low resolution, lacking sufficient detail to reveal key antibiotic– gyrase interactions. Preliminary data suggests that the designed gyrase mutants will yield high-resolution structures, permitting a better understanding of antibiotic–gyrase interactions, with implications for drug design.  Should the method be successful, it will be immediately applicable to ~26,000 structures in the Protein Data Bank, and countless structures that have not been published due to lack of resolution. Re-engineered, high-resolution structures of these proteins could yield structural data on molecular interactions pertinent to disease, drug development, and basic understanding of protein function. PROJECT NARRATIVE This project will develop methods to improve low-resolution crystal structures of proteins to high-resolution sufficient (1) to resolve key mechanistic features of enzyme function and disease, and (2) to design or screen drug molecules. In addition to method development, we will pursue crystal structure of M. tuberculosis DNA gyrase in complex with relevant antibiotic drugs.",Computational Design of Crystal Lattice Interactions to Determine Recalcitrant Protein Structures,9644449,F31GM123616,"['Address', 'Affect', 'Algorithm Design', 'Anti-Bacterial Agents', 'Antibiotics', 'Area', 'Basic Science', 'Benchmarking', 'Binding', 'Biochemical', 'Biological', 'Biological Process', 'Complex', 'Crystallization', 'Crystallography', 'DNA', 'DNA Gyrase', 'Data', 'Development', 'Discipline', 'Disease', 'Drug Design', 'Drug Screening', 'Engineering', 'Enzymes', 'Free Energy', 'Glean', 'Goals', 'Immune response', 'Ligand Binding', 'Ligands', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Mycobacterium tuberculosis', 'Pharmaceutical Preparations', 'Pharmacology', 'Protein Engineering', 'Proteins', 'Proxy', 'Publishing', 'Research', 'Resolution', 'Side', 'Signal Transduction', 'Solvents', 'Staphylococcus aureus', 'Structure', 'Surface', 'System', 'Testing', 'X-Ray Crystallography', 'base', 'data warehouse', 'design', 'drug development', 'experimental study', 'functional gain', 'improved', 'insight', 'interfacial', 'macromolecule', 'method development', 'mutant', 'novel', 'nuclease', 'protein function', 'protein protein interaction', 'protein structure', 'research and development', 'structural biology', 'three dimensional structure', 'tool']",NIGMS,JOHNS HOPKINS UNIVERSITY,F31,2018,37308,807432003,0.003108174359030108
"Quantifying molecular consequences of human missense variants with large-scale interactome perturbation studies SUMMARY Almost all proteins function through interacting with other proteins. Previous studies have shown that the vast majority of damaging single amino acid mutations in proteins disrupt only a subset of specific protein-protein interactions, and that mutations in the same protein that disrupt different interactions tend to cause clinically distinct disorders. Therefore, it is of great importance to determine interaction-specific disruptions caused by each mutation. Furthermore, rapid advances in sequencing technologies have enabled the identification of tens of millions of single nucleotide variants (SNVs) in the human population, driving an urgent need to understand the impact of each SNV on the human interactome network. Unfortunately, there is currently no method that is capable of predicting the specific impact of a large fraction of these SNVs on individual protein-protein interactions. To address this issue, we propose to leverage our massively-parallel site-directed mutagenesis pipeline, Clone-seq, to generate clones for ~6,000 coding SNVs in the human population: ~4,000 from gnomAD and ~2,000 to be submitted by the international human genetics community. We will then experimentally examine the impact on protein stability and individual protein-protein interactions for every variant using high-throughput DUAL-FLUO and InPOINT (integrating PCA, LUMIER, Y2H, and wNAPPA) assays. This proposal brings together three groups with complementary expertise in high-throughput interactome experiments and network analysis from the Yu lab, in genomic and population genetic studies from the Clark lab, and in comprehensive biophysical and structural modeling of mutation’s impact on binding free energy of protein interactions from the Alexov lab. Out of the ~6,000 SNVs, we expect to identify ~1,200 disruptive SNVs and ~4,000 different SNV-interaction pairs where the SNV disrupt that specific interaction. The data produced by our project will increase the available experimental information by >140× in number of human proteins and >500× in number of interactions, allowing us for the first time to comprehensively assess the relationships between the impact of SNVs on interactions and their various population genetic attributes (including, but not limited to, allele frequency and flanking haplotype, inter-population differentiation, local rate of recombination, allele age, modes of selection). Finally, we will establish a computational-experimental- integrated iterative learning scheme to build a multi-layer random-forest-based framework, SIMPACT, which can accurately predict specific impacts on all individual protein-protein interactions for all missense SNVs. Our proposed work will fuel hypothesis-driven research, will significantly improve our functional understanding of variants, and will likely fundamentally change the experimental design and data interpretation for whole genome/exome studies going forward. NARRATIVE The dramatic increase of DNA variants discovered through advances in sequencing technologies has been inadequately translated into therapeutic successes. Although many of these variants are related to human disorders, the overwhelming number of non-functional variants makes the assessment of functional significance a steep challenge. In this study, we aim to develop a high-throughput pipeline to quickly clone and directly test a large number of coding variants for their impact on the human interactome network and use the results to build a machine learning pipeline to predict functional impact of all coding variants, in anticipation that both our experimental data and computational pipeline will lead to broad clinical and therapeutic applications.",Quantifying molecular consequences of human missense variants with large-scale interactome perturbation studies,9702305,R01GM125639,"['Address', 'Age', 'Alleles', 'Amino Acids', 'Automobile Driving', 'Binding', 'Biochemical', 'Biological Assay', 'Biophysics', 'Cells', 'Clinical', 'Code', 'Communities', 'Coupling', 'Crystallization', 'DNA', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease', 'Experimental Designs', 'Free Energy', 'Gene Frequency', 'Genes', 'Genetic Recombination', 'Genetic study', 'Genome', 'Genomics', 'Haplotypes', 'Homology Modeling', 'Human', 'Human Genetics', 'Individual', 'International', 'Learning', 'Letters', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Molecular', 'Mutation', 'Nucleotides', 'Output', 'Pathway Analysis', 'Population', 'Population Genetics', 'Property', 'Proteins', 'Proteomics', 'Research', 'Resolution', 'Resources', 'Scheme', 'Site-Directed Mutagenesis', 'Structural Models', 'Structure', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Translating', 'Variant', 'Work', 'base', 'biological research', 'biophysical model', 'biophysical techniques', 'exome', 'experimental study', 'forest', 'human interactome', 'improved', 'interest', 'molecular phenotype', 'mutant', 'next generation sequencing', 'protein function', 'protein protein interaction', 'screening', 'success', 'web portal', 'whole genome']",NIGMS,CORNELL UNIVERSITY,R01,2018,99500,91477866,0.0075077354569708975
"Classifying addictions using machine learning analysis of multidimensional data ABSTRACT This Independent Scientist Award will significantly enhance my research capabilities, enabling me to become a leading quantitative investigator in the field of substance use disorders (SUDs). Specifically, it will allow me to increase my knowledge in the areas of SUD phenotypes, treatment and genetics. SUDs are clinically and etiologically heterogeneous and their classification has been difficult. This application reflects my ongoing commitment to developing an innovative and interdisciplinary research program on the classification of SUDs through quantitative analysis of multidimensional data. My extensive training in computational science and prior research on biomedical informatics have provided me with the skills to design, implement and evaluate advanced algorithms and sophisticated analyses to solve challenging problems in classifying SUDs. My ongoing NIDA-funded R01 employs a large (n=~12,000) sample aggregated from multiple genetic studies of cocaine, opioid, and alcohol dependence to develop and evaluate novel statistical models to generate clinical SUD subtypes that are optimized for gene finding. This K02 proposal extends that work to evaluate treatment outcome in refined subgroups of SUD populations using data from treatment studies for cocaine, opioid, alcohol and multiple substance dependence. This project will integrate data from diagnostic behavioral variables and genotypes, as well as biological/neurobiological features of the disorders and repeated measures of treatment outcome. The primary career development goals of this application are to: (1) understand the reliability, validity and functional mechanisms of various phenotyping methods; (2) to continue training in the genetics of addictions; and (3) to gain greater knowledge of different treatment approaches and their efficacy. A solid foundation in these areas will enhance my ability to realize the full potential of the data collected and aggregated from multiple dimensions, and to use the data to design the most clinically useful analysis and generate innovative solutions to diagnostic and predictive challenges in SUD research. Through formal coursework, directed readings, individual tutoring and intensive multidisciplinary collaboration with a diverse team of world-renowned researchers, I will receive training and collect pilot data for future R01 projects by examining (Aim I): whether clinically-defined highly heritable subtypes derived in my current R01 project predict differential treatment response; (Aim II) whether new statistical models that directly combine treatment data with behavioral, biological, and genomic data identify refined subtypes with confirmatory multilevel evidence; and (Aim III) whether there are genetic and social moderators of treatment outcome by subtype. The overall goal of this proposal is to further my independent and multidisciplinary research program in the development of statistical methods for refined classification of SUDs. The K02 award will provide me with the protected time necessary to fully engage in the training activities described that will enhance my knowledge and skills to enable me to make important, novel contributions to the genetics and treatment of SUD. PROJECT NARRATIVE This project will develop novel statistical and quantitative tools to identify homogeneous subtypes of substance use disorders (SUDs) and other complex diseases to enhance gene finding and treatment matching. The proposed project will perform secondary analyses of existing data from treatment studies of cocaine, opioid, alcohol, and mixed SUDs. The proposed novel approaches are expected to advance precision medicine approaches to SUDs by enabling treatment matching and a more refined SUD classification to gene finding.",Classifying addictions using machine learning analysis of multidimensional data,9427988,K02DA043063,"['Adherence', 'Aftercare', 'Alcohol dependence', 'Alcohols', 'Algorithms', 'Area', 'Behavioral', 'Biological', 'Biological Markers', 'Biosensor', 'Characteristics', 'Classification', 'Clinical', 'Cluster Analysis', 'Cocaine', 'Cocaine Dependence', 'Collaborations', 'Combined Modality Therapy', 'Complex', 'Computational Science', 'DSM-IV', 'DSM-V', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic and Statistical Manual of Mental Disorders', 'Dimensions', 'Disease', 'Drug Use Disorder', 'Electroencephalography', 'Etiology', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Heritability', 'Heterogeneity', 'Independent Scientist Award', 'Individual', 'Interdisciplinary Study', 'Investigation', 'Joints', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Neurobiology', 'Opiate Addiction', 'Opioid', 'Patients', 'Pattern', 'Pharmacogenetics', 'Pharmacotherapy', 'Phenotype', 'Population', 'Reading', 'Recording of previous events', 'Research', 'Research Personnel', 'Risk Factors', 'Sampling', 'Scientist', 'Signs and Symptoms', 'Solid', 'Statistical Methods', 'Statistical Models', 'Subgroup', 'Substance Addiction', 'Substance Use Disorder', 'Surveys', 'Symptoms', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Work', 'addiction', 'alcohol use disorder', 'biomarker performance', 'biomedical informatics', 'career development', 'cocaine use', 'contingency management', 'design', 'disease classification', 'disorder subtype', 'endophenotype', 'genetic association', 'genomic data', 'imaging genetics', 'improved', 'innovation', 'neural correlate', 'novel', 'novel strategies', 'opioid use disorder', 'outcome prediction', 'personalized medicine', 'precision medicine', 'programs', 'recruit', 'secondary analysis', 'skills', 'social', 'tool', 'treatment planning', 'treatment response', 'tutoring']",NIDA,UNIVERSITY OF CONNECTICUT STORRS,K02,2018,162800,36067938,-0.027878295650114515
"An integrated neural network analysis and video microscopy platform for fully automated particle tracking Project Summary/Abstract  Particle tracking (PT) is a biophysical tool for elucidating molecular interactions, transport phenomena of diverse species, and rheological properties of complex materials. PT experiments involve first obtaining high resolution videos that capture time-resolved increments of particles, followed by extraction of traces of entities of interest from videos in the form of spatial locations over time, a process we refer to as path conversion. Finally, quantitative analysis of the traces will yield diffusivities, viscoelasticity, etc.  Lung diseases, such as cystic fibrosis and COPD, are characterized by a highly viscoelastic mucus layer that is incapable of being cleared by mucociliary clearance. Not surprisingly, the viscoelasticity of mucus often directly reflects disease progression. A variety of mucolytics are being investigated, but due to the variable composition and properties of mucus between patients, effective mucolytics treatment will likely be different between individuals; too little/inappropriate mucolytics will not be effective in restoring mucus clearance, whereas too much may result in bronchorrhea. Although microbeads-based rheology has been performed on a variety of mucus specimens in basic research, the capacity for high throughput characterization of rheological properties of biological specimens in a clinical setting is currently not available. This limitation can be attributed to inefficiencies of path conversion: current PT software requires extensive human supervision/intervention to achieve accurate path conversion, not only resulting in poor reproducibility and throughput but also restricting its use to only expert labs. Our vision is to make PT as objective and easy to use as a simple plate reader that can be readily utilized by clinicians (diagnostics, disease progression, therapy effectiveness), pharma (preclinical/clinical drug screening), and research professionals. Towards this goal, we have created a neural network tracker (NNT) that automatically determines the location of all particles in each frame with zero user-input (i.e. no parameter for users to change), and retains the identity of all particles from frame to frame. The innovation is that NNT can robustly, reproducibly, and accurately track a wide range of 2D/3D videos with virtually no need for human intervention, achieving unparalleled time savings. We have already successfully deployed NNT over the Google cloud, which offers exceptional scalability. Nevertheless, for time-sensitive applications, such as an automated PT rheometer, the transfer of large video data files is likely prohibitive. Therefore, in this Phase I STTR, we seek to enable real-time NNT-based PT analysis on the local machine while video microscopy data is being acquired by the microscope, and allow data from PT analysis to drive the operation of the microscope. In Aim 1, we will integrate our NNT with a single objective fluorescence microscope system called Monoptes. Aim 2 will evaluate the performance of our NNT- Monoptes system. If successful, our technology would form the basis of a fully automated PT system capable of measuring rheological properties of fluids/materials or distribution of particle sizes in a 96-well plate format. Project Narrative Particle tracking is a powerful biophysical tool in life and physical sciences, but unfortunately, its application has been strongly limited by inefficiencies in accurately extracting particle traces from raw movies. Unlike conventional particle tracking methods, we have combined artificial intelligence and machine learning to create a software that can consistently provide superior and truly automated tracking performance compared to current alternatives. In this proposal, we will integrate this latest advance with sophisticated instrumentation to develop a microscope system capable of fully automated particle tracking microscopy in a 96-well plate format. If successful, the instrument will likely be utilized by clinicians (diagnostics, disease progression, therapy effectiveness), pharma (preclinical/clinical drug screening of patients), and research professionals.",An integrated neural network analysis and video microscopy platform for fully automated particle tracking,9620574,R41GM130202,"['Acceleration', 'Adopted', 'Antibodies', 'Artificial Intelligence', 'Basic Science', 'Binding', 'Biological', 'Biological Neural Networks', 'Biological Sciences', 'Chronic Obstructive Airway Disease', 'Clinical', 'Clinical Trials', 'Code', 'Complex', 'Computer software', 'Cystic Fibrosis', 'Data', 'Data Files', 'Decision Making', 'Diagnosis', 'Diagnostic', 'Diffuse', 'Disease Progression', 'Drug Carriers', 'Drug Screening', 'Effectiveness', 'Elasticity', 'Engineering', 'Gaussian model', 'Goals', 'HIV Infections', 'Heterogeneity', 'Human', 'Image', 'Individual', 'Intervention', 'Liquid substance', 'Location', 'Lung diseases', 'Machine Learning', 'Manuals', 'Measurement', 'Measures', 'Methods', 'Microscope', 'Microscopy', 'Microspheres', 'Motion', 'Mucociliary Clearance', 'Mucolytics', 'Mucous body substance', 'Output', 'Particle Size', 'Particulate', 'Pathway Analysis', 'Patients', 'Performance', 'Phase', 'Photobleaching', 'Positioning Attribute', 'Process', 'Property', 'Radial', 'Reader', 'Reproducibility', 'Research', 'Resolution', 'Respiratory physiology', 'Rheology', 'Risk', 'Running', 'Sampling', 'Savings', 'Series', 'Small Business Technology Transfer Research', 'Software Tools', 'Specimen', 'Spottings', 'Supervision', 'System', 'Technology', 'TensorFlow', 'Time', 'Video Microscopy', 'Viscosity', 'Vision', 'Woman', 'base', 'biophysical tools', 'cloud based', 'drug development', 'experimental study', 'fluorescence microscope', 'innovation', 'instrument', 'instrumentation', 'interest', 'movie', 'novel', 'operation', 'particle', 'patient screening', 'physical science', 'pre-clinical', 'submicron', 'virtual', 'viscoelasticity']",NIGMS,"AI TRACKING SOLUTIONS, LLC",R41,2018,224894,0,-0.004557961804306035
"Assay Classifier Engine (ACE) for enhancing splice sensor assay performance SUMMARY:  The goal of this proposal is to improve the sensitivity and specificity of the Spinach-based splice sensor platform by developing a novel multiprobe (MP) assay design and a companion machine learning-based classification algorithm called assay classifier engine (ACE). Improvement in sensitivity and specificity of the splice sensor platform enables its application to detect endogenous RNA isoforms with low copy number and distinguish alternative RNA isoforms that share high degree of sequence similarities.  The aim of any assay development effort is to achieve excellent assay specificity and sensitivity. However, this is often a futile endeavor since specificity and sensitivity are two inversely correlated factors. The underlying reason for poor sensitivity or specificity is due to the off-target signals generated by competing molecules present in the sample. In the field of diagnostics, one of the ways these issues are addressed is to perform multiple single probe testing instead of one single probe testing. While individual singe probe assays might have poor specificity and sensitivity, when combined, these assays synergistically improve the sensitivity and specificity of the ultimate diagnostic determination. In the field of research and drug discovery, researchers have employed a multitude of strategies (e.g. signal amplification, reaction cascades, or sample enrichment) to improve sensitivity and MP design or strand displacement strategies to improve specificity. Some of the PCR- based methods have combined both enzyme-based signal amplification and MP strategies to improve assay determination. However, when it comes to detecting targets that are highly similar to their competitors, such as detecting single nucleotide polymorphism, DNA methylation, RNA modification and alternative splicing, there is still an unmet need for more sensitive and specific analytical methods.  In the past few years, Lucerna has developed Spinach-based sensors to detect intractable metabolites and biomolecules. One such sensor is the splice sensor, which is a Spinach-based sensor that can generate fluorescence signal based on the alternative RNA isoform of interest. One of the challenges encountered during splice sensor assay development is the lack of sensitivity toward low copy number RNA isoforms and low specificity when distinguishing two splice isoforms that share a high sequence similarity. To overcome this challenge in this proposal, we will develop a MP assay panel comprised of splice sensor variants that recognize the target RNA and the competitor with varying binding affinities and differing signal responses. We will use data sets generated from the MP assay to train a ML-based ACE algorithm to make target determination in test samples. Further, we will develop a quantitative MP data set and re-train the ACE algorithm to classify the assay signals into various categories based on target concentrations in the test sample. This new ACE algorithm will then be tested against conventional single probe assays to determine specificity and sensitivity improvement of the MP assay platform. PROJECT NARRATIVE: Improved specificity and sensitivity are highly sought-after features in assays where there are high similarity between the target and its competitors or when the target exists naturally in very low abundance. To address this unmet need, we will develop a fluorescence sensor-based multiprobe assay approach and a companion machine learning-based assay classifier engine (ACE). The ACE algorithm will integrate the multiprobe assay data and classify them based on trained machine learning models to make sample determination with enhanced specificity, sensitivity, and dynamic range than possible with conventional single probe assays.",Assay Classifier Engine (ACE) for enhancing splice sensor assay performance,9622514,R43GM130258,"['Address', 'Adopted', 'Affinity', 'Algorithms', 'Alternative Splicing', 'Area Under Curve', 'Binding', 'Binding Sites', 'Biochemical', 'Biological Assay', 'Categories', 'Cells', 'Characteristics', 'Classification', 'Companions', 'Custom', 'DNA Methylation', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Enzyme-Linked Immunosorbent Assay', 'Enzymes', 'Evaluation', 'Exhibits', 'Fluorescence', 'Goals', 'Individual', 'Learning', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Modification', 'Molecular', 'Nerve Degeneration', 'Nucleotides', 'Output', 'Pattern', 'Performance', 'Process', 'Protein Isoforms', 'RNA', 'RNA Splicing', 'Reaction', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Series', 'Side', 'Signal Transduction', 'Single Nucleotide Polymorphism', 'Specificity', 'Spinach - dietary', 'Technology', 'Testing', 'Time', 'Titrations', 'Training', 'Variant', 'analytical method', 'aptamer', 'assay development', 'base', 'cost', 'design', 'drug discovery', 'experience', 'improved', 'interest', 'novel', 'outcome forecast', 'predictive modeling', 'response', 'sensor', 'targeted biomarker']",NIGMS,"LUCERNA, INC.",R43,2018,224925,981315,-0.014730191449194071
"Towards automated phenotyping in epilepsy Over 5 million children and adults in the United States have had a diagnosis of epilepsy or a seizure disorder. However, treatment options for the epilepsies remain inadequate, because many patients suffer from uncontrolled seizures and from the negative side effects of treatment. A major obstacle to the faster development of new anti-convulsant therapies is the fact that rigorous preclinical epilepsy research typically requires labor-intensive and expensive 24/7 video-EEG monitoring of seizures that rests on the subjective scoring of seizure phenotypes by human observers (as exemplified by the widely used Racine scale of behavioral seizures). We propose to test if it is possible to perform objective, inexpensive and automated phenotyping of mice in various mouse models of acquired and genetic epilepsies. The approach rests on the recent recognition that mouse behaviors are structured in stereotyped modules at sub-second timescales that are arranged according to specific rules. These characteristic behavioral modules, and the transitions between them, can be identified without observer bias by combined 3D imaging and machine learning (ML) -assisted analytic methods. We propose to adopt this novel ML-assisted 3D video analysis technology to epilepsy research, in order to test if it can be used to identify mice with chronic temporal lobe epilepsy (TLE) during inter-ictal and ictal periods in two distinct experimental TLE models, and under various experimental conditions. In addition, we will also test whether the approach is able to automatically detect not only the overtly epileptic mice in a genetic model of severe childhood epilepsy (homozygous voltage-gated sodium channel β-subunit SCN1B-/- knock-out mice), but also distinguish the seemingly normal, non-epileptic, SCN1B+/- heterozygous mice from the wild-type controls. We anticipate that these results will have a potentially transformative effect on the field by demonstrating the feasibility and power of automated, objective, user-independent, inexpensive analysis of acquired and genetic epilepsy phenotypes. There is an urgent need for new therapies for patients with uncontrolled epilepsy. The project will test if it is possible to objectively characterize epileptic phenotypes in mice using a breakthrough technology involving machine learning-assisted analysis of 3-dimensional video data of behavior. If successful, this innovative approach is expected to dramatically accelerate epilepsy research by enabling the objective, automated, inexpensive phenotyping of experimental animals to aid the testing of novel anticonvulsant therapies.",Towards automated phenotyping in epilepsy,9503816,R21NS102908,"['Adopted', 'Adult', 'Adverse effects', 'Animal Behavior', 'Animal Model', 'Animals', 'Anticonvulsants', 'Behavior', 'Behavioral', 'Characteristics', 'Child', 'Childhood', 'Chronic', 'Complex', 'Data', 'Development', 'Diagnosis', 'Electroencephalography', 'Epilepsy', 'Exhibits', 'Frequencies', 'Genetic', 'Genetic Models', 'Hippocampus (Brain)', 'Human', 'Human immunodeficiency virus test', 'Image', 'Knockout Mice', 'Machine Learning', 'Modeling', 'Monitor', 'Mus', 'Neurons', 'Observer Variation', 'Patients', 'Phenotype', 'Pilocarpine', 'Probability', 'Recurrence', 'Research', 'Rest', 'Seizures', 'Sodium Channel', 'Stereotyping', 'Structure', 'Technology', 'Temporal Lobe Epilepsy', 'Testing', 'Three-Dimensional Imaging', 'Three-dimensional analysis', 'Time', 'Translational Research', 'United States', 'Wild Type Mouse', 'analytical method', 'base', 'cost', 'dravet syndrome', 'evidence base', 'high throughput analysis', 'innovation', 'kainate', 'learning strategy', 'mouse model', 'novel', 'novel therapeutics', 'pre-clinical', 'voltage']",NINDS,STANFORD UNIVERSITY,R21,2018,237423,560644462,-0.0075140278497617214
"QuBBD: Geometric Time-Frequency Methods for Multi-modal Physiological Monitoring Modern health monitoring devices at hospitals and wearable sensors in households generate a large amount of time series data at high rate, capturing the physiological status of patients in a real-lime fashion. The premise is that these technology advances enable a data-driven healthcare system that starts making fast, accurate, objective and inexpensive decisions based upon data, in addition to an individual physician's experience and preference. However, there is a significant gap in the mathematical theory and computational tools to promptly extract actionable information from multi-modal non-stationary time series data in a robust and tractable manner, which has become a serious roadblock to further utilize bigger data for better healthcare monitoring. The goal of this research program is to develop a mathematical framework for extracting time-frequency and geometric representations of multi-modal physiological data, in an online and robust manner, and use them to design machine learning algorithms to improve real-lime health monitoring. Specifically, we hypothesize that the development of time-series and geometric methods for large streaming multi-modal monitoring data will lead to more accurate diagnosis on various physiological monitoring applications, including detection and prediction of rare events such as seizure and arrhythmia, classification of sleep stages for newborns and children, and real-time artifact removal of physiological data. To achieve our goal, we plan to develop novel theoretical and computational tools for analyzing non-stationary multi-modal time series data with noise, corruption and missing data as well as real-time algorithms for filtering and event detection from such data. The tools and algorithms will be applied on clinical tasks at the Nationwide Children's Hospital. In addition, the real-time workflow will be implemented on Hadoop clusters with a mission of public sharing of both data and software. The development from the interdisciplinary team composed of mathematicians, biomedical informaticians as well as the hospital will not only transform the frontiers of mathematics knowledge, but also significantly impact clinical applications, data science education, and the development of the $11 O billion emerging market of wireless health. The goal of this project is to develop a series of novel computational theory and software to extract physiological information from the large multi-modal data streams generated by modern health monitoring devices. The tools will be applied to various clinical tasks such as detection and prediction of seizure and arrhythmia and classification of sleep stages for newborns and children, aiming for more accurate diagnosis.",QuBBD: Geometric Time-Frequency Methods for Multi-modal Physiological Monitoring,9568758,R01EB025018,"['Address', 'Algorithms', 'Arrhythmia', 'Behavior', 'Big Data', 'Breathing', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Collaborations', 'Complex', 'Computer software', 'Data', 'Data Science', 'Detection', 'Development', 'Diagnostic', 'Education', 'Environment', 'Event', 'Excision', 'Frequencies', 'Goals', 'Health', 'Healthcare', 'Healthcare Systems', 'Hospitals', 'Household', 'Human body', 'Individual', 'Infant', 'Knowledge', 'Limes', 'Machine Learning', 'Mathematics', 'Measures', 'Methods', 'Mission', 'Modality', 'Modernization', 'Monitor', 'Morphologic artifacts', 'Nature', 'Newborn Infant', 'Noise', 'Outcome', 'Patients', 'Pattern', 'Pediatric Hospitals', 'Physicians', 'Physiologic Monitoring', 'Physiological', 'Property', 'Public Domains', 'Research', 'Resources', 'Seizures', 'Series', 'Sleep Stages', 'Stream', 'Techniques', 'Technology', 'Time', 'Universities', 'Validation', 'Wireless Technology', 'accurate diagnosis', 'base', 'biological systems', 'clinical application', 'clinical practice', 'computerized tools', 'design', 'diagnostic biomarker', 'experience', 'frontier', 'geometric methodologies', 'graduate student', 'heart rate variability', 'improved', 'insight', 'mathematical theory', 'monitoring device', 'multimodality', 'novel', 'preference', 'programs', 'science education', 'signal processing', 'student training', 'theories', 'tool', 'wearable device']",NIBIB,CARNEGIE-MELLON UNIVERSITY,R01,2018,258070,30434536,3.076726386959261e-05
"Novel Nanopore-based RNA Sequencing using Nucleobase-specific Tags Project Summary Cost-effective, and accurate sequencing of RNA, composed of both canonical and modified bases, of any length, without conversion to cDNA, and without amplification are the objectives of this project, and the ultimate goal is to sequence the transcriptome, and determine in a time-sensitive manner relative distribution of its components. Such accomplishment will directly impact prevention, diagnosis, and cure of disease and materialize the promise of personalized medicine. Current methods, such as Illumina's RNA-Seq, and the single molecule approaches of Pacific Biosciences and of Oxford Nanopore Technologies, still lag behind in many of the critical attributes mentioned above. The unresolved issue with nanopore-based sequencing is the observation that ion current vs. time recording does not refer to a single nucleobase, but to a short sequence of 4 or more bases. The problem, partially resolved with the use of sophisticated algorithms and learning machines, appears intractable for RNA that includes numerous post-transcriptional base modifications. As an illustration, if a nanopore reads a sequence of 4 bases and the specific RNA to be sequenced has a total of 8 different nucleobases (4 canonical and 4 modified), then 48 = 65,536 signals need to be discriminated from within an ion current range of 20 to 40 pA with a standard deviation of ±1 pA; this is an impossible computational task. However, if the nanopore could sense one base at a time and yield distinct ion current for each base, there will be only 8 different recordings to distinguish from, a much simpler task. Our own published results indicate that oligodeoxynucleotides conjugated with a pyrimidine-specific tag (Osmium tetroxide 2,2'-bipyrimidine or OsBp) yield enzyme-free, slow/readable translocation via α- Hemolysin, and distinct ion current levels for intact, T(OsBp), and C(OsBp) bases, suggesting that a single tag can yield sequencing information on purine, T, and C. The latter leads to the conjecture that the presence of a second, purine-specific, label would allow identification of all four canonical bases. Furthermore each tag has intrinsic selectivity for one base over another, and this will provide a handle for additional discrimination among the modified bases. In this phase I proposal we aim to demonstrate (i) near 100% labeling (true positives) with 0% internucleotide bond cleavage, and 0% false positives for RNA(OsBp), as we have already shown for DNA(OsBp), (ii) comparable labeling attributes for a purine-specific tag, and (iii) readable translocation with single pyrimidine base discrimination for RNA(OsBp). Success in these efforts will lead to single base discrimination and sequencing of RNA, including a number of post-transcriptionally modified bases, and pave the road for sequencing the transcriptome. ! PUBLIC HEALTH RELEVANCE: Advances in personalized medicine for diagnosis and treatment of disease require sequencing the RNA transcriptome with technologies that are currently unavailable. Nanopore-systems that exhibit single-base discrimination, like the one addressed in this proposal, will allow sequencing the transcriptome in an accurate, timely, and cost-effective manner.",Novel Nanopore-based RNA Sequencing using Nucleobase-specific Tags,9506880,R43HG010051,"['Address', 'Algorithmic Software', 'Algorithms', 'Base Sequence', 'Belief', 'Biological Assay', 'Biological Sciences', 'Cells', 'Complementary DNA', 'DNA', 'Development', 'Diagnosis', 'Digit structure', 'Discrimination', 'Disease', 'Enzymes', 'Exhibits', 'Genetic Transcription', 'Goals', 'Hemolysin', 'In Vitro', 'Individual', 'Investigation', 'Ions', 'Label', 'Length', 'Machine Learning', 'Measures', 'Methods', 'Modification', 'Monitor', 'Nucleic Acids', 'Nucleotides', 'Oligonucleotides', 'Osmium Tetroxide', 'Phase', 'Platinum', 'Platinum Compounds', 'Prevention', 'Protocols documentation', 'Publishing', 'Purines', 'Pyrimidine', 'Pyrimidines', 'RNA', 'RNA Sequences', 'Readability', 'Residual state', 'Signal Transduction', 'Site', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Transfer RNA', 'VDAC1 gene', 'analytical tool', 'base', 'constriction', 'cost effective', 'design', 'improved', 'nanopore', 'new technology', 'novel', 'nucleobase', 'personalized medicine', 'public health relevance', 'sensor', 'single molecule', 'success', 'transcriptome', 'transcriptome sequencing']",NHGRI,"YENOS ANALYTICAL, LLC",R43,2018,280000,276817,-0.013283834907540985
"Statistical Methods in Trans-Omics Chronic Disease Research Project Summary The broad, long-term objectives of this research are the development of novel and high-impact statistical methods for medical studies of chronic diseases, with a focus on trans-omics precision medicine research. The speciﬁc aims of this competing renewal application include: (1) derivation of efﬁcient and robust statistics for integrative association analysis of multiple omics platforms (DNA sequences, RNA expressions, methylation proﬁles, protein expressions, metabolomics proﬁles, etc.) with arbitrary patterns of missing data and with detection limits for quantitative measurements; (2) exploration of statistical learning approaches for handling multiple types of high- dimensional omics variables with structural associations and with substantial missing data; and (3) construction of a multivariate regression model of the effects of somatic mutations on gene expressions in cancer tumors for discovery of subject-speciﬁc driver mutations, leveraging gene interaction network information and accounting for inter-tumor heterogeneity in mutational effects. All these aims have been motivated by the investigators' applied research experience in trans-omics studies of cancer and cardiovascular diseases. The proposed solutions are based on likelihood and other sound statistical principles. The theoretical properties of the new statistical methods will be rigorously investigated through innovative use of advanced mathematical arguments. Computationally efﬁcient and numerically stable algorithms will be developed to implement the inference procedures. The new methods will be evaluated extensively with simulation studies that mimic real data and applied to several ongoing trans-omics precision medicine projects, most of which are carried out at the University of North Carolina at Chapel Hill. Their scientiﬁc merit and computational feasibility are demonstrated by preliminary simulation results and real examples. Efﬁcient, reliable, and user-friendly open-source software with detailed documentation will be produced and disseminated to the broad scientiﬁc community. The proposed work will advance the ﬁeld of statistical genomics and facilitate trans-omics precision medicine studies of chronic diseases. Project Narrative The proposed research intends to develop novel and high-impact statistical methods for integrative analysis of trans-omics data from ongoing precision medicine studies of chronic diseases. The goal is to facilitate the creation of a new era of medicine in which each patient receives individualized care that matches their genetic code.",Statistical Methods in Trans-Omics Chronic Disease Research,9445086,R01HG009974,"['Accounting', 'Address', 'Algorithms', 'Applied Research', 'Biological', 'Cardiovascular Diseases', 'Characteristics', 'Chronic Disease', 'Communities', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Set', 'Derivation procedure', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Documentation', 'Equation', 'Formulation', 'Gene Expression', 'Genes', 'Genetic Code', 'Genetic Transcription', 'Genomics', 'Goals', 'Grant', 'Information Networks', 'Institution', 'Inter-tumoral heterogeneity', 'Joints', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Medicine', 'Mental disorders', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular', 'Molecular Abnormality', 'Molecular Profiling', 'Mutation', 'Mutation Analysis', 'National Human Genome Research Institute', 'North Carolina', 'Patients', 'Pattern', 'Precision Medicine Initiative', 'Prevention', 'Procedures', 'Process', 'Property', 'Public Health', 'Research', 'Research Personnel', 'Resources', 'Somatic Mutation', 'Statistical Methods', 'Symptoms', 'System', 'Tail', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'actionable mutation', 'base', 'disease phenotype', 'experience', 'gene interaction', 'genome sequencing', 'high dimensionality', 'innovation', 'learning strategy', 'metabolomics', 'multiple omics', 'novel', 'open source', 'outcome prediction', 'personalized care', 'precision medicine', 'programs', 'protein expression', 'research and development', 'semiparametric', 'simulation', 'sound', 'statistics', 'theories', 'tool', 'tumor', 'tumor heterogeneity', 'user-friendly']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2018,305167,511185245,0.002075909372766252
"Integrated Instrument for non-natural aptamer generation Project Summary DNA and RNA aptamers are a useful class of synthetic affinity reagents. However, their performance can be greatly improved through the site-specific incorporation of chemically modified, ‘non-natural’ nucleotides that provide a greater chemical repertoire to enable superior aptamer affinity and specificity. Because a broad spectrum of chemical functional groups can be incorporated, non-natural aptamers offer the exciting potential for targeting molecules for which the generation of monoclonal antibodies remains difficult, such as small- molecule drugs, metabolites and carbohydrates. Unfortunately, the access to non-natural aptamers is severely limited. This is because the process of generating non-natural aptamers is technically challenging and limited to a few specialized laboratories. The goal of this project is to develop an integrated instrument, the Non-Natural Aptamer Array (N2A2) that eliminates these bottlenecks and enable rapid and facile non-natural aptamer discovery at virtually any research laboratory. The N2A2 will be built on a modified version of a benchtop commercial sequencer (Illumina MiSeq), and will perform every stage of non-natural aptamer discovery— including sequencing, screening and binding measurements—as part of a single work-flow. There are three main innovative aspects of our N2A2 system. First, our approach will entirely eliminate the need for polymerase engineering, and thus allows us to incorporate virtually any chemical functional group through click chemistry. Second, N2A2 will enable us to directly obtain the binding affinity (Kd) of ~10^7 aptamers directly in complex samples (e.g. cell lysate or serum), thereby resulting in aptamers with high-specificity. Finally, we will develop a machine-learning (ML) approach to identify key motifs (“k-mers”) and predict novel sequences with potentially higher affinity and specificity that can be tested using the N2A2 instrument. We believe this powerful combination of massively parallel, sequence-linked binding measurements with ML-based predictions will allow us to explore sequence space that is currently inaccessible to traditional in vitro selection methods, and enable us to discover aptamers with superior performance. The success of this project will produce an integrated instrument that greatly streamlines and accelerates the discovery of non-natural aptamers for a wide range of targets in complex media. The instrument is based on a commercially available sequencer and we will make all software available to the public. In this way, we believe the N2A2 instrument could broadly expand access to robust, high quality, custom affinity reagents for biomedical research and clinical diagnostics. Project Narrative We will develop an integrated instrument that simplifies the discovery of non-natural aptamer reagents for a wide range of molecules that are difficult to target using conventional antibody reagents. The access to these custom reagents will accelerate biomedical research and clinical diagnostics.",Integrated Instrument for non-natural aptamer generation,9579149,R01GM129313,"['Affinity', 'Algorithms', 'Antibodies', 'Binding', 'Biomedical Research', 'Carbohydrates', 'Cells', 'Chemicals', 'Chemistry', 'Chinese Hamster Ovary Cell', 'Complex', 'Computer software', 'Custom', 'DNA', 'Data', 'Data Analyses', 'Directed Molecular Evolution', 'Engineering', 'Generations', 'Goals', 'Graph', 'In Vitro', 'Label', 'Laboratories', 'Laboratory Research', 'Link', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Monoclonal Antibodies', 'Nucleotides', 'Opioid', 'Performance', 'Pharmaceutical Preparations', 'Polymerase', 'Process', 'Proteins', 'RNA', 'Reagent', 'Reproducibility', 'SLEB2 gene', 'Sampling', 'Serum', 'Site', 'Specificity', 'System', 'Testing', 'Tyrosine', 'Work', 'aptamer', 'base', 'clinical diagnostics', 'functional group', 'improved', 'innovation', 'instrument', 'novel', 'scaffold', 'screening', 'small molecule', 'success', 'virtual']",NIGMS,STANFORD UNIVERSITY,R01,2018,314000,560644462,0.0016641053416738133
"Synergistic integration of topology and machine learning for the predictions of protein-ligand binding affinities and mutation impacts Project Summary Fundamental challenges that hinder the current understanding of biomolecular systems are their tremendous complexity, high dimensionality and excessively large data sets associated with their geometric modeling and simulations. These challenges call for innovative strategies for handling massive biomolecular datasets. Topology, in contrast to geometry, provides a unique tool for dimensionality reduction and data simplification. However, traditional topology typically incurs with excessive reduction in geometric information. Persistent homology is a new branch of topology that is able to bridge traditional topology and geometry, but suffers from neglecting biological information. Built upon PI’s recent work in the topological data analysis of biomolecules, this project will explore how to integrate topological data analysis and machine learning to significantly improve the current state-of-the-art predictions of protein-ligand binding and mutation impact established in the PI’s preliminary studies. These improvements will be achieved through developing physics-embedded topological methodologies and advanced deep learning architectures for tackling heterogeneous biomolecular data sets arising from a variety of physical and biological considerations. Finally, the PI will establish robust databases and online servers for the proposed predictions. Project Narrative The project concerns the integration of topological data analysis and machine learning architectures for the predictions of protein-ligand binding affinities and mutation induced protein stability changes from massive data sets. This new data approach has considerable impact for future generation methods in computational biophysics and drug design.",Synergistic integration of topology and machine learning for the predictions of protein-ligand binding affinities and mutation impacts,9591863,R01GM126189,"['Address', 'Affinity', 'Algorithms', 'Architecture', 'Big Data', 'Binding', 'Binding Proteins', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Biophysics', 'Characteristics', 'Chemicals', 'Classification', 'Complex', 'Computer software', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Dimensions', 'Drug Design', 'Electrostatics', 'Elements', 'Free Energy', 'Freedom', 'Future Generations', 'Geometry', 'Handwriting', 'Image Analysis', 'Induced Mutation', 'Ions', 'Learning', 'Ligand Binding', 'Ligands', 'Lipids', 'Machine Learning', 'Medical', 'Membrane', 'Membrane Proteins', 'Metals', 'Methodology', 'Methods', 'Mutation', 'Physics', 'Plant Roots', 'Proteins', 'Psychological Transfer', 'Site', 'Speech', 'System', 'Techniques', 'Thermodynamics', 'Work', 'algebraic topology', 'base', 'cofactor', 'data warehouse', 'deep learning', 'direct application', 'high dimensionality', 'improved', 'innovation', 'language processing', 'learning strategy', 'metallicity', 'models and simulation', 'multitask', 'mutant', 'neglect', 'next generation', 'search engine', 'tool', 'trend', 'user-friendly']",NIGMS,MICHIGAN STATE UNIVERSITY,R01,2018,319737,89938253,0.015889504472531187
"Molecular impact of mutations in monogenic disease and cancer ABSTRACT  Next generation genome scale sequencing of patients is now becoming routine for two classes of disease: rare  Mendelian traits and cancer. In favorable cases, these data allow identification of relevant mutations and thus  aid diagnosis and therapy. In both classes of disease, the most common type of mutation is missense -­ single  base  changes  that  result  in  an  amino  acid  substitution  in  a  protein.  Uncertainty  as  to  the  impact  of  these  mutations on in vivo protein activity has resulted in a very conservative approach to their interpretation in the  clinic,  so  causing  many  missed  opportunities  for  targeted  treatment.  The  goal  of  this  project  is  to  use  a  combination of three strategies to make the interpretation of these mutations much more applicable in the clinic.  There are already a large number of computational methods that attempt to determine the impact of missense  mutations on function, and there is substantial evidence that these have useful accuracy. The primary difficulty  is that the accuracy in any particular case is not reliably calibrated. Therefore, our first aim is to use a combination  of these methods to develop an approach focused on more reliable estimates for the probability of high impact  on  protein  function  (i.e.  more  confident  P  values).  The  second  aim  is  to  maximize  the  utilization  of  three-­ dimensional structural information, largely ignored by most computational methods. A large fraction of missense  mutations in these classes of disease act by destabilizing protein structure and knowledge of structure allows  these to be identified with much higher reliability. Also, structure provides a framework for detailed annotation  and comprehension of function. To facilitate the utilization of structure, we will implement a modeling platform  that leverages available experimental information to maximize the structural data available for analyzing mutation  impact.  An  important  aspect  of  the  platform  is  incorporation  of  methods  for  evaluating  the  reliability  of  the  structural features relevant to analysis of each mutation. In the third aim we will build specific functional models  for each protein of interest, integrating information from current databases, the literature, and community input,  so as to provide the richest possible background against which to judge the impact of mutations. Proteopedia, a  well established media wiki for proteins, will be used to provide an integrated view of text, data, and structure. A  key component of the information resource will be contributions from curators, who will provide annotation and  also solicit input from other experts. This aspect of the project builds on experience with other crowdsourcing  endeavors,  including  CASP,  CAGI  and  Proteopedia.    There  will  be  three  primary  outcomes  from  the  project:  First, improved reliability for the interpretation of missense mutations. Second, a prototype mutation annotation  procedure suitable for use in a clinical setting. Third, the resource will provide information of benefit to a range  of other scientists, thus facilitating the analysis of disease related mutations.      NARRATIVE  Genome  scale  DNA  sequencing  is  now  contributing  to  diagnosis  and  therapy  in  cases  of  rare  human  disease and cancer.  Full exploitation of these data is currently hampered by inadequate understanding  of which DNA changes affect protein function so as to contribute to disease. This project aims to develop  the methods and tools needed to remove that obstacle. ",Molecular impact of mutations in monogenic disease and cancer,9504498,R01GM120364,"['AIDS diagnosis', 'AIDS therapy', 'Address', 'Affect', 'Amino Acid Substitution', 'Clinic', 'Clinical', 'Communities', 'Comprehension', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Consensus', 'DNA', 'DNA sequencing', 'Data', 'Databases', 'Diagnosis', 'Dimensions', 'Disease', 'Goals', 'Information Resources', 'Knowledge', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mendelian disorder', 'Methods', 'Missense Mutation', 'Modeling', 'Molecular', 'Mutation', 'Mutation Analysis', 'Patients', 'Play', 'Probability', 'Procedures', 'Process', 'Proteins', 'Rare Diseases', 'Reporting', 'Resources', 'Role', 'Scientist', 'Structural Models', 'Structure', 'Tertiary Protein Structure', 'Text', 'Uncertainty', 'base', 'clinically relevant', 'crowdsourcing', 'experience', 'genome-wide', 'human disease', 'human model', 'improved', 'in vivo', 'interest', 'learning strategy', 'next generation', 'primary outcome', 'protein function', 'protein protein interaction', 'protein structure', 'prototype', 'targeted treatment', 'tool', 'trait', 'wiki']",NIGMS,"UNIV OF MARYLAND, COLLEGE PARK",R01,2018,336086,58406802,0.006014092250400296
"A Modular Automated Platform for Large-scale Drosophila Experiments and Handling PROJECT SUMMARY / ABSTRACT Animal model systems are a powerful tool researchers use to investigate almost all aspects of biology: genetics, development, neuroscience, disease, and more. And fruit flies – Drosophila melanogaster – with their small size, easy care, and remarkable array of available genetic toolkits, occupy a sweet spot on the model organism spectrum. Over 75% of human diseases with a genetic basis have an analogue in the fly, and Drosophila have been a part of the research for six Nobel prizes. Furthermore, the advent of CRISPR/cas9 and other modern genetic tools has opened the door to modeling other diseases and pathways, leading to greater use of Drosophila for drug screens. A great deal of the work (and the majority of the budget) involved in fly experiments is tedious manual labor, and with advances in computer vision, machine learning, and other analytic techniques, the stage is set to automate many phenotypic screens. In this Phase I SBIR, we propose a robotic system – modular automated platform for large-scale experiments (MAPLE) – that can accomplish a wide variety of fly-handling tasks in Drosophila labs. This robot is the fruit fly version of a liquid handling robot, with a large, open workspace that can house a plethora of modules and several manipulators that can move small parts and animals around that workspace. Building on a collaboration between the de Bivort Lab and FlySorter completed in 2017, we will design, fabricate and validate a commercial system that can collect virgin flies, run behavioral assays, conduct drug screens, and adapt to the needs of fly labs through easy-to-code Python scripts. By strategically combining modules and instructions to the robot, MAPLE can perform a wide variety of tasks in a fly lab, saving experimentalists from repetitive chores, cutting labor costs, and increasing scientific output. Just as pipette robots have become standard equipment in wet labs, we envision our fly handling robot will be the engine that powers Drosophila labs in academia and pharma, enabling new kinds of experiments and freeing researchers from the drudgery of fly pushing. PROJECT NARRATIVE Fruit flies – Drosophila melanogaster – are a powerful model organism used in the study of disease, neuroscience, development, genetics, and recently in drug screens, too, largely through phenotypic screening. This labor-intensive work is time consuming and expensive, and ripe for automation. We propose a fly-handling robot – analogous to a liquid pipetting robot in a wet lab – that can perform a variety of tasks in Drosophila labs, free researchers from the drudgery of fly pushing, and enable a broader spectrum of experiments that will increase scientific knowledge.",A Modular Automated Platform for Large-scale Drosophila Experiments and Handling,9623017,R43MH119092,"['Academia', 'Address', 'Affect', 'Air', 'Anesthesia procedures', 'Animal Model', 'Animals', 'Architecture', 'Automation', 'Basic Science', 'Behavior', 'Behavioral Assay', 'Biological Models', 'Biology', 'Budgets', 'CRISPR/Cas technology', 'Carbon Dioxide', 'Caring', 'Code', 'Collaborations', 'Computer Vision Systems', 'Computer software', 'Computers', 'Custom', 'Data Collection', 'Deposition', 'Detection', 'Development', 'Disease', 'Disease Pathway', 'Drosophila genus', 'Drosophila melanogaster', 'Drug Screening', 'Drug usage', 'Ensure', 'Equipment', 'Feedback', 'Genetic', 'Genetic Screening', 'Genetic study', 'Grant', 'Hand', 'Human', 'Instruction', 'Knowledge', 'Libraries', 'Liquid substance', 'Machine Learning', 'Manuals', 'Modeling', 'Modernization', 'Neurosciences', 'Nobel Prize', 'Organism', 'Output', 'Performance', 'Phase', 'Phenotype', 'Procedures', 'Protocols documentation', 'Pythons', 'Reagent', 'Research', 'Research Personnel', 'Robot', 'Robotics', 'Running', 'Savings', 'Scanning', 'Small Business Innovation Research Grant', 'Speed', 'Surface', 'System', 'Techniques', 'Testing', 'Time', 'Transgenic Organisms', 'Travel', 'Universities', 'Update', 'Vacuum', 'Work', 'analog', 'bone', 'cost', 'design', 'drug discovery', 'experimental study', 'flexibility', 'fly', 'graduate student', 'health science research', 'human disease', 'improved', 'operation', 'programs', 'repository', 'robot control', 'screening', 'tool', 'touchscreen']",NIMH,"FLYSORTER, LLC",R43,2018,348007,0,-0.024955582004644008
"Advanced Computational Approaches for NMR Data-mining ABSTRACT Nuclear magnetic resonance spectroscopy (NMR)-based metabolomics is a powerful method for identifying metabolic perturbations that report on different biological states and sample types. Compared to mass spectrometry, NMR provides robust and highly reproducible quantitative data in a matter of minutes, which makes it very suitable for first-line clinical diagnostics. Although the metabolome is known to provide an instantaneous snap-shot of the biological status of a cell, tissue, and organism, the utilization of NMR in clinical practice is hindered by cumbersome data analysis. Major challenges include high-dimensionality of the data, overlapping signals, variability of resonance frequencies (chemical shift), non-ideal shapes of signals, and low signal-to-noise ratio (SNR) for low concentration metabolites. Existing approaches fail to address these challenges and sample analysis is time-consuming, manually done, and requires considerable knowledge of NMR spectroscopy. Recent developments in the field of sparse methods for machine learning and accelerated convex optimization for high dimensional problems, as well as kernel-based spatial clustering show promise at enabling us to overcome these challenges and achieve fully automated, operator-independent analysis. We are developing two novel, powerful, and automated algorithms that capitalize on these recent developments in machine learning. In Aim 1, we describe ‘NMRQuant’ for automated identification and quantification of annotated metabolites irrespective of the chemical shift, low SNR, and signal shape variability. In Aim 2, we describe ‘SPA-STOCSY’ for automated de-novo identification of molecular fragments of unknown, non- annotated metabolites. Based on substantial preliminary data, we propose to evaluate these algorithms' sensitivity, specificity, stability, and resistance to noise on phantom, biological, and clinical samples, comparing them to current methods. We will validate the accuracy of analyses by experimental 2D NMR, spike-in, and mass spectrometry. The proposed efforts will produce new NMR analytical software for discovery of both annotated and non-annotated metabolites, substantially improving accuracy and reproducibility of NMR analysis. Such analytical ability would change the existing paradigm of NMR-based metabolomics and provide an even stronger complement to current mass spectrometry-based methods. This approach, once thoroughly validated, will enable NMR to reach wide network of applications in biomedical, pharmaceutical, and nutritional research and clinical medicine. NARRATIVE This project seeks to develop an advanced and automated platform for identifying NMR metabolomics biomarkers of diseases and for fundamental studies of biological systems. When fully developed, these approaches could be used to detect small molecules in the blood or urine, indicative of the onset of various diseases, drug toxicity, or environmental effects on the organism.",Advanced Computational Approaches for NMR Data-mining,9406318,R01GM120033,"['Address', 'Algorithms', 'Animal Disease Models', 'Biological', 'Biological Markers', 'Blood', 'Cancer Etiology', 'Cardiovascular Diseases', 'Cells', 'Chemicals', 'Clinic', 'Clinical', 'Clinical Medicine', 'Complement', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Drug toxicity', 'Early Diagnosis', 'Frequencies', 'Health', 'Human', 'Knowledge', 'Left', 'Libraries', 'Link', 'Machine Learning', 'Manuals', 'Mass Spectrum Analysis', 'Measures', 'Medical', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'NMR Spectroscopy', 'Nature', 'Neurodegenerative Disorders', 'Noise', 'Nuclear Magnetic Resonance', 'Nutritional', 'Obesity', 'Organism', 'Outcome', 'Patients', 'Pharmacologic Substance', 'Phenotype', 'Plague', 'Process', 'Regulation', 'Relaxation', 'Reporting', 'Reproducibility', 'Research', 'Residual state', 'Resistance', 'Sampling', 'Sensitivity and Specificity', 'Shapes', 'Signal Transduction', 'Societies', 'Sodium Chloride', 'Spectrum Analysis', 'Statistical Algorithm', 'Temperature', 'Time', 'Tissues', 'Treatment outcome', 'Urine', 'Variant', 'base', 'biological systems', 'biomarker discovery', 'clinical diagnostics', 'clinical implementation', 'clinical practice', 'data mining', 'experimental analysis', 'experimental study', 'high dimensionality', 'improved', 'infancy', 'metabolome', 'metabolomics', 'novel', 'personalized medicine', 'phenotypic biomarker', 'small molecule', 'stem']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2018,356625,323604360,0.01289135021324345
"A platform for mining, visualization and design of microbial interaction networks Project Summary One of the burning questions in the study of the human microbiome is whether and how it is possible to design specific strategies for rebalancing the taxonomic and functional properties of human-associated microbial communities, triggering the transition from “disease states” to “healthy states”. While empirical studies provide strong support for the idea that we may be able to cure, or at least  treat, a number of diseases by simply transplanting microbiomes, or inducing changes through taxonomic or environmental perturbations, to date little mechanistic understanding exists on how microbial communities work, and on how to extend microbiome research from an empirical science to a systematic, quantitative field of biomedicine. We propose here to establish a computational platform--   a database (Aim 1) with fully integrated analytical software (Aims 2 and 3) --- developed for and with the cooperation of the scientific community. The resource goes beyond cataloguing microbial abundances under different condition; its aim is to enable an understanding of networks of interacting species and their condition-dependence, with the goal of eventually facilitating disease diagnosis and prognosis, and designing therapeutic strategies for microbiome intervention. Our project is centered around three key aims: 1.	The creation of a Microbial Interaction Network Database (MIND), a public resource that will collect data on inter-species interactions from metagenomic sequencing projects, computer simulations and direct experiments. This database will be accessed through a web-based platform complemented with tools for microbial interaction network analysis and visualization, akin to highly fruitful tools previously developed for the study of genetic networks; the database will also serve as the public repository of microbial networks associated with human diseases; 2.	The implementation of an integrated tool for simulation of interspecies interactions under different environments, based on genomic data and whole-cell models of metabolism; 3.	The implementation of new algorithms for microbial community analysis and engineering. These algorithms, including stoichiometric, machine-learning and statistical approaches will facilitate a “synthetic ecology” approach to help design strategies (e.g. microbial transplants or probiotic mixtures) for preventing and targeting microbiome-associated diseases. Our work will fill a major gap in current microbiome research, creating the first platform for global microbial interaction data integration, mining and computation. Project Narrative Among the major developments of the genomic revolution has been the ability to identify thousands of microbial species and strains living in communities in 5 major habitats in the human body, and the recognition that the relative abundances of these populations is strongly correlated with environment: disease state, diet, treatment protocol and so on. A major challenge in utilizing the deluge of health relevant data is structuring it into a database that facilitates understanding inter-microbial interactions in these communities. The aim of this proposal is to create a database and integrated computational platform, open to and contributed to by the research community, which will greatly accelerate the conversion of data into health related actionable knowledge.","A platform for mining, visualization and design of microbial interaction networks",9420621,R01GM121950,"['Affect', 'Algorithms', 'Cataloging', 'Catalogs', 'Cell model', 'Clinical', 'Communities', 'Complement', 'Complex', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Dependence', 'Development', 'Diet', 'Discipline', 'Disease', 'Ecology', 'Ecosystem', 'Empirical Research', 'Engineering', 'Environment', 'Evolution', 'Future', 'Genetic', 'Genetic study', 'Genome', 'Genomics', 'Goals', 'Habitats', 'Health', 'Human Biology', 'Human Microbiome', 'Human body', 'Imagery', 'Individual', 'Intervention', 'Knowledge', 'Laboratories', 'Machine Learning', 'Measurable', 'Mediating', 'Metabolic', 'Metabolism', 'Metadata', 'Methods', 'Microbe', 'Mining', 'Nature', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pattern', 'Population', 'Preventive Medicine', 'Probiotics', 'Property', 'Research', 'Resources', 'Science', 'Scientist', 'Structure', 'Taxonomy', 'Technology', 'Therapeutic', 'Time', 'Transplantation', 'Treatment Protocols', 'Work', 'base', 'computer framework', 'data integration', 'data to knowledge', 'design', 'disease diagnosis', 'experimental study', 'feeding', 'genome-wide', 'genomic data', 'human disease', 'human microbiota', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microbiome research', 'microbiota transplantation', 'microorganism interaction', 'novel diagnostics', 'novel therapeutics', 'open source', 'outcome forecast', 'prevent', 'repository', 'simulation', 'tool', 'user-friendly']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2018,374683,61050884,0.006708013419382738
"Interpretable and extendable deep learning model for biological sequence analysis and prediction Project Abstract Bioinformatics and computational biology have become the core of biomedical research. The PI Dr. Dong Xu's work in this area focuses on development of novel computational algorithms, software and information systems, as well as on broad applications of these tools and other informatics resources for diverse biological and medical problems. He works on many research problems in protein structure prediction, post-translational modification prediction, high-throughput biological data analyses, in silico studies of plants, microbes and cancers, biological information systems, and mobile App development for healthcare. He has published more than 300 papers, with about 12,000 citations and H-index of 55. In this project, the PI proposes to develop deep-learning algorithms, tools, web resources for analyses and predictions of biological sequences, including DNA, RNA, and protein sequences. The availability of these data provides emerging opportunities for precision medicine and other areas, while deep learning as a cutting-edge technology in machine learning, presents a new powerful method for analyses and predictions of biological sequences. With rapidly accumulating sequence data and fast development of deep-learning methods, there is an urgent need to systematically investigate how to best apply deep learning in sequence analyses and predictions. For this purpose, the PI will develop cutting-edge deep-learning methods with the following goals for the next five years:  (1) Develop a series of novel deep-learning methods and models to specifically target biological sequence analyses and predictions in: (a) general unsupervised representations of DNA/RNA, protein and SNP/mutation sequences that capture both local and global features for various applications; (b) methods to make deep-learning models interpretable for understanding biological mechanisms and generating hypotheses; (c) “rule learning”, which abstracts the underlying “rules” by combining unsupervised learning of large unlabeled data and supervised learning of small labeled data so that it can classify new unlabeled data.  (2) Apply the proposed deep-learning model to DNA/RNA sequence annotation, genotype-phenotype analyses, cancer mutation analyses, protein function/structure prediction, protein localization prediction, and protein post-translational modification prediction. The PI will exploit particular properties associated with each of these problems to improve the deep-learning models. He will develop a set of related prediction and analysis tools, which will improve the state-of-art performance and shed some light on related biological mechanisms.  (3) Make the data, models, and tools freely accessible to the research community. The system will be designed modular and open-source, available through GitHub. They will be available like integrated circuit modules, which are universal and ready to plug in for different applications. The PI will develop a web resource for biological sequence representations, analyses, and predictions, as well as tutorials to help biologists with no computational knowledge to apply deep learning to their specific research problems. Relevance to Public Health Biological sequences, including DNA, RNA and protein sequences, represent the largest sources of growing big data in current biology and medicine, which provide tremendous opportunities for precision medicine, synthetic biology, and other areas. Deep learning as an emerging machine-learning method has a great potential in utilizing these data in biomedical research. This project will develop and apply cutting-edge deep- learning methods to deliver various sequence-based computational tools for gaining new knowledge, accelerating drug development, and improving personalized diagnosis and treatment.",Interpretable and extendable deep learning model for biological sequence analysis and prediction,9485584,R35GM126985,"['Algorithmic Software', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Big Data', 'Bioinformatics', 'Biological', 'Biological Models', 'Biology', 'Biomedical Research', 'Communities', 'Computational Biology', 'Computational algorithm', 'Computer Simulation', 'DNA', 'DNA Sequence', 'Data', 'Data Analyses', 'Development', 'Genotype', 'Goals', 'Healthcare', 'Information Systems', 'Knowledge', 'Label', 'Learning', 'Light', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Medicine', 'Methods', 'Microbe', 'Modeling', 'Mutation', 'Mutation Analysis', 'Paper', 'Performance', 'Phenotype', 'Plants', 'Plug-in', 'Post-Translational Protein Processing', 'Property', 'Proteins', 'Public Health', 'Publishing', 'RNA', 'RNA Sequences', 'Research', 'Resource Informatics', 'Sequence Analysis', 'Series', 'Source', 'Supervision', 'System', 'Technology', 'Work', 'computerized tools', 'deep learning', 'design', 'drug development', 'improved', 'indexing', 'learning strategy', 'mobile application', 'novel', 'online resource', 'open source', 'personalized diagnostics', 'personalized medicine', 'precision medicine', 'protein structure function', 'protein structure prediction', 'software systems', 'synthetic biology', 'tool', 'unsupervised learning']",NIGMS,UNIVERSITY OF MISSOURI-COLUMBIA,R35,2018,378183,63611576,0.025952459790816354
"Data-driven analysis of protein structure, function, and regulation PROJECT SUMMARY Proteins are capable of performing a wide variety of complex molecular functions and play a central role in all biological processes. A detailed and quantitative understanding of the relationship between a protein's sequence and it's biochemical properties would have a profound impact across all areas of biology, medicine, and biotechnology. We are developing data-driven approaches for dissecting the molecular basis of protein function. Our general framework involves designing informative libraries of protein sequences, experimentally mapping the relationship between sequence and function, and extracting detailed functional information from large sequence-function data sets. This work leverages emerging technologies and methods in DNA sequencing and synthesis, microfluidic screening, large-scale statistical learning, and optimization. We will develop generalizable platforms that can be applied to study a wide variety of enzymes and membrane transport proteins. PROJECT NARRATIVE A detailed and quantitative understanding of the relationship between a protein's sequence and it's biochemical properties would have a profound impact across all areas of biology, medicine, and biotechnology. This important capability would allow us to design new therapeutic and diagnostic proteins, and diagnose genetic diseases before they manifest symptoms. The goal of this proposal is to develop a general framework for dissecting the molecular basis of protein structure, function, and regulation.","Data-driven analysis of protein structure, function, and regulation",9532591,R35GM119854,"['Amino Acid Sequence', 'Area', 'Biochemical', 'Biological Process', 'Biology', 'Biotechnology', 'Complex', 'DNA biosynthesis', 'DNA sequencing', 'Data', 'Data Set', 'Emerging Technologies', 'Enzymes', 'Genetic Diseases', 'Goals', 'Libraries', 'Machine Learning', 'Medicine', 'Membrane Transport Proteins', 'Methods', 'Microfluidics', 'Molecular', 'Play', 'Property', 'Protein Analysis', 'Proteins', 'Regulation', 'Role', 'Symptoms', 'Work', 'design', 'genetic disorder diagnosis', 'novel diagnostics', 'novel therapeutics', 'protein function', 'protein structure function', 'screening']",NIGMS,UNIVERSITY OF WISCONSIN-MADISON,R35,2018,382500,338121506,0.03237762551668807
"Towards a comprehensive multiscale 3D human interactome network PROJECT SUMMARY/ABSTRACT Almost all proteins function through interacting with other proteins. On average, a protein interacts with ~5 other protein partners in the current human interactome. Therefore, it is of great importance to accurately determine the interface of each interaction, in order to understand how each protein works with different partners to carry out different functions. In our previous Nature Biotechnology study, we implemented a proteome-scale homology modeling approach to generate the first 3D human structural interactome: the interface for each interaction in this network was determined at atomic resolution through co-crystal structures and homology models. Using our 3D interactome, we found that, among >1,800 known disease genes associated with two or more clinically distinctly disorders, pairs of mutations on the same gene but in different interfaces with different partners are significantly more likely to cause distinct diseases. However, only 4,150 human protein interactions have co-crystal structures and 2,921 have high-quality homology models. ~50,000 interactions (87% of the current human interactome) are not amenable to current structural modeling methods. Here, we propose to develop a big-data-driven machine-learning approach integrating biophysiochemical, evolutionary, structural, and population genetic features to identify interaction- specific interfaces for the whole human interactome. Because several key features are unavailable for many proteins and interactions, we propose an innovative approach to use an ensemble of random forest classifiers, named Ensemble Protein Interface Classifier (EPIC), to address this large-scale non-random missing data problem (Aim 1). The high throughput of our massively parallel Clone-seq and INtegrated PrOtein INteractome perTurbation screening (InPOINT) pipeline! uniquely enables us to perform real-time experimental parameter optimization (in Years 2-4 we will clone ~1,500 mutations and examine their impact on ~2,500 interactions every year to iteratively evaluate and refine EPIC; Aim 2). Finally, we will construct a comprehensive multiscale 3D interactome for all known human protein-protein interactions: we will collect/generate atomic- resolution structural models for interactions whenever possible (co-crystal structures and homology models); we will accurately determine interaction-specific interface residues and domains for the whole human interactome. We will deploy an interactive web portal to disseminate our results and allow functional genomic inference in the context of our structural interactome (Aim 3). Our comprehensive multiscale 3D human interactome and the accompanying web portal will greatly reduce the barrier-to-entry for performing systematic structural analysis on a large number of proteins and their interactions, and open the flood gates for such analyses in genomic studies. NARRATIVE Almost all proteins function through interacting with other proteins and the structural details of these interaction interfaces are key in understanding protein function. However, the interfaces for vast majority of human protein interactions are currently unknown. Here, we propose to establish an innovative ensemble classifier approach and implement an unprecedented large-scale computational-experimental iterative learning scheme to predict interfaces for the whole human interactome, in anticipation that our predicted interfaces will help dissect functional sites of disease mutations and be useful for rational drug design to target these sites.",Towards a comprehensive multiscale 3D human interactome network,9520213,R01GM124559,"['Address', 'Big Data', 'Binding', 'Biotechnology', 'Cells', 'Clinical', 'Crystallization', 'Data', 'Disease', 'Docking', 'Drug Design', 'Evolution', 'Floods', 'Genes', 'Genomics', 'Goals', 'Graph', 'Homology Modeling', 'Human', 'Individual', 'Learning', 'Letters', 'Machine Learning', 'Methods', 'Missense Mutation', 'Modeling', 'Molecular', 'Mutation', 'Names', 'Nature', 'Population Genetics', 'Proteins', 'Proteome', 'Resolution', 'Resources', 'Scheme', 'Site', 'Structural Models', 'Structure', 'Study models', 'Testing', 'Time', 'Variant', 'Work', 'biological research', 'forest', 'functional genomics', 'human interactome', 'improved', 'innovation', 'learning strategy', 'protein function', 'protein protein interaction', 'screening', 'user-friendly', 'web portal', 'web-accessible']",NIGMS,CORNELL UNIVERSITY,R01,2018,384167,91477866,0.0197534589109463
"Defining the multi-dimensional code of zinc finger specificity-Resubmission-1 Project Summary The Cys2His2 zinc finger DNA-binding domain is the most common domain in human yet the DNA-binding specificities for the great majority of these proteins remain undefined. Mutations in many of these domains, both with and without known DNA-binding data, have been linked to a host of diseases from Alzheimers (REST) to Cancer (e.g. Slug, WT1, CTCF). Therefore, the characterization of these proteins holds great value. Unfortunately common methodologies used to determine the DNA-binding specificity of transcription factors have failed to address the zinc finger, at least in part because of an inability to fully define the large target specificities required of the average mammalian zinc finger protein. Even when ChIP-Seq data exists it is limited because the size of the genome does not allow us to capture the full binding potential of a factor that could offer a ≥21bp target sequence. As a result, without a comprehensive understanding of a protein’s binding potential, SNPs across the genome will continue to represent potential binding sites that we are unable to predict. In sum, decades of research have enlightened our understanding of this domain but we are still in the dark when it comes to its function as a transcription factors. Recently we have taken an alternative approach to define this domain, demonstrating that a synthetic, one-by-one screen of individual zinc fingers allows us to predict the specificity of multi-fingered proteins with similar or greater accuracy than all prior prediction algorithms. However, this approach fails to take into consideration the influences that adjacent fingers have on one another. We have produced the equivalent of a comprehensive snapshot of what a zinc finger is capable of in just one of many potential contextual environments. Here we propose to scale this approach and screen the zinc finger under an inclusive set of contextual environments. We will consider the most common direct and indirect influences on adjacent finger binding as well as factors that impact the geometry with which the zinc fingers engage the DNA. We will use these results to provide a complete picture of how adjacent zinc fingers determine their specificity and by scaffolding these two-fingered models, predict and design the specificity of large, multi-fingered proteins. In this way, we will define a multi-dimensional code of zinc finger specificity that allows us to predict all zinc finger DNA-binding specificities, how any neighbor finger context would modify this specificity, and the factors that result in adjacent finger incompatibility and loss of DNA-binding function. We will apply this model to predict the specificity of all human zinc finger proteins, validate these predictions through in vivo characterization of an informed set of transcription factors, and test predicted mechanisms of multi-fingered binding with designer, artificial factors. Project Narrative The proposed research is relevant to public health because the ZF domain is the most common in human yet it remains largely uncharacterized. A holistic understanding of ZF function will provide insight into how ZF mutations are related to disease and allow us to predict harmful binding sites due to SNPs across the genome.",Defining the multi-dimensional code of zinc finger specificity-Resubmission-1,9435146,R01GM118851,"['Achievement', 'Address', 'Alzheimer&apos', 's Disease', 'Amino Acids', 'Binding', 'Binding Proteins', 'Binding Sites', 'ChIP-seq', 'Charge', 'Code', 'Collection', 'Communities', 'Comprehension', 'DNA', 'DNA Binding', 'DNA Binding Domain', 'Data', 'Disease', 'Distal', 'Environment', 'Exposure to', 'Fingers', 'Genetic Transcription', 'Genome', 'Geometry', 'Goals', 'Human', 'Hybrids', 'Individual', 'Lead', 'Libraries', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Mammals', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Nucleotides', 'Proteins', 'Public Health', 'Reporter', 'Research', 'Sampling', 'Schizophrenia', 'Series', 'Specificity', 'Structure', 'Sum', 'System', 'Systems Biology', 'Testing', 'WT1 gene', 'Work', 'Zinc Fingers', 'base', 'design', 'exhaustion', 'experimental study', 'in vivo', 'insight', 'loss of function', 'model design', 'prediction algorithm', 'predictive modeling', 'predictive test', 'scaffold', 'screening', 'slug', 'transcription factor', 'user-friendly', 'web site']",NIGMS,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,R01,2018,393339,329565273,0.003535696619737962
"Joint Analysis of Microbiome and Other Genomic Data Types PROJECT SUMMARY In the same way that the human genome project created invaluable genomic maps, the objective of this project is to develop methods for eventual construction of comprehensive genetic and metabolomic by microbome relationship maps. Such maps would be an invaluable resource for improving our understanding as to the underlying mechanisms by which microbes and –omics features influence human diseases and conditions, potentially leading to identification of novel therapeutic targets. To these ends, this proposal seeks to develop statistical and computational tools for mapping associations and interactions between microbes and other – omic features and for further utilizing other –omics to improve microbiome based prediction models. Specifically, motivated by studies examining the role of the vaginal microbiome and other –omics in birth outcomes and menopause, we aim to develop statistical methodology for (1) mapping genetic variants that influence microbiome composition so as to understand the innate component of the microbiome as well as learn mechanisms by which genetics influence outcomes; (2) creating global metabolic maps integrating both microbes and metabolites which will enable understanding of how perturbations might influence the system and identify key pathways for therapeutic target; (3) exploiting other –omics in constructing more accurate microbiome based prediction models for preterm birth; (4) developing, distributing and supporting software packages for the proposed methods. The methods are based on frameworks in which we have considerable experience, but novel technical contributions are made to accommodate features of the data such as population stratification and relatedness in genetics, phylogenetic structure, and compositionality, as well as practical considerations such as availability of samples and other –omics data. Consequently, these new methods have the potential for accelerating mechanistic and translational microbiome studies, developing vital resources for enabling systematic achievement of many biological, clinical, and public health problems that have eluded researchers for decades. PROJECT NARRATIVE The methods developed in this proposal will enable improved understanding of the interactions between microbes and other –omics, thus aiding in elucidation of the mechanisms by which microbes and –omic features influence health outcomes and aiding in identification of potential molecular targets. Further emphasis is placed on utilization of other –omics to develop microbiome based prediction models in pregnancy outcomes, improving early detection of women who are at risk of preterm delivery.",Joint Analysis of Microbiome and Other Genomic Data Types,9577818,R01GM129512,"['Achievement', 'Address', 'Area', 'Automobile Driving', 'Biological', 'Birth', 'Chromosome Mapping', 'Clinical', 'Complex', 'Computer software', 'Data', 'Data Set', 'Development', 'Early Diagnosis', 'Future', 'Genes', 'Genetic', 'Genomics', 'Grouping', 'Health', 'Human Genome Project', 'Joints', 'Lasso', 'Learning', 'Machine Learning', 'Maintenance', 'Maps', 'Medical', 'Menopause', 'Metabolic', 'Methodology', 'Methods', 'Microbe', 'Modeling', 'Modification', 'Molecular Target', 'Network-based', 'Outcome', 'Pathway interactions', 'Performance', 'Phylogenetic Analysis', 'Pregnancy Outcome', 'Premature Birth', 'Procedures', 'Public Health', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'Role', 'Sampling', 'Scientific Advances and Accomplishments', 'Structure', 'System', 'Taxonomy', 'Training', 'Woman', 'Work', 'base', 'biological systems', 'computerized tools', 'experience', 'genetic variant', 'genomic data', 'human disease', 'improved', 'interest', 'loss of function', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microbiome analysis', 'microbiome components', 'microbiome composition', 'microbiome research', 'new therapeutic target', 'novel', 'open source', 'population stratification', 'predictive modeling', 'simulation', 'software development', 'therapeutic target', 'tool', 'translational study', 'user friendly software', 'vaginal microbiome']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2018,396000,758431960,-0.008849231567807293
"Informatics Tools for Quantitative Digital Pathology Profiling and Integrated Prognostic Modeling PROJECT SUMMARY Accurate biomarker-driven prognostic stratification, response prediction, and cohort enrichment are critical for realizing precision treatment strategies and population health management approaches that optimize quality of life and survival for cancer patients. Genomics holds promise for improving classification and prognostication of malignancies, yet oncology practice continues to rely heavily on immunohistochemistry (IHC) as a fundamental tool due to its practicality and ability to provide protein-level and subcellular localization information. The goal of this proposal is to create an open-source software resource for the quantitative analysis of IHC stained tissues and effective integration of IHC, genomic, and clinical features for cancer classification and prognostication. This proposal builds on our collective experience in computer-assisted analysis of microscopic images (including IHC images), development of machine-learning methods to address the challenges of classification and prognostication with heterogeneous and high-dimensional data, and leadership in collection and large-scale analysis of cancer outcomes involving collaboration with multiple medical centers. This effort for the first time will create tools to integrate quantitative IHC imaging, clinical, and genomic information that will in turn enable the research community to explore strategies for the classification of malignancies and prediction of outcomes. The proposed tools will be developed and extensively validated in close collaboration with clinical, genomic, and digital pathology data from the NCI-supported Lymphoma Epidemiology of Outcomes (LEO) cohort study. The software tools produced by this proposal will enable the characterization of subcellular protein expression in cell nuclei, membranes and cytoplasmic compartments. Spatial features of protein expression heterogeneity, along with patient-level summaries of protein expression will be used to develop machine-learning classifiers for cancer subtypes, using diffuse large b-cell lymphomas as a driving application. Technology for automatic tuning of machine learning algorithms will enable a broad class of clinically and biologically motivated users to utilize these tools in their investigations. We will also provide an interactive dashboard that enables users to integrate genomic and IHC-based features to explore prognostic models of patient survival. These tools will be released and documented under an open-source model, integrated with HistomicsTK (https://histomicstk.readthedocs.io/en/latest/), and available to the broader cancer research community. PUBLIC HEALTH RELEVANCE Classification of a patient’s cancer is critical for personalizing their therapy, and in many cases involves the pathologic examination of tissues treated with multiple immunohistochemical stains. This process is highly subjective, leading to problems with reliable classification and suboptimal treatment and this proposal is focused on developing software tools to improve the reliability of cancer diagnosis and prognosis. These tools will provide a reliable means of interpreting information derived from a practice known as immunohistochemistry to improve the consistency of diagnosis and patient outcomes.",Informatics Tools for Quantitative Digital Pathology Profiling and Integrated Prognostic Modeling,9507539,U01CA220401,"['Academic Medical Centers', 'Address', 'Adopted', 'Algorithms', 'Automobile Driving', 'Behavior', 'Biological', 'Cancer Patient', 'Cancer Prognosis', 'Cations', 'Cell Nucleus', 'Classification', 'Clinic', 'Clinical', 'Clinical Trials', 'Cohort Studies', 'Collaborations', 'Collection', 'Color', 'Communities', 'Computer Assisted', 'Computer software', 'Data', 'Data Set', 'Decision Trees', 'Development', 'Diagnosis', 'Disease', 'ERBB2 gene', 'Enrollment', 'Epidemiology', 'Flowers', 'Genetic Markers', 'Genomics', 'Goals', 'Heterogeneity', 'Histology', 'Image', 'Image Analysis', 'Immunohistochemistry', 'Informatics', 'Institution', 'Investigation', 'Leadership', 'Learning', 'Letters', 'Lymphoma', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Membrane', 'Methods', 'Modeling', 'Newly Diagnosed', 'Non-Hodgkin&apos', 's Lymphoma', 'Outcome', 'Pathologic', 'Pathologist', 'Patient-Focused Outcomes', 'Patients', 'Precision therapeutics', 'Prediction of Response to Therapy', 'Process', 'Proteins', 'Quality of life', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Science', 'Site', 'Software Tools', 'Stains', 'Standardization', 'Stratification', 'Technology', 'Testing', 'Time', 'Tissue Model', 'Tissue Stains', 'Tissues', 'Variant', 'Work', 'anticancer research', 'base', 'biomarker-driven', 'cancer classification', 'cancer diagnosis', 'cancer subtypes', 'cancer survival', 'clinical practice', 'clinical research site', 'cohort', 'dashboard', 'data management', 'design', 'digital', 'digital pathology', 'experience', 'follow-up', 'genomic biomarker', 'genomic data', 'health management', 'high dimensionality', 'improved', 'interactive tool', 'large cell Diffuse non-Hodgkin&apos', 's lymphoma', 'learning strategy', 'malignant breast neoplasm', 'microscopic imaging', 'neoplasm resource', 'oncology', 'open source', 'operation', 'outcome prediction', 'pathology imaging', 'patient subsets', 'population health', 'predict clinical outcome', 'predicting response', 'predictive modeling', 'prognostic', 'prognostic value', 'protein expression', 'public health relevance', 'software development', 'tissue processing', 'tool', 'translational scientist', 'treatment strategy', 'tumor heterogeneity', 'whole slide imaging']",NCI,EMORY UNIVERSITY,U01,2018,467872,507546965,-0.03287062513879911
"Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions Project Summary/Abstract People with mobility restrictions and limited to no upper extremity use depend on others to position the objects that they rely upon for health, communication, productivity, and general well-being. The technology developed in this project directly increases users’ independence by responding to them without another person’s intervention. The independence resulting from the proposed technology allows a user to perform activities related to self-care and communication. Eye tracking and head tracking access to speech devices, tablets, or computers requires very specific positioning. If the user’s position relative to the device are not aligned precisely, within a narrow operating window, the device will no longer detect the eyes or head, rendering the device inoperable. Auto-positioning technology uses computer vision and robotic positioning to automatically move devices to a usable position. The system moves without assistance from others, ensuring the user has continual access to communication. In a generalized application, the technology can target any area of the user’s body to position relevant equipment, such as for hydration or a head array for power wheelchair control. Research and development of the automatic positioning product will be accomplished through user-centered, iterative design, and design for manufacturing. Input from people with disabilities, their family members, therapists, and assistive technology professionals define the functional requirements of the technology and guide its evolution. Throughout technical development, prototype iterations are demonstrated and user-tested, providing feedback to advance the design. Design for manufacturability influences the outcomes to optimize the product pipeline, ensure high quality and deliver a cost effective product. Phase 1 Aims demonstrate the feasibility of 1) movement and control algorithms 2) face tracking algorithms and logic to maintain device positioning and 3) integration with commercial eye tracking device software development kits (SDK). Phase 2 Aims include technical, usability, and manufacturability objectives leading to development of a product for commercialization. Software development advances computer vision capabilities to recognize facial expressions and gestures. A new sensor module serves as a gesture switch or face tracker. App development provides the user interface, remote monitoring and technical support. Design of scaled down actuators and an enclosed three degree of freedom alignment module reduces the form factor, allowing for smaller footprint applications. Rigorous user testing by people with different diagnoses and end uses will ensure the product addresses a range of needs and is easy to use. Testing involves professionals and family members to evaluate ease of set-up, functionality, and customization. Extended user testing will investigate and measure outcomes and effects on their independence, access and health. Prototype tooling and design for cost-effective manufacturing will produce units for user and regulatory testing, and eventual sale. Project Narrative People who are essentially quadriplegic, with significant mobility impairments and limited to no upper extremity use, are dependent on others to maintain proper positioning for access to devices essential for their health, mobility and communication, such as eye gaze speech devices, call systems, phones, tablets, wheelchair controls, and hydration and suctioning tubes. Auto-positioning technology uses computer vision to detect specific targets, such as facial features, to control a robotic mounting and positioning system; automatically repositioning devices for best access without assistance from others, even when their position changes. This technology enables continuous access to communication, maintains one’s ability to drive a wheelchair and allows a person to drink or suction themselves, providing good hydration, respiratory health and hygiene.",Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions,9644103,R44HD093467,"['Address', 'Advanced Development', 'Algorithms', 'Area', 'Articular Range of Motion', 'Beds', 'Caring', 'Cerebral Palsy', 'Communication', 'Computer Vision Systems', 'Computers', 'Custom', 'Data', 'Dependence', 'Development', 'Devices', 'Diagnosis', 'Disabled Persons', 'Duchenne muscular dystrophy', 'Ensure', 'Equipment', 'Evolution', 'Eye', 'Face', 'Facial Expression', 'Family', 'Family member', 'Feedback', 'Freedom', 'Gestures', 'Goals', 'Head', 'Head and neck structure', 'Health', 'Health Communication', 'Hydration status', 'Hygiene', 'Immunity', 'Impairment', 'Individual', 'Intervention', 'Intuition', 'Joints', 'Lighting', 'Logic', 'Methods', 'Monitor', 'Motor', 'Movement', 'Oral cavity', 'Outcome', 'Outcome Measure', 'Personal Satisfaction', 'Persons', 'Phase', 'Positioning Attribute', 'Powered wheelchair', 'Productivity', 'Publishing', 'Quadriplegia', 'Quality of life', 'Research Design', 'Robotics', 'Safety', 'Sales', 'Saliva', 'Self Care', 'Self-Help Devices', 'Services', 'Signal Transduction', 'Spastic Tetraplegia', 'Speech', 'Spinal Muscular Atrophy', 'Spinal cord injury', 'Suction', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Tube', 'Update', 'Upper Extremity', 'Variant', 'Wheelchairs', 'Work', 'application programming interface', 'base', 'body system', 'commercialization', 'communication device', 'cost', 'cost effective', 'cost effectiveness', 'design', 'experience', 'gaze', 'improved', 'iterative design', 'manufacturability', 'meetings', 'product development', 'prototype', 'psychosocial', 'research and development', 'respiratory health', 'sensor', 'software development', 'tool', 'usability', 'user centered design']",NICHD,"BLUE SKY DESIGNS, INC.",R44,2018,519349,0,-0.035706585597099924
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9527181,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2018,545116,685608202,0.006598087424920724
"Quantifying molecular consequences of human missense variants with large-scale interactome perturbation studies Almost all proteins function through interacting with other proteins. Previous studies have shown that the vast majority of damaging single amino acid mutations in proteins disrupt only a subset of specific protein-protein interactions, and that mutations in the same protein that disrupt different interactions tend to cause clinically distinct disorders. Therefore, it is of great importance to determine interaction-specific disruptions caused by each mutation. Furthermore, rapid advances in sequencing technologies have enabled the identification of tens of millions of single nucleotide variants (SNVs) in the human population, driving an urgent need to understand the impact of each SNV on the human interactome network. Unfortunately, there is currently no method that is capable of predicting the specific impact of a large fraction of these SNVs on individual protein-protein interactions. To address this issue, we propose to leverage our massively-parallel site-directed mutagenesis pipeline, Clone-seq, to generate clones for ~6,000 coding SNVs in the human population: ~4,000 from gnomAD and ~2,000 to be submitted by the international human genetics community. We will then experimentally examine the impact on protein stability and individual protein-protein interactions for every variant using high-throughput DUAL-FLUO and InPOINT (integrating PCA, LUMIER, Y2H, and wNAPPA) assays. This proposal brings together three groups with complementary expertise in high-throughput interactome experiments and network analysis from the Yu lab, in genomic and population genetic studies from the Clark lab, and in comprehensive biophysical and structural modeling of mutation’s impact on binding free energy of protein interactions from the Alexov lab. Out of the ~6,000 SNVs, we expect to identify ~1,200 disruptive SNVs and ~4,000 different SNV-interaction pairs where the SNV disrupt that specific interaction. The data produced by our project will increase the available experimental information by >140× in number of human proteins and >500× in number of interactions, allowing us for the first time to comprehensively assess the relationships between the impact of SNVs on interactions and their various population genetic attributes (including, but not limited to, allele frequency and flanking haplotype, inter-population differentiation, local rate of recombination, allele age, modes of selection). Finally, we will establish a computational-experimental- integrated iterative learning scheme to build a multi-layer random-forest-based framework, SIMPACT, which can accurately predict specific impacts on all individual protein-protein interactions for all missense SNVs. Our proposed work will fuel hypothesis-driven research, will significantly improve our functional understanding of variants, and will likely fundamentally change the experimental design and data interpretation for whole genome/exome studies going forward. The dramatic increase of DNA variants discovered through advances in sequencing technologies has been inadequately translated into therapeutic successes. Although many of these variants are related to human disorders, the overwhelming number of non-functional variants makes the assessment of functional significance a steep challenge. In this study, we aim to develop a high-throughput pipeline to quickly clone and directly test a large number of coding variants for their impact on the human interactome network and use the results to build a machine learning pipeline to predict functional impact of all coding variants, in anticipation that both our experimental data and computational pipeline will lead to broad clinical and therapeutic applications.",Quantifying molecular consequences of human missense variants with large-scale interactome perturbation studies,9420242,R01GM125639,"['Address', 'Age', 'Alleles', 'Amino Acids', 'Automobile Driving', 'Binding', 'Biochemical', 'Biological Assay', 'Biophysics', 'Cells', 'Clinical', 'Code', 'Communities', 'Coupling', 'Crystallization', 'DNA', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease', 'Experimental Designs', 'Free Energy', 'Gene Frequency', 'Genes', 'Genetic Recombination', 'Genetic study', 'Genome', 'Genomics', 'Haplotypes', 'Homology Modeling', 'Human', 'Human Genetics', 'Individual', 'International', 'Learning', 'Letters', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Molecular', 'Mutation', 'Nucleotides', 'Output', 'Pathway Analysis', 'Population', 'Population Genetics', 'Property', 'Proteins', 'Proteomics', 'Research', 'Resolution', 'Resources', 'Scheme', 'Site-Directed Mutagenesis', 'Structural Models', 'Structure', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Translating', 'Variant', 'Work', 'base', 'biological research', 'biophysical model', 'biophysical techniques', 'exome', 'experimental study', 'forest', 'human interactome', 'improved', 'interest', 'molecular phenotype', 'mutant', 'next generation sequencing', 'protein function', 'protein protein interaction', 'screening', 'success', 'web portal', 'whole genome']",NIGMS,CORNELL UNIVERSITY,R01,2018,577782,91477866,0.007632084166965168
"Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine The overall goal of this project is to develop a safe, broadly effective, and affordable vaccine to prevent group A streptococcal infections. Antibodies against the N-terminal hypervariable region (HVR) of surface M (Emm) proteins of GAS are opsonic and are associated with protection against infection. Immunity has classically been described as “type-specific”, leading to the assumption that natural immunity confers protection against only one of the more than 200 different emm types of GAS. We now have new information that calls into question this classic view and serves as the basis for an entirely different approach to GAS vaccine design and development. A recent comprehensive sequence analysis of M proteins from a global collection of 175 emm types of GAS resulted in a new emm cluster typing system that classified 96.2% of all contemporary GAS isolates into 48 emm clusters containing structurally and functionally related M proteins. Moreover, 117 emm types contained in 16 clusters accounted for 94.4% of GAS infections in the world. Indeed, preclinical studies indicated that a multivalent vaccine containing N-terminal peptides from 30 prevalent M types cross-opsonized a significant number of non-vaccine emm types of GAS that co-localized in clusters with vaccine emm types. The frequency of cross-opsonic antibodies, combined with the emm cluster data, prompted us to conclude that there is a need for a paradigm shift away from the concept of “type-specific” immunity against GAS infections to one of “cluster-specific” immunity. Our overall hypothesis is that immunity to GAS infections is the result of both type-specific and cross-reactive antibodies against the N-terminal regions of M proteins and that a new approach employing computational predictions of peptide structures will result in a multivalent vaccine that will induce broadly protective immunity in populations throughout the world. Our preliminary results indicate the feasibility of using structure-based design to predict the antigenic relatedness of M peptides within a cluster. The specific aims of this proposal are to: 1) Apply computational structure-based design in an iterative process with immunological data from Aim 2 to predict the minimal number of M peptide sequences that are most representative of the structural and physicochemical properties of the peptides in one emm cluster containing 17 GAS emm types, 2) determine the cross-reactive immunogenicity of the selected peptides with all seventeen emm types of GAS in the cluster, and apply the results to refine the computational design predictions in Aim 1, 3) apply the refined computational parameters from Aims 1 and 2 to analyze the remaining epidemiologically important emm clusters, select a comprehensive panel of peptides representing all emm types, construct four multivalent recombinant vaccine proteins, and assess potential cross-protective immunogenicity using in vitro bactericidal assays against all 117 emm types of GAS, and 4) determine the protective immunogenicity of the final multivalent vaccine in unique transgenic mice expressing human C4BP and factor H that will be immunized and then challenged with multiple emm types of GAS. The world needs an effective, safe and affordable vaccine to prevent group A streptococcal (GAS) infections. Although most GAS infections are mild, there are more than 18 million people with a chronic complication of a severe GAS disease worldwide, over 15 million of whom have rheumatic heart disease, another 2 million cases of severe disease occur each year and a total of 517,000 deaths annually are estimated to be due to this organism. Vaccine prevention of even a fraction of these life-threatening diseases could have a significant impact on the health of people around the world.",Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine,9502903,R01AI132117,"['Animals', 'Antibodies', 'Bacteria', 'Base Sequence', 'Binding', 'Biological Assay', 'Cell surface', 'Cells', 'Cessation of life', 'Chronic', 'Collection', 'Complement Factor H', 'Complementarity Determining Regions', 'Complication', 'Computer Analysis', 'Data', 'Development', 'Disease', 'Ensure', 'Enzyme-Linked Immunosorbent Assay', 'Epidemiology', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Human', 'Immune', 'Immune Sera', 'Immunity', 'Immunize', 'Immunologics', 'In Vitro', 'Infection', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Mus', 'N-terminal', 'Natural Immunity', 'Organism', 'Oryctolagus cuniculus', 'Peptide Vaccines', 'Peptide antibodies', 'Peptides', 'Population', 'Prevention', 'Process', 'Property', 'Proteins', 'Recombinant Vaccines', 'Recombinants', 'Rheumatic Heart Disease', 'Sequence Analysis', 'Streptococcal Infections', 'Streptococcal Vaccines', 'Structure', 'Surface', 'System', 'Testing', 'Transgenic Mice', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'bactericide', 'base', 'cross reactivity', 'design', 'experimental study', 'flexibility', 'hybrid protein', 'immunogenic', 'immunogenicity', 'innovation', 'molecular dynamics', 'multiple myeloma M Protein', 'novel', 'novel strategies', 'peptide structure', 'preclinical study', 'prevent', 'protein aminoacid sequence', 'protein structure', 'retinal S antigen peptide M', 'synthetic peptide', 'tool', 'vaccine development', 'vaccine evaluation']",NIAID,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2018,879004,46216755,0.013057258446658905
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9478117,U54AI117924,"['Address', 'Biological', 'Blood coagulation', 'Breast Cancer Risk Factor', 'Clinical', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'biomedical scientist', 'clinical investigation', 'clinical predictors', 'education research', 'graduate student', 'high dimensionality', 'improved', 'innovation', 'interest', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning', 'undergraduate student']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2018,897471,338121506,0.013915772071026993
"Mobility Data Integration to Insight     DESCRIPTION (provided by applicant): Mobility is essential for human health. Regular physical activity helps prevent heart disease and stroke, relieves symptoms of depression, and promotes weight loss. Unfortunately, many conditions, such as cerebral palsy, osteoarthritis, and obesity, limit mobility at an enormous personal and societal cost. While vast amounts of data are available from hundreds of research labs and millions of smartphones, there is a dearth of methods for analyzing this massive, heterogeneous dataset.  We propose to establish the National Center for Mobility Data Integration to Insight (the Mobilize Center) to overcome the data science challenges facing mobility big data and biomedical big data in general. Our preliminary work identified four bottlenecks in data science, which drive four Data Science Research Cores.  The Cores include Biomechanical Modeling, Statistical Learning, Behavioral and Social Modeling, and Integrative Modeling and Prediction. Our Cores will produce novel methods to integrate diverse modeling modalities and gain insight from noisy, sparse, heterogeneous, and time-varying big data. Our data-sharing consortia, with clinical, research, and industry partners, will provide mobility data for over ten million people.  Three Driving Biomedical Problems will focus and validate our data science research.  The Mobilize Center will disseminate our novel data science tools to thousands of researchers and create a sustainable data-sharing consortium. We will train tens of thousands of scientists to use data science methods in biomedicine through our in-person and online educational programs. We will establish a cohesive, vibrant, and sustainable National Center through the leadership of an experienced executive team and will help unify the BD2K consortia through our Biomedical Computation Review publication and the Simtk.org resource portal.  The Mobilize Center will lay the groundwork for the next generation of data science systems and revolutionize diagnosis and treatment for millions of people affected by limited mobility.         PUBLIC HEALTH RELEVANCE:  Regular physical activity is essential for human health, yet a broad range of conditions impair mobility. This project will transform human movement research by developing tools for data analysis and creating software that will advance research to prevent, diagnose, and reduce impairments that limit human movement.            ",Mobility Data Integration to Insight,9542295,U54EB020405,"['Accelerometer', 'Affect', 'Area', 'Automobile Driving', 'Behavioral', 'Behavioral Model', 'Big Data', 'Big Data to Knowledge', 'Biomechanics', 'Biomedical Computing', 'Biomedical Research', 'Body Weight decreased', 'Cellular Phone', 'Cerebral Palsy', 'Child', 'Classification', 'Clinical Research', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Data Sources', 'Degenerative polyarthritis', 'Diabetes Mellitus', 'Diagnosis', 'Educational workshop', 'Elderly', 'Ethics', 'Exercise', 'Fellowship', 'Fostering', 'Gait', 'Health', 'Heart Diseases', 'Human', 'Impairment', 'Individual', 'Injury', 'Joints', 'Leadership', 'Limb structure', 'Machine Learning', 'Medical center', 'Methods', 'Mission', 'Modality', 'Modeling', 'Movement', 'NCI Scholars Program', 'Nature', 'Obesity', 'Overweight', 'Pathology', 'Personal Satisfaction', 'Persons', 'Physical activity', 'Prevention', 'Problem Solving', 'Public Health', 'Publications', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Running', 'Scientist', 'Stroke', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Walking', 'Work', 'base', 'biomechanical model', 'clinical decision-making', 'cognitive function', 'cohesion', 'cost', 'data integration', 'data modeling', 'data sharing', 'depressive symptoms', 'experience', 'flexibility', 'health data', 'improved', 'improved outcome', 'industry partner', 'insight', 'massive open online courses', 'models and simulation', 'motor impairment', 'next generation', 'novel', 'novel strategies', 'online resource', 'prevent', 'programs', 'public health relevance', 'reduce symptoms', 'role model', 'social', 'social model', 'societal costs', 'surgery outcome', 'tool', 'visiting scholar', 'wearable device']",NIBIB,STANFORD UNIVERSITY,U54,2018,955747,560644462,0.010724549121839193
"Understanding molecular rules governing protein allostery by deep mutational scanning Project Summary/Abstract Allostery is action at a distance in proteins. It is a property by which perturbation at one site of a protein causes an effect at a distant site of the protein. As nature's biological switches allosteric proteins regulate virtually every major process including catalysis, transcription, transport and signaling. Kinases, G-protein coupled receptors and nuclear receptors are therapeutically important allosteric proteins that play a major role in human health. Disruption of allosteric communication caused by mutations in these proteins is strongly associated with many types of disease abnormalities including cancer. Since the discovery of allostery in the 1960s, the molecular mechanism by which perturbation at one end of a protein is allosterically transmitted to the other end remains a mystery. Understanding molecular “rules” governing allostery is a fundamental problem in protein biochemistry and biophysics. Such rules may provide a deep insight into how proteins work through interactions between residues. Current approaches to studying allostery, which are biophysical, structural or computational, have two limitations. First, they provide an incomplete picture. Allostery involves the interplay of protein dynamics, structural changes and their effects on function. Probing each independently, as most studies do, does not give a complete understanding. Structure alone cannot explain dynamics, dynamical movement does not imply functional role, and functional studies may not provide insight into the underlying molecular causes. Second, detailed mechanistic studies of individual allosteric proteins while invaluable, are tedious, and cannot be scaled up to investigate many types of allosteric proteins. As a result, unifying heuristic rules would be difficult to infer from such individual case studies. The goal of my research is to develop a generalized, scalable method integrating structure, function and dynamics to understand, quantify and predict molecular drivers of protein allostery. With allosteric transcription factors as a model system, we will determine residues important for allostery (`hotspots') and their connectivity (`pathway') by deep mutational scanning, a method for large-scale functional characterization. We establish a critical link between structure and function by computationally modeling each mutation with Rosetta. We use machine learning on this rich sequence-structure-function dataset to recognize common molecular features (van der Waals, electrostatics, hydrogen bonds etc.) of allosteric hotspot residues, to build a predictive molecular model of allostery. Thus, we integrate high-throughput functional studies, structure-based modeling and machine learning to understand molecular rules governing allostery. Any allosteric protein whose activity can be coupled to a high-throughput screen, of which there are many, is amenable to our approach. Over time as the number and diversity of allosteric protein datasets increases, we expect that the accuracy and generalizability of predictions will continuously improve. Our long-term objective is to be able to (a) predict functional impact of thousands of mutations in disease-associated allosteric proteins revealed by genome sequencing (b) discover novel allosteric sites that improve selectivity and efficacy of drugs. Project Narrative GPCRs, nuclear receptors, ion channels and kinases are allosteric proteins that alone account for 44% of all human protein drug targets. We propose to develop a broad experimental and computational platform to understand molecular mechanism of allostery. This study will pave the way to explaining how dysfunction in allosteric proteins manifests into disease, and developing `smart' (allosteric) drugs that restore normal function.",Understanding molecular rules governing protein allostery by deep mutational scanning,9562195,DP2GM132682,"['Allosteric Site', 'Biological', 'Biological Models', 'Biophysics', 'Case Study', 'Catalysis', 'Communication', 'Computer Simulation', 'Coupled', 'Data Set', 'Disease', 'Distant', 'Drug Targeting', 'Electrostatics', 'Functional disorder', 'G-Protein-Coupled Receptors', 'Genetic Transcription', 'Goals', 'Health', 'Human', 'Hydrogen Bonding', 'Individual', 'Ion Channel', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Molecular', 'Movement', 'Mutation', 'Nature', 'Nuclear Receptors', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phosphotransferases', 'Play', 'Process', 'Property', 'Protein Biochemistry', 'Protein Dynamics', 'Proteins', 'Research', 'Role', 'Signal Transduction', 'Site', 'Structure', 'Therapeutic', 'Time', 'Work', 'base', 'drug efficacy', 'genome sequencing', 'heuristics', 'high throughput screening', 'improved', 'insight', 'molecular modeling', 'mutation screening', 'novel', 'scale up', 'transcription factor', 'virtual']",NIGMS,UNIVERSITY OF WISCONSIN-MADISON,DP2,2018,2146877,338121506,0.027059839175912422
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9266344,U54AI117924,"['Address', 'Biological', 'Blood coagulation', 'Clinical', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'biomedical scientist', 'clinical investigation', 'clinical predictors', 'education research', 'graduate student', 'high dimensionality', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning', 'undergraduate student']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2017,18271,338121506,0.013915772071026993
"Proteomics of ciliopathy protein complexes No abstract available Project Narrative! Ciliopathies are a class of birth defects caused by defective cilia. This project will determine conserved ciliary protein complexes in a series of targeted proteomics experiments, systematically mapping ciliary protein organization as a guide for research into ciliopathies and cilia function.",Proteomics of ciliopathy protein complexes,9470242,F31GM123683,"['Animals', 'Biochemical', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Candidate Disease Gene', 'Cilia', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collection', 'Comparative Biology', 'Complex', 'Congenital Abnormality', 'Core Protein', 'Data', 'Data Set', 'Databases', 'Defect', 'Development', 'Disease', 'Embryo', 'Epithelium', 'Event', 'Eye', 'Fractionation', 'Future', 'Genes', 'Genetic screening method', 'Hand', 'Health', 'Hereditary Disease', 'Human', 'Human Genetics', 'Investigation', 'Joubert syndrome', 'Kidney', 'Knock-out', 'Knockout Mice', 'Link', 'Machine Learning', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Methods', 'Modeling', 'Molecular Biology', 'Mus', 'Mutation', 'Noise', 'Organism', 'Outcome', 'Pattern', 'Phenotype', 'Plant Proteins', 'Plants', 'Process', 'Protein-Protein Interaction Map', 'Proteins', 'Proteomics', 'Research', 'Resources', 'Route', 'Sampling', 'Series', 'Signal Transduction', 'Syndrome', 'System', 'Techniques', 'Testing', 'Whole Organism', 'Xenopus', 'Xenopus laevis', 'base', 'biological systems', 'ciliopathy', 'cilium biogenesis', 'comparative', 'exome', 'experimental study', 'heart function', 'in vivo', 'link protein', 'mental function', 'novel', 'protein complex', 'protein protein interaction', 'reproductive function', 'skeletal']",NIGMS,"UNIVERSITY OF TEXAS, AUSTIN",F31,2017,34644,91740242,0.02892797837072897
"Mobility Data Integration to Insight     DESCRIPTION (provided by applicant): Mobility is essential for human health. Regular physical activity helps prevent heart disease and stroke, relieves symptoms of depression, and promotes weight loss. Unfortunately, many conditions, such as cerebral palsy, osteoarthritis, and obesity, limit mobility at an enormous personal and societal cost. While vast amounts of data are available from hundreds of research labs and millions of smartphones, there is a dearth of methods for analyzing this massive, heterogeneous dataset.  We propose to establish the National Center for Mobility Data Integration to Insight (the Mobilize Center) to overcome the data science challenges facing mobility big data and biomedical big data in general. Our preliminary work identified four bottlenecks in data science, which drive four Data Science Research Cores.  The Cores include Biomechanical Modeling, Statistical Learning, Behavioral and Social Modeling, and Integrative Modeling and Prediction. Our Cores will produce novel methods to integrate diverse modeling modalities and gain insight from noisy, sparse, heterogeneous, and time-varying big data. Our data-sharing consortia, with clinical, research, and industry partners, will provide mobility data for over ten million people.  Three Driving Biomedical Problems will focus and validate our data science research.  The Mobilize Center will disseminate our novel data science tools to thousands of researchers and create a sustainable data-sharing consortium. We will train tens of thousands of scientists to use data science methods in biomedicine through our in-person and online educational programs. We will establish a cohesive, vibrant, and sustainable National Center through the leadership of an experienced executive team and will help unify the BD2K consortia through our Biomedical Computation Review publication and the Simtk.org resource portal.  The Mobilize Center will lay the groundwork for the next generation of data science systems and revolutionize diagnosis and treatment for millions of people affected by limited mobility.         PUBLIC HEALTH RELEVANCE:  Regular physical activity is essential for human health, yet a broad range of conditions impair mobility. This project will transform human movement research by developing tools for data analysis and creating software that will advance research to prevent, diagnose, and reduce impairments that limit human movement.            ",Mobility Data Integration to Insight,9333122,U54EB020405,"['Accelerometer', 'Affect', 'Area', 'Automobile Driving', 'Behavioral', 'Behavioral Model', 'Big Data', 'Big Data to Knowledge', 'Biomechanics', 'Biomedical Computing', 'Biomedical Research', 'Body Weight decreased', 'Cellular Phone', 'Cerebral Palsy', 'Child', 'Classification', 'Clinical Research', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Data Sources', 'Degenerative polyarthritis', 'Diabetes Mellitus', 'Diagnosis', 'Educational workshop', 'Elderly', 'Ethics', 'Exercise', 'Fellowship', 'Fostering', 'Gait', 'Health', 'Heart Diseases', 'Human', 'Impairment', 'Individual', 'Injury', 'Joints', 'Leadership', 'Limb structure', 'Machine Learning', 'Medical center', 'Methods', 'Mission', 'Modality', 'Modeling', 'Movement', 'NCI Scholars Program', 'Nature', 'Obesity', 'Operative Surgical Procedures', 'Overweight', 'Pathology', 'Persons', 'Physical activity', 'Prevention', 'Problem Solving', 'Public Health', 'Publications', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Running', 'Scientist', 'Stroke', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Walking', 'Work', 'base', 'biomechanical model', 'clinical decision-making', 'cognitive function', 'cohesion', 'cost', 'data integration', 'data modeling', 'data sharing', 'depressive symptoms', 'experience', 'flexibility', 'health data', 'improved', 'improved outcome', 'industry partner', 'insight', 'massive open online courses', 'motor impairment', 'next generation', 'novel', 'novel strategies', 'online resource', 'prevent', 'programs', 'public health relevance', 'reduce symptoms', 'role model', 'sensor', 'social', 'social model', 'tool', 'visiting scholar']",NIBIB,STANFORD UNIVERSITY,U54,2017,68981,560644462,0.010724549121839193
"Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions Project Summary/Abstract People with mobility restrictions and limited to no upper extremity use depend on others to position the objects that they rely upon for health, communication, productivity, and general well-being. The technology developed in this project directly increases users’ independence by responding to them without another person’s intervention. The independence resulting from the proposed technology allows a user to perform activities related to self-care and communication. Eye tracking and head tracking access to speech devices, tablets, or computers requires very specific positioning. If the user’s position relative to the device are not aligned precisely, within a narrow operating window, the device will no longer detect the eyes or head, rendering the device inoperable. Auto-positioning technology uses computer vision and robotic positioning to automatically move devices to a usable position. The system moves without assistance from others, ensuring the user has continual access to communication. In a generalized application, the technology can target any area of the user’s body to position relevant equipment, such as for hydration or a head array for power wheelchair control. Research and development of the automatic positioning product will be accomplished through user-centered, iterative design, and design for manufacturing. Input from people with disabilities, their family members, therapists, and assistive technology professionals define the functional requirements of the technology and guide its evolution. Throughout technical development, prototype iterations are demonstrated and user-tested, providing feedback to advance the design. Design for manufacturability influences the outcomes to optimize the product pipeline, ensure high quality and deliver a cost effective product. Phase 1 Aims demonstrate the feasibility of 1) movement and control algorithms 2) face tracking algorithms and logic to maintain device positioning and 3) integration with commercial eye tracking device software development kits (SDK). Phase 2 Aims include technical, usability, and manufacturability objectives leading to development of a product for commercialization. Software development advances computer vision capabilities to recognize facial expressions and gestures. A new sensor module serves as a gesture switch or face tracker. App development provides the user interface, remote monitoring and technical support. Design of scaled down actuators and an enclosed three degree of freedom alignment module reduces the form factor, allowing for smaller footprint applications. Rigorous user testing by people with different diagnoses and end uses will ensure the product addresses a range of needs and is easy to use. Testing involves professionals and family members to evaluate ease of set-up, functionality, and customization. Extended user testing will investigate and measure outcomes and effects on their independence, access and health. Prototype tooling and design for cost-effective manufacturing will produce units for user and regulatory testing, and eventual sale. Project Narrative People who are essentially quadriplegic, with significant mobility impairments and limited to no upper extremity use, are dependent on others to maintain proper positioning for access to devices essential for their health, mobility and communication, such as eye gaze speech devices, call systems, phones, tablets, wheelchair controls, and hydration and suctioning tubes. Auto-positioning technology uses computer vision to detect specific targets, such as facial features, to control a robotic mounting and positioning system; automatically repositioning devices for best access without assistance from others, even when their position changes. This technology enables continuous access to communication, maintains one’s ability to drive a wheelchair and allows a person to drink or suction themselves, providing good hydration, respiratory health and hygiene.",Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions,9407137,R44HD093467,"['Address', 'Advanced Development', 'Algorithms', 'Area', 'Articular Range of Motion', 'Beds', 'Caring', 'Cerebral Palsy', 'Communication', 'Computer Vision Systems', 'Computers', 'Custom', 'Data', 'Dependence', 'Development', 'Devices', 'Diagnosis', 'Disabled Persons', 'Duchenne muscular dystrophy', 'Ensure', 'Equipment', 'Evolution', 'Eye', 'Face', 'Facial Expression', 'Family', 'Family member', 'Feedback', 'Freedom', 'Gestures', 'Goals', 'Head', 'Head and neck structure', 'Health', 'Health Communication', 'Hydration status', 'Hygiene', 'Immunity', 'Impairment', 'Individual', 'Intervention', 'Intuition', 'Joints', 'Lighting', 'Logic', 'Methods', 'Monitor', 'Motor', 'Movement', 'Oral cavity', 'Outcome', 'Outcome Measure', 'Persons', 'Phase', 'Phonation', 'Positioning Attribute', 'Powered wheelchair', 'Productivity', 'Publishing', 'Quadriplegia', 'Quality of life', 'Research Design', 'Robotics', 'Safety', 'Sales', 'Saliva', 'Self Care', 'Self-Help Devices', 'Services', 'Signal Transduction', 'Spastic Tetraplegia', 'Speech', 'Spinal Muscular Atrophy', 'Spinal cord injury', 'Suction', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Tube', 'Update', 'Upper Extremity', 'Variant', 'Wheelchairs', 'Work', 'application programming interface', 'base', 'body system', 'commercialization', 'communication device', 'cost', 'cost effective', 'cost effectiveness', 'design', 'experience', 'gaze', 'improved', 'iterative design', 'meetings', 'product development', 'prototype', 'psychosocial', 'research and development', 'respiratory health', 'sensor', 'software development', 'tool', 'usability', 'user centered design']",NICHD,"BLUE SKY DESIGNS, INC.",R44,2017,159267,0,-0.035706585597099924
"Classifying addictions using machine learning analysis of multidimensional data ABSTRACT This Independent Scientist Award will significantly enhance my research capabilities, enabling me to become a leading quantitative investigator in the field of substance use disorders (SUDs). Specifically, it will allow me to increase my knowledge in the areas of SUD phenotypes, treatment and genetics. SUDs are clinically and etiologically heterogeneous and their classification has been difficult. This application reflects my ongoing commitment to developing an innovative and interdisciplinary research program on the classification of SUDs through quantitative analysis of multidimensional data. My extensive training in computational science and prior research on biomedical informatics have provided me with the skills to design, implement and evaluate advanced algorithms and sophisticated analyses to solve challenging problems in classifying SUDs. My ongoing NIDA-funded R01 employs a large (n=~12,000) sample aggregated from multiple genetic studies of cocaine, opioid, and alcohol dependence to develop and evaluate novel statistical models to generate clinical SUD subtypes that are optimized for gene finding. This K02 proposal extends that work to evaluate treatment outcome in refined subgroups of SUD populations using data from treatment studies for cocaine, opioid, alcohol and multiple substance dependence. This project will integrate data from diagnostic behavioral variables and genotypes, as well as biological/neurobiological features of the disorders and repeated measures of treatment outcome. The primary career development goals of this application are to: (1) understand the reliability, validity and functional mechanisms of various phenotyping methods; (2) to continue training in the genetics of addictions; and (3) to gain greater knowledge of different treatment approaches and their efficacy. A solid foundation in these areas will enhance my ability to realize the full potential of the data collected and aggregated from multiple dimensions, and to use the data to design the most clinically useful analysis and generate innovative solutions to diagnostic and predictive challenges in SUD research. Through formal coursework, directed readings, individual tutoring and intensive multidisciplinary collaboration with a diverse team of world-renowned researchers, I will receive training and collect pilot data for future R01 projects by examining (Aim I): whether clinically-defined highly heritable subtypes derived in my current R01 project predict differential treatment response; (Aim II) whether new statistical models that directly combine treatment data with behavioral, biological, and genomic data identify refined subtypes with confirmatory multilevel evidence; and (Aim III) whether there are genetic and social moderators of treatment outcome by subtype. The overall goal of this proposal is to further my independent and multidisciplinary research program in the development of statistical methods for refined classification of SUDs. The K02 award will provide me with the protected time necessary to fully engage in the training activities described that will enhance my knowledge and skills to enable me to make important, novel contributions to the genetics and treatment of SUD. PROJECT NARRATIVE This project will develop novel statistical and quantitative tools to identify homogeneous subtypes of substance use disorders (SUDs) and other complex diseases to enhance gene finding and treatment matching. The proposed project will perform secondary analyses of existing data from treatment studies of cocaine, opioid, alcohol, and mixed SUDs. The proposed novel approaches are expected to advance precision medicine approaches to SUDs by enabling treatment matching and a more refined SUD classification to gene finding.",Classifying addictions using machine learning analysis of multidimensional data,9224405,K02DA043063,"['Adherence', 'Aftercare', 'Alcohol dependence', 'Alcohols', 'Algorithms', 'Area', 'Behavioral', 'Biological', 'Biological Markers', 'Biosensor', 'Characteristics', 'Classification', 'Clinical', 'Cluster Analysis', 'Cocaine', 'Cocaine Dependence', 'Collaborations', 'Combined Modality Therapy', 'Complex', 'Computational Science', 'DSM-IV', 'DSM-V', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic and Statistical Manual of Mental Disorders', 'Dimensions', 'Disease', 'Drug Use Disorder', 'Electroencephalography', 'Etiology', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic study', 'Genomics', 'Genotype', 'Goals', 'Heritability', 'Heterogeneity', 'Independent Scientist Award', 'Individual', 'Interdisciplinary Study', 'Investigation', 'Joints', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Neurobiology', 'Opiate Addiction', 'Opioid', 'Patients', 'Pattern', 'Pharmacogenetics', 'Pharmacotherapy', 'Phenotype', 'Population', 'Reading', 'Recording of previous events', 'Recruitment Activity', 'Research', 'Research Personnel', 'Risk Factors', 'Sampling', 'Scientist', 'Signs and Symptoms', 'Solid', 'Statistical Methods', 'Statistical Models', 'Subgroup', 'Substance Addiction', 'Substance Use Disorder', 'Surveys', 'Symptoms', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Work', 'addiction', 'alcohol use disorder', 'biomedical informatics', 'career development', 'cocaine use', 'contingency management', 'design', 'disease classification', 'disorder subtype', 'endophenotype', 'genetic association', 'genomic data', 'imaging genetics', 'improved', 'innovation', 'neural correlate', 'novel', 'novel strategies', 'opioid use disorder', 'outcome prediction', 'personalized medicine', 'precision medicine', 'programs', 'secondary analysis', 'skills', 'social', 'tool', 'treatment planning', 'treatment response', 'tutoring']",NIDA,UNIVERSITY OF CONNECTICUT STORRS,K02,2017,163452,36067938,-0.027878295650114515
"Towards automated phenotyping in epilepsy Over 5 million children and adults in the United States have had a diagnosis of epilepsy or a seizure disorder. However, treatment options for the epilepsies remain inadequate, because many patients suffer from uncontrolled seizures and from the negative side effects of treatment. A major obstacle to the faster development of new anti-convulsant therapies is the fact that rigorous preclinical epilepsy research typically requires labor-intensive and expensive 24/7 video-EEG monitoring of seizures that rests on the subjective scoring of seizure phenotypes by human observers (as exemplified by the widely used Racine scale of behavioral seizures). We propose to test if it is possible to perform objective, inexpensive and automated phenotyping of mice in various mouse models of acquired and genetic epilepsies. The approach rests on the recent recognition that mouse behaviors are structured in stereotyped modules at sub-second timescales that are arranged according to specific rules. These characteristic behavioral modules, and the transitions between them, can be identified without observer bias by combined 3D imaging and machine learning (ML) -assisted analytic methods. We propose to adopt this novel ML-assisted 3D video analysis technology to epilepsy research, in order to test if it can be used to identify mice with chronic temporal lobe epilepsy (TLE) during inter-ictal and ictal periods in two distinct experimental TLE models, and under various experimental conditions. In addition, we will also test whether the approach is able to automatically detect not only the overtly epileptic mice in a genetic model of severe childhood epilepsy (homozygous voltage-gated sodium channel β-subunit SCN1B-/- knock-out mice), but also distinguish the seemingly normal, non-epileptic, SCN1B+/- heterozygous mice from the wild-type controls. We anticipate that these results will have a potentially transformative effect on the field by demonstrating the feasibility and power of automated, objective, user-independent, inexpensive analysis of acquired and genetic epilepsy phenotypes. There is an urgent need for new therapies for patients with uncontrolled epilepsy. The project will test if it is possible to objectively characterize epileptic phenotypes in mice using a breakthrough technology involving machine learning-assisted analysis of 3-dimensional video data of behavior. If successful, this innovative approach is expected to dramatically accelerate epilepsy research by enabling the objective, automated, inexpensive phenotyping of experimental animals to aid the testing of novel anticonvulsant therapies.",Towards automated phenotyping in epilepsy,9369284,R21NS102908,"['Adopted', 'Adult', 'Adverse effects', 'Animal Behavior', 'Animal Model', 'Animals', 'Anticonvulsants', 'Behavior', 'Behavioral', 'Characteristics', 'Child', 'Childhood', 'Chronic', 'Complex', 'Data', 'Development', 'Diagnosis', 'Electroencephalography', 'Epilepsy', 'Exhibits', 'Frequencies', 'Genetic', 'Genetic Models', 'Hippocampus (Brain)', 'Human', 'Human immunodeficiency virus test', 'Image', 'Knockout Mice', 'Machine Learning', 'Modeling', 'Monitor', 'Mus', 'Neurons', 'Observer Variation', 'Patients', 'Phenotype', 'Pilocarpine', 'Probability', 'Recurrence', 'Research', 'Rest', 'Seizures', 'Sodium Channel', 'Stereotyping', 'Structure', 'Syndrome', 'Technology', 'Temporal Lobe Epilepsy', 'Testing', 'Three-Dimensional Imaging', 'Three-dimensional analysis', 'Time', 'Translational Research', 'United States', 'Wild Type Mouse', 'analytical method', 'base', 'cost', 'evidence base', 'high throughput analysis', 'innovation', 'kainate', 'learning strategy', 'mouse model', 'novel', 'novel therapeutics', 'pre-clinical', 'voltage']",NINDS,STANFORD UNIVERSITY,R21,2017,197528,560644462,-0.0075140278497617214
"SWIFT-ActiveScreener: research and development of an intelligent web-based document screening system Project Summary More than 4,000 systematic reviews are performed each year in the fields of environmental health and evidence- based medicine, with each review requiring, on average, between six months to one year of effort to complete. In order to remain accurate, systematic reviews require regular updates after their initial publication, with most reviews out of date within five years. In the screening phase of systematic review, researchers use detailed inclusion/exclusion criteria to decide whether each article in a set of candidate citations is relevant to the research question under consideration. For each article considered, a researcher reads the title and abstract and evaluates its content with respect to the prespecified criteria. A typical review may require screening thousands or tens of thousands of articles in this manner. Under the assumption that it takes a skilled reviewer 30-90 seconds, on average, to screen a single abstract, dual-screening a set of 10,000 abstracts may require between 150 to 500 hours of labor. We have shown in previous work that automated machine learning methods for article prioritization can reduce by more than 50% the human effort required to screen articles for inclusion in a systematic review. Recently, we have further extended these methods and packaged them into a web-based, collaborative systematic review software application called SWIFT-Active Screener. Active Screener has been used successfully to reduce the effort required to screen articles for systematic reviews conducted at a variety of organizations including the National Institute of Environmental Health Science (NIEHS), the United States Environmental Protection Agency (EPA), the United States Department of Agriculture (USDA), The Endocrine Disruption Exchange (TEDX), and the Evidence Based Toxicology Collaboration (EBTC). These early adopters have provided us with an abundance of useful data and user feedback, and we have identified several areas where we can continue to improve our methods and software. Our goal for the current proposal is to conduct additional research and development to make significant improvements to SWIFT-Active Screener, including several innovations that will be necessary for commercial success. The research we propose encompasses three specific aims: (1) Investigate several improvements to statistical algorithms used for article prioritization and recall estimation. We will explore promising avenues for further improving the performance of our existing algorithms and address critical technical issues that limit the applicability of our current methods (Aim 1 – Improved Statistical Models). (2) Explore ways in which we can improve our models and methods to handle the scenario in which an existing systematic review is updated with new data several years after its initial publication (Aim 2 – New Methods for Systematic Review Updates). (3) Investigate several questions related to scaling the system to support hundreds to thousands of simultaneous screeners (Aim 3 - Software Engineering for Scalability, Usability and Full Text Extraction). Project Narrative Systematic review is a formal process used widely in evidence-based medicine and environmental health research to identify, assess, and integrate the primary scientific literature with the goal of answering a specific, targeted question in pursuit of the current scientific consensus. By conducting research and development to build a web-based, collaborative systematic review software application that uses machine learning to prioritize documents for screening, we will make an important contribution toward ongoing efforts to automate systematic review. These efforts will serve to make systematic reviews both more efficient to produce and less expensive to maintain, a result which will greatly accelerate the process by which scientific consensus is obtained in a variety of medical and health-related disciplines having great public significance.",SWIFT-ActiveScreener: research and development of an intelligent web-based document screening system,9467160,R43ES029001,"['Address', 'Algorithms', 'Area', 'Collaborations', 'Computer software', 'Consensus', 'Data', 'Discipline', 'Endocrine disruption', 'Environmental Health', 'Evidence Based Medicine', 'Exclusion Criteria', 'Feedback', 'Goals', 'Health', 'Hour', 'Human', 'Literature', 'Machine Learning', 'Medical', 'Methods', 'Modeling', 'National Institute of Environmental Health Sciences', 'Online Systems', 'Performance', 'Phase', 'Process', 'Publications', 'Research', 'Research Personnel', 'Software Engineering', 'Statistical Algorithm', 'Statistical Models', 'System', 'Text', 'Toxicology', 'United States Department of Agriculture', 'United States Environmental Protection Agency', 'Update', 'Work', 'evidence base', 'improved', 'innovation', 'learning strategy', 'research and development', 'screening', 'success', 'systematic review', 'usability']",NIEHS,"SCIOME, LLC",R43,2017,211900,2510992,0.003909810381642715
"Deep learning for representation of codes used for SEER-Medicare claims research ﻿    DESCRIPTION (provided by applicant):  We propose developing an algorithm and user-friendly software to better identify treatments using Medicare claims data. We will validate our approach using procedures listed in the Surveillance, Epidemiology, and End Results (SEER) database as a gold standard. In this way, we hope to better match procedures identified using Medicare claims data with SEER listed procedures.  The focus of this research is observational (i.e. non-randomized) data. Well-run randomized clinical trials can provide the best level of evidence of treatment effects. However, randomized trials in the United States have suffered from poor accrual for many interventions. Despite the fact that well-designed randomized clinical trials should be the gold standard, well-designed observational studies might be the only method of obtaining inferences concerning comparative effectiveness for some cancer interventions.  In cancer research, one of the most commonly used databases for observational research is the linked SEER-Medicare database. SEER-Medicare data has provided useful measurements of the effectiveness of a number of cancer therapies. Algorithms for identifying relevant treatment and diagnosis codes using Medicare data are often based on clinical reasoning and scientific evidence. One group of researchers, for example, developed an algorithm for identifying laparoscopic surgery among kidney cancer cases before claims codes for laparoscopic surgery were well developed. While such algorithms are useful for others pursuing similar investigations, there may still be substantial mismatch between treatment identified by the SEER cancer registry and treatment identified through Medicare claims. In this work, we propose developing a rigorous machine learning algorithm that can help researchers in better identifying treatments in Medicare claims data. Specifically, we will design a neural language modeling algorithm and implement a software system that finds vector representations of diagnosis and procedure codes.  We plan on using the neural language modeling algorithm to learn vector representations from SEER- Medicare claims data where related procedure and diagnosis codes are ""neighbors"" (i.e. closely related). We will investigate whether the codes we identify within neighborhoods correspond to the procedure codes used for published SEER-Medicare studies. We will then design a software assistant interface that will allow an investigator to explore which codes are related to a given seed of diagnosis or procedure codes. Finally, we will investigate the sensitivity and specificity of the algorithm by comparing procedures identified using Medicare claims with procedures listed in the SEER database. We will replicate analyses from a published SEER-Medicare paper to investigate if estimated treatment effects differ when using our novel algorithm compared to using the algorithm in the published paper. PUBLIC HEALTH RELEVANCE: In cancer research, one of the most commonly used databases for observational research is the linked Surveillance, Epidemiology, and End Results (SEER)-Medicare database. To improve the identification of procedures when using Medicare claims data, we will design a software assistant interface that will allow an investigator to explore which codes are related to a given seed of diagnosis or procedure codes. This should improve the identification of procedures when using Medicare claims data, and make conclusions drawn from analyses using the database more reliable and consistent.",Deep learning for representation of codes used for SEER-Medicare claims research,9188540,R21CA202130,"['Algorithms', 'Cancer Intervention', 'Clinical', 'Code', 'Computer software', 'Data', 'Databases', 'Diagnosis', 'Effectiveness', 'Ethical Issues', 'Funding', 'Future', 'Gold', 'International Classification of Diseases', 'Intervention', 'Investigation', 'Investigational Therapies', 'Language', 'Laparoscopic Surgical Procedures', 'Learning', 'Level of Evidence', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Medical', 'Medical Records', 'Medicare', 'Medicare claim', 'Methods', 'Modeling', 'Natural Language Processing', 'Neighborhoods', 'Observational Study', 'Outcome', 'Paper', 'Patients', 'Procedures', 'Process', 'Proxy', 'Publishing', 'Randomized Clinical Trials', 'Records', 'Renal carcinoma', 'Research', 'Research Personnel', 'Running', 'SEER Program', 'Seeds', 'Sensitivity and Specificity', 'Software Tools', 'Statistical Study', 'Terminology', 'Testing', 'Time', 'United States', 'Update', 'Work', 'anticancer research', 'base', 'cancer therapy', 'comparative effectiveness', 'design', 'health disparity', 'improved', 'interest', 'malignant breast neoplasm', 'neoplasm registry', 'novel', 'public health relevance', 'randomized trial', 'relating to nervous system', 'software systems', 'treatment effect', 'usability', 'user friendly software', 'vector', 'volunteer']",NCI,RESEARCH INST OF FOX CHASE CAN CTR,R21,2017,219753,29585825,-0.02901089096380602
"Molecular impact of mutations in monogenic disease and cancer ABSTRACT  Next generation genome scale sequencing of patients is now becoming routine for two classes of disease: rare  Mendelian traits and cancer. In favorable cases, these data allow identification of relevant mutations and thus  aid diagnosis and therapy. In both classes of disease, the most common type of mutation is missense -­ single  base  changes  that  result  in  an  amino  acid  substitution  in  a  protein.  Uncertainty  as  to  the  impact  of  these  mutations on in vivo protein activity has resulted in a very conservative approach to their interpretation in the  clinic,  so  causing  many  missed  opportunities  for  targeted  treatment.  The  goal  of  this  project  is  to  use  a  combination of three strategies to make the interpretation of these mutations much more applicable in the clinic.  There are already a large number of computational methods that attempt to determine the impact of missense  mutations on function, and there is substantial evidence that these have useful accuracy. The primary difficulty  is that the accuracy in any particular case is not reliably calibrated. Therefore, our first aim is to use a combination  of these methods to develop an approach focused on more reliable estimates for the probability of high impact  on  protein  function  (i.e.  more  confident  P  values).  The  second  aim  is  to  maximize  the  utilization  of  three-­ dimensional structural information, largely ignored by most computational methods. A large fraction of missense  mutations in these classes of disease act by destabilizing protein structure and knowledge of structure allows  these to be identified with much higher reliability. Also, structure provides a framework for detailed annotation  and comprehension of function. To facilitate the utilization of structure, we will implement a modeling platform  that leverages available experimental information to maximize the structural data available for analyzing mutation  impact.  An  important  aspect  of  the  platform  is  incorporation  of  methods  for  evaluating  the  reliability  of  the  structural features relevant to analysis of each mutation. In the third aim we will build specific functional models  for each protein of interest, integrating information from current databases, the literature, and community input,  so as to provide the richest possible background against which to judge the impact of mutations. Proteopedia, a  well established media wiki for proteins, will be used to provide an integrated view of text, data, and structure. A  key component of the information resource will be contributions from curators, who will provide annotation and  also solicit input from other experts. This aspect of the project builds on experience with other crowdsourcing  endeavors,  including  CASP,  CAGI  and  Proteopedia.    There  will  be  three  primary  outcomes  from  the  project:  First, improved reliability for the interpretation of missense mutations. Second, a prototype mutation annotation  procedure suitable for use in a clinical setting. Third, the resource will provide information of benefit to a range  of other scientists, thus facilitating the analysis of disease related mutations.      NARRATIVE  Genome  scale  DNA  sequencing  is  now  contributing  to  diagnosis  and  therapy  in  cases  of  rare  human  disease and cancer.  Full exploitation of these data is currently hampered by inadequate understanding  of which DNA changes affect protein function so as to contribute to disease. This project aims to develop  the methods and tools needed to remove that obstacle. ",Molecular impact of mutations in monogenic disease and cancer,9356560,R01GM120364,"['AIDS diagnosis', 'Address', 'Affect', 'Amino Acid Substitution', 'Clinic', 'Clinical', 'Communities', 'Comprehension', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Consensus', 'DNA', 'DNA sequencing', 'Data', 'Databases', 'Diagnosis', 'Dimensions', 'Disease', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mendelian disorder', 'Methods', 'Missense Mutation', 'Modeling', 'Molecular', 'Mutation', 'Mutation Analysis', 'Patients', 'Play', 'Probability', 'Procedures', 'Process', 'Proteins', 'Rare Diseases', 'Reporting', 'Resources', 'Role', 'Scientist', 'Structural Models', 'Structure', 'Tertiary Protein Structure', 'Text', 'Uncertainty', 'base', 'clinically relevant', 'crowdsourcing', 'experience', 'genome-wide', 'human disease', 'improved', 'in vivo', 'interest', 'learning strategy', 'next generation', 'primary outcome', 'protein protein interaction', 'protein structure', 'prototype', 'targeted treatment', 'tool', 'trait', 'wiki']",NIGMS,"UNIV OF MARYLAND, COLLEGE PARK",R01,2017,343120,58406802,0.006014092250400296
"Advanced Computational Approaches for NMR Data-mining ABSTRACT Nuclear magnetic resonance spectroscopy (NMR)-based metabolomics is a powerful method for identifying metabolic perturbations that report on different biological states and sample types. Compared to mass spectrometry, NMR provides robust and highly reproducible quantitative data in a matter of minutes, which makes it very suitable for first-line clinical diagnostics. Although the metabolome is known to provide an instantaneous snap-shot of the biological status of a cell, tissue, and organism, the utilization of NMR in clinical practice is hindered by cumbersome data analysis. Major challenges include high-dimensionality of the data, overlapping signals, variability of resonance frequencies (chemical shift), non-ideal shapes of signals, and low signal-to-noise ratio (SNR) for low concentration metabolites. Existing approaches fail to address these challenges and sample analysis is time-consuming, manually done, and requires considerable knowledge of NMR spectroscopy. Recent developments in the field of sparse methods for machine learning and accelerated convex optimization for high dimensional problems, as well as kernel-based spatial clustering show promise at enabling us to overcome these challenges and achieve fully automated, operator-independent analysis. We are developing two novel, powerful, and automated algorithms that capitalize on these recent developments in machine learning. In Aim 1, we describe ‘NMRQuant’ for automated identification and quantification of annotated metabolites irrespective of the chemical shift, low SNR, and signal shape variability. In Aim 2, we describe ‘SPA-STOCSY’ for automated de-novo identification of molecular fragments of unknown, non- annotated metabolites. Based on substantial preliminary data, we propose to evaluate these algorithms' sensitivity, specificity, stability, and resistance to noise on phantom, biological, and clinical samples, comparing them to current methods. We will validate the accuracy of analyses by experimental 2D NMR, spike-in, and mass spectrometry. The proposed efforts will produce new NMR analytical software for discovery of both annotated and non-annotated metabolites, substantially improving accuracy and reproducibility of NMR analysis. Such analytical ability would change the existing paradigm of NMR-based metabolomics and provide an even stronger complement to current mass spectrometry-based methods. This approach, once thoroughly validated, will enable NMR to reach wide network of applications in biomedical, pharmaceutical, and nutritional research and clinical medicine. NARRATIVE This project seeks to develop an advanced and automated platform for identifying NMR metabolomics biomarkers of diseases and for fundamental studies of biological systems. When fully developed, these approaches could be used to detect small molecules in the blood or urine, indicative of the onset of various diseases, drug toxicity, or environmental effects on the organism.",Advanced Computational Approaches for NMR Data-mining,9260548,R01GM120033,"['Address', 'Algorithms', 'Alpha Cell', 'Animal Disease Models', 'Biological', 'Biological Markers', 'Blood', 'Cancer Etiology', 'Cardiovascular system', 'Chemicals', 'Clinic', 'Clinical', 'Clinical Medicine', 'Complement', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Drug toxicity', 'Early Diagnosis', 'Frequencies', 'Health', 'Human', 'Knowledge', 'Left', 'Libraries', 'Link', 'Machine Learning', 'Manuals', 'Mass Spectrum Analysis', 'Measures', 'Medical', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'NMR Spectroscopy', 'Nature', 'Noise', 'Nuclear Magnetic Resonance', 'Nutritional', 'Obesity', 'Organism', 'Outcome', 'Patients', 'Pharmacologic Substance', 'Phenotype', 'Plague', 'Process', 'Regulation', 'Relaxation', 'Reporting', 'Reproducibility', 'Research', 'Residual state', 'Resistance', 'Sampling', 'Sensitivity and Specificity', 'Shapes', 'Signal Transduction', 'Societies', 'Sodium Chloride', 'Spectrum Analysis', 'Statistical Algorithm', 'Temperature', 'Time', 'Tissues', 'Treatment outcome', 'Urine', 'Variant', 'base', 'biological systems', 'biomarker discovery', 'clinical diagnostics', 'clinical practice', 'data mining', 'experimental analysis', 'experimental study', 'high dimensionality', 'improved', 'infancy', 'metabolome', 'metabolomics', 'novel', 'personalized medicine', 'phenotypic biomarker', 'small molecule', 'stem']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2017,356625,323604360,0.01289135021324345
"A platform for mining, visualization and design of microbial interaction networks Project Summary One of the burning questions in the study of the human microbiome is whether and how it is possible to design specific strategies for rebalancing the taxonomic and functional properties of human-associated microbial communities, triggering the transition from “disease states” to “healthy states”. While empirical studies provide strong support for the idea that we may be able to cure, or at least  treat, a number of diseases by simply transplanting microbiomes, or inducing changes through taxonomic or environmental perturbations, to date little mechanistic understanding exists on how microbial communities work, and on how to extend microbiome research from an empirical science to a systematic, quantitative field of biomedicine. We propose here to establish a computational platform--   a database (Aim 1) with fully integrated analytical software (Aims 2 and 3) --- developed for and with the cooperation of the scientific community. The resource goes beyond cataloguing microbial abundances under different condition; its aim is to enable an understanding of networks of interacting species and their condition-dependence, with the goal of eventually facilitating disease diagnosis and prognosis, and designing therapeutic strategies for microbiome intervention. Our project is centered around three key aims: 1.	The creation of a Microbial Interaction Network Database (MIND), a public resource that will collect data on inter-species interactions from metagenomic sequencing projects, computer simulations and direct experiments. This database will be accessed through a web-based platform complemented with tools for microbial interaction network analysis and visualization, akin to highly fruitful tools previously developed for the study of genetic networks; the database will also serve as the public repository of microbial networks associated with human diseases; 2.	The implementation of an integrated tool for simulation of interspecies interactions under different environments, based on genomic data and whole-cell models of metabolism; 3.	The implementation of new algorithms for microbial community analysis and engineering. These algorithms, including stoichiometric, machine-learning and statistical approaches will facilitate a “synthetic ecology” approach to help design strategies (e.g. microbial transplants or probiotic mixtures) for preventing and targeting microbiome-associated diseases. Our work will fill a major gap in current microbiome research, creating the first platform for global microbial interaction data integration, mining and computation. Project Narrative Among the major developments of the genomic revolution has been the ability to identify thousands of microbial species and strains living in communities in 5 major habitats in the human body, and the recognition that the relative abundances of these populations is strongly correlated with environment: disease state, diet, treatment protocol and so on. A major challenge in utilizing the deluge of health relevant data is structuring it into a database that facilitates understanding inter-microbial interactions in these communities. The aim of this proposal is to create a database and integrated computational platform, open to and contributed to by the research community, which will greatly accelerate the conversion of data into health related actionable knowledge.","A platform for mining, visualization and design of microbial interaction networks",9221662,R01GM121950,"['Affect', 'Algorithms', 'Cataloging', 'Catalogs', 'Cell model', 'Clinical', 'Communities', 'Complement', 'Complex', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Dependence', 'Development', 'Diet', 'Discipline', 'Disease', 'Ecology', 'Ecosystem', 'Empirical Research', 'Engineering', 'Environment', 'Evolution', 'Future', 'Genetic', 'Genetic study', 'Genome', 'Genomics', 'Goals', 'Habitats', 'Health', 'Human', 'Human Biology', 'Human Microbiome', 'Human body', 'Imagery', 'Individual', 'Intervention', 'Knowledge', 'Laboratories', 'Machine Learning', 'Measurable', 'Mediating', 'Metabolic', 'Metabolism', 'Metadata', 'Methods', 'Microbe', 'Mining', 'Nature', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pattern', 'Population', 'Preventive Medicine', 'Probiotics', 'Property', 'Research', 'Resources', 'Science', 'Scientist', 'Structure', 'Taxonomy', 'Technology', 'Therapeutic', 'Time', 'Transplantation', 'Treatment Protocols', 'Work', 'base', 'computer framework', 'data integration', 'data to knowledge', 'design', 'disease diagnosis', 'experimental study', 'feeding', 'genome-wide', 'genomic data', 'human disease', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microbiota transplantation', 'microorganism interaction', 'novel diagnostics', 'novel therapeutics', 'open source', 'outcome forecast', 'prevent', 'repository', 'simulation', 'tool', 'user-friendly']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2017,377226,61050884,0.006708013419382738
"Towards a comprehensive multiscale 3D human interactome network PROJECT SUMMARY/ABSTRACT Almost all proteins function through interacting with other proteins. On average, a protein interacts with ~5 other protein partners in the current human interactome. Therefore, it is of great importance to accurately determine the interface of each interaction, in order to understand how each protein works with different partners to carry out different functions. In our previous Nature Biotechnology study, we implemented a proteome-scale homology modeling approach to generate the first 3D human structural interactome: the interface for each interaction in this network was determined at atomic resolution through co-crystal structures and homology models. Using our 3D interactome, we found that, among >1,800 known disease genes associated with two or more clinically distinctly disorders, pairs of mutations on the same gene but in different interfaces with different partners are significantly more likely to cause distinct diseases. However, only 4,150 human protein interactions have co-crystal structures and 2,921 have high-quality homology models. ~50,000 interactions (87% of the current human interactome) are not amenable to current structural modeling methods. Here, we propose to develop a big-data-driven machine-learning approach integrating biophysiochemical, evolutionary, structural, and population genetic features to identify interaction- specific interfaces for the whole human interactome. Because several key features are unavailable for many proteins and interactions, we propose an innovative approach to use an ensemble of random forest classifiers, named Ensemble Protein Interface Classifier (EPIC), to address this large-scale non-random missing data problem (Aim 1). The high throughput of our massively parallel Clone-seq and INtegrated PrOtein INteractome perTurbation screening (InPOINT) pipeline! uniquely enables us to perform real-time experimental parameter optimization (in Years 2-4 we will clone ~1,500 mutations and examine their impact on ~2,500 interactions every year to iteratively evaluate and refine EPIC; Aim 2). Finally, we will construct a comprehensive multiscale 3D interactome for all known human protein-protein interactions: we will collect/generate atomic- resolution structural models for interactions whenever possible (co-crystal structures and homology models); we will accurately determine interaction-specific interface residues and domains for the whole human interactome. We will deploy an interactive web portal to disseminate our results and allow functional genomic inference in the context of our structural interactome (Aim 3). Our comprehensive multiscale 3D human interactome and the accompanying web portal will greatly reduce the barrier-to-entry for performing systematic structural analysis on a large number of proteins and their interactions, and open the flood gates for such analyses in genomic studies. NARRATIVE Almost all proteins function through interacting with other proteins and the structural details of these interaction interfaces are key in understanding protein function. However, the interfaces for vast majority of human protein interactions are currently unknown. Here, we propose to establish an innovative ensemble classifier approach and implement an unprecedented large-scale computational-experimental iterative learning scheme to predict interfaces for the whole human interactome, in anticipation that our predicted interfaces will help dissect functional sites of disease mutations and be useful for rational drug design to target these sites.",Towards a comprehensive multiscale 3D human interactome network,9375803,R01GM124559,"['Address', 'Big Data', 'Binding', 'Biotechnology', 'Cells', 'Clinical', 'Crystallization', 'Data', 'Disease', 'Docking', 'Drug Design', 'Evolution', 'Floods', 'Genes', 'Genomics', 'Goals', 'Graph', 'Homology Modeling', 'Human', 'Individual', 'Learning', 'Letters', 'Machine Learning', 'Methods', 'Missense Mutation', 'Modeling', 'Molecular', 'Mutation', 'Names', 'Nature', 'Population Genetics', 'Proteins', 'Proteome', 'Resolution', 'Resources', 'Scheme', 'Site', 'Structural Models', 'Structure', 'Study models', 'Testing', 'Time', 'Variant', 'Work', 'biological research', 'forest', 'functional genomics', 'improved', 'innovation', 'learning strategy', 'protein function', 'protein protein interaction', 'screening', 'user-friendly', 'web portal', 'web-accessible']",NIGMS,CORNELL UNIVERSITY,R01,2017,379776,91477866,0.0197534589109463
"Data-driven analysis of protein structure, function, and regulation PROJECT SUMMARY Proteins are capable of performing a wide variety of complex molecular functions and play a central role in all biological processes. A detailed and quantitative understanding of the relationship between a protein's sequence and it's biochemical properties would have a profound impact across all areas of biology, medicine, and biotechnology. We are developing data-driven approaches for dissecting the molecular basis of protein function. Our general framework involves designing informative libraries of protein sequences, experimentally mapping the relationship between sequence and function, and extracting detailed functional information from large sequence-function data sets. This work leverages emerging technologies and methods in DNA sequencing and synthesis, microfluidic screening, large-scale statistical learning, and optimization. We will develop generalizable platforms that can be applied to study a wide variety of enzymes and membrane transport proteins. PROJECT NARRATIVE A detailed and quantitative understanding of the relationship between a protein's sequence and it's biochemical properties would have a profound impact across all areas of biology, medicine, and biotechnology. This important capability would allow us to design new therapeutic and diagnostic proteins, and diagnose genetic diseases before they manifest symptoms. The goal of this proposal is to develop a general framework for dissecting the molecular basis of protein structure, function, and regulation.","Data-driven analysis of protein structure, function, and regulation",9318542,R35GM119854,"['Amino Acid Sequence', 'Area', 'Biochemical', 'Biological Process', 'Biology', 'Biotechnology', 'Complex', 'DNA biosynthesis', 'DNA sequencing', 'Data', 'Data Set', 'Emerging Technologies', 'Enzymes', 'Goals', 'Hereditary Disease', 'Libraries', 'Machine Learning', 'Medicine', 'Membrane Transport Proteins', 'Methods', 'Microfluidics', 'Molecular', 'Play', 'Property', 'Protein Analysis', 'Proteins', 'Regulation', 'Role', 'Symptoms', 'Work', 'design', 'genetic disorder diagnosis', 'novel diagnostics', 'novel therapeutics', 'protein function', 'protein structure function', 'screening']",NIGMS,UNIVERSITY OF WISCONSIN-MADISON,R35,2017,382500,338121506,0.03237762551668807
"Defining the multi-dimensional code of zinc finger specificity-Resubmission-1 Project Summary The Cys2His2 zinc finger DNA-binding domain is the most common domain in human yet the DNA-binding specificities for the great majority of these proteins remain undefined. Mutations in many of these domains, both with and without known DNA-binding data, have been linked to a host of diseases from Alzheimers (REST) to Cancer (e.g. Slug, WT1, CTCF). Therefore, the characterization of these proteins holds great value. Unfortunately common methodologies used to determine the DNA-binding specificity of transcription factors have failed to address the zinc finger, at least in part because of an inability to fully define the large target specificities required of the average mammalian zinc finger protein. Even when ChIP-Seq data exists it is limited because the size of the genome does not allow us to capture the full binding potential of a factor that could offer a ≥21bp target sequence. As a result, without a comprehensive understanding of a protein’s binding potential, SNPs across the genome will continue to represent potential binding sites that we are unable to predict. In sum, decades of research have enlightened our understanding of this domain but we are still in the dark when it comes to its function as a transcription factors. Recently we have taken an alternative approach to define this domain, demonstrating that a synthetic, one-by-one screen of individual zinc fingers allows us to predict the specificity of multi-fingered proteins with similar or greater accuracy than all prior prediction algorithms. However, this approach fails to take into consideration the influences that adjacent fingers have on one another. We have produced the equivalent of a comprehensive snapshot of what a zinc finger is capable of in just one of many potential contextual environments. Here we propose to scale this approach and screen the zinc finger under an inclusive set of contextual environments. We will consider the most common direct and indirect influences on adjacent finger binding as well as factors that impact the geometry with which the zinc fingers engage the DNA. We will use these results to provide a complete picture of how adjacent zinc fingers determine their specificity and by scaffolding these two-fingered models, predict and design the specificity of large, multi-fingered proteins. In this way, we will define a multi-dimensional code of zinc finger specificity that allows us to predict all zinc finger DNA-binding specificities, how any neighbor finger context would modify this specificity, and the factors that result in adjacent finger incompatibility and loss of DNA-binding function. We will apply this model to predict the specificity of all human zinc finger proteins, validate these predictions through in vivo characterization of an informed set of transcription factors, and test predicted mechanisms of multi-fingered binding with designer, artificial factors. Project Narrative The proposed research is relevant to public health because the ZF domain is the most common in human yet it remains largely uncharacterized. A holistic understanding of ZF function will provide insight into how ZF mutations are related to disease and allow us to predict harmful binding sites due to SNPs across the genome.",Defining the multi-dimensional code of zinc finger specificity-Resubmission-1,9237919,R01GM118851,"['Achievement', 'Address', 'Alzheimer&apos', 's Disease', 'Amino Acids', 'Binding', 'Binding Proteins', 'Binding Sites', 'ChIP-seq', 'Charge', 'Code', 'Collection', 'Communities', 'Comprehension', 'DNA', 'DNA Binding', 'DNA Binding Domain', 'Darkness', 'Data', 'Disease', 'Distal', 'Environment', 'Fingers', 'Genetic Transcription', 'Genome', 'Geometry', 'Goals', 'Human', 'Hybrids', 'Individual', 'Lead', 'Libraries', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Mammals', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Nucleotides', 'Proteins', 'Public Health', 'Reporter', 'Research', 'Sampling', 'Schizophrenia', 'Series', 'Specificity', 'Structure', 'Sum', 'System', 'Systems Biology', 'Testing', 'WT1 gene', 'Work', 'Zinc Fingers', 'base', 'design', 'exhaustion', 'experimental study', 'in vivo', 'insight', 'loss of function', 'model design', 'prediction algorithm', 'predictive modeling', 'scaffold', 'screening', 'slug', 'transcription factor', 'user-friendly', 'web site']",NIGMS,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,R01,2017,410714,329565273,0.003535696619737962
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9403171,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Learning', 'Link', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2017,548068,685608202,0.006598087424920724
"Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine The overall goal of this project is to develop a safe, broadly effective, and affordable vaccine to prevent group A streptococcal infections. Antibodies against the N-terminal hypervariable region (HVR) of surface M (Emm) proteins of GAS are opsonic and are associated with protection against infection. Immunity has classically been described as “type-specific”, leading to the assumption that natural immunity confers protection against only one of the more than 200 different emm types of GAS. We now have new information that calls into question this classic view and serves as the basis for an entirely different approach to GAS vaccine design and development. A recent comprehensive sequence analysis of M proteins from a global collection of 175 emm types of GAS resulted in a new emm cluster typing system that classified 96.2% of all contemporary GAS isolates into 48 emm clusters containing structurally and functionally related M proteins. Moreover, 117 emm types contained in 16 clusters accounted for 94.4% of GAS infections in the world. Indeed, preclinical studies indicated that a multivalent vaccine containing N-terminal peptides from 30 prevalent M types cross-opsonized a significant number of non-vaccine emm types of GAS that co-localized in clusters with vaccine emm types. The frequency of cross-opsonic antibodies, combined with the emm cluster data, prompted us to conclude that there is a need for a paradigm shift away from the concept of “type-specific” immunity against GAS infections to one of “cluster-specific” immunity. Our overall hypothesis is that immunity to GAS infections is the result of both type-specific and cross-reactive antibodies against the N-terminal regions of M proteins and that a new approach employing computational predictions of peptide structures will result in a multivalent vaccine that will induce broadly protective immunity in populations throughout the world. Our preliminary results indicate the feasibility of using structure-based design to predict the antigenic relatedness of M peptides within a cluster. The specific aims of this proposal are to: 1) Apply computational structure-based design in an iterative process with immunological data from Aim 2 to predict the minimal number of M peptide sequences that are most representative of the structural and physicochemical properties of the peptides in one emm cluster containing 17 GAS emm types, 2) determine the cross-reactive immunogenicity of the selected peptides with all seventeen emm types of GAS in the cluster, and apply the results to refine the computational design predictions in Aim 1, 3) apply the refined computational parameters from Aims 1 and 2 to analyze the remaining epidemiologically important emm clusters, select a comprehensive panel of peptides representing all emm types, construct four multivalent recombinant vaccine proteins, and assess potential cross-protective immunogenicity using in vitro bactericidal assays against all 117 emm types of GAS, and 4) determine the protective immunogenicity of the final multivalent vaccine in unique transgenic mice expressing human C4BP and factor H that will be immunized and then challenged with multiple emm types of GAS. The world needs an effective, safe and affordable vaccine to prevent group A streptococcal (GAS) infections. Although most GAS infections are mild, there are more than 18 million people with a chronic complication of a severe GAS disease worldwide, over 15 million of whom have rheumatic heart disease, another 2 million cases of severe disease occur each year and a total of 517,000 deaths annually are estimated to be due to this organism. Vaccine prevention of even a fraction of these life-threatening diseases could have a significant impact on the health of people around the world.",Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine,9357870,R01AI132117,"['Animals', 'Antibodies', 'Bacteria', 'Base Sequence', 'Binding', 'Biological Assay', 'Cell surface', 'Cells', 'Cessation of life', 'Chronic', 'Collection', 'Complement Factor H', 'Complementarity Determining Regions', 'Complication', 'Computer Analysis', 'Data', 'Development', 'Disease', 'Ensure', 'Enzyme-Linked Immunosorbent Assay', 'Epidemiology', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Human', 'Immune', 'Immune Sera', 'Immunity', 'Immunize', 'Immunologics', 'In Vitro', 'Infection', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Mus', 'N-terminal', 'Natural Immunity', 'Organism', 'Oryctolagus cuniculus', 'Peptide Vaccines', 'Peptide antibodies', 'Peptides', 'Population', 'Prevention', 'Process', 'Property', 'Protein Hybridization', 'Proteins', 'Recombinant Vaccines', 'Recombinants', 'Rheumatic Heart Disease', 'Sequence Analysis', 'Streptococcal Infections', 'Streptococcal Vaccines', 'Structure', 'Surface', 'System', 'Testing', 'Transgenic Mice', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'bactericide', 'base', 'cross reactivity', 'design', 'experimental study', 'flexibility', 'hybrid protein', 'immunogenic', 'immunogenicity', 'innovation', 'molecular dynamics', 'multiple myeloma M Protein', 'novel', 'novel strategies', 'peptide structure', 'preclinical study', 'prevent', 'protein aminoacid sequence', 'protein structure', 'retinal S antigen peptide M', 'synthetic peptide', 'tool', 'vaccine development', 'vaccine evaluation']",NIAID,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2017,720717,46216755,0.013057258446658905
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9270103,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'education research', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2016,43143,338121506,0.013915772071026993
"Mobility Data Integration to Insight     DESCRIPTION (provided by applicant): Mobility is essential for human health. Regular physical activity helps prevent heart disease and stroke, relieves symptoms of depression, and promotes weight loss. Unfortunately, many conditions, such as cerebral palsy, osteoarthritis, and obesity, limit mobility at an enormous personal and societal cost. While vast amounts of data are available from hundreds of research labs and millions of smartphones, there is a dearth of methods for analyzing this massive, heterogeneous dataset.  We propose to establish the National Center for Mobility Data Integration to Insight (the Mobilize Center) to overcome the data science challenges facing mobility big data and biomedical big data in general. Our preliminary work identified four bottlenecks in data science, which drive four Data Science Research Cores.  The Cores include Biomechanical Modeling, Statistical Learning, Behavioral and Social Modeling, and Integrative Modeling and Prediction. Our Cores will produce novel methods to integrate diverse modeling modalities and gain insight from noisy, sparse, heterogeneous, and time-varying big data. Our data-sharing consortia, with clinical, research, and industry partners, will provide mobility data for over ten million people.  Three Driving Biomedical Problems will focus and validate our data science research.  The Mobilize Center will disseminate our novel data science tools to thousands of researchers and create a sustainable data-sharing consortium. We will train tens of thousands of scientists to use data science methods in biomedicine through our in-person and online educational programs. We will establish a cohesive, vibrant, and sustainable National Center through the leadership of an experienced executive team and will help unify the BD2K consortia through our Biomedical Computation Review publication and the Simtk.org resource portal.  The Mobilize Center will lay the groundwork for the next generation of data science systems and revolutionize diagnosis and treatment for millions of people affected by limited mobility.         PUBLIC HEALTH RELEVANCE:  Regular physical activity is essential for human health, yet a broad range of conditions impair mobility. This project will transform human movement research by developing tools for data analysis and creating software that will advance research to prevent, diagnose, and reduce impairments that limit human movement.            ",Mobility Data Integration to Insight,9103879,U54EB020405,"['Accelerometer', 'Affect', 'Area', 'Automobile Driving', 'Behavioral', 'Behavioral Model', 'Big Data', 'Big Data to Knowledge', 'Biomechanics', 'Biomedical Computing', 'Biomedical Research', 'Body Weight decreased', 'Cellular Phone', 'Cerebral Palsy', 'Child', 'Classification', 'Clinical Research', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Data Sources', 'Degenerative polyarthritis', 'Diabetes Mellitus', 'Diagnosis', 'Educational workshop', 'Elderly', 'Ethics', 'Exercise', 'Fellowship', 'Fostering', 'Gait', 'Health', 'Heart Diseases', 'Human', 'Impairment', 'Individual', 'Injury', 'Joints', 'Leadership', 'Limb structure', 'Machine Learning', 'Medical center', 'Mental Depression', 'Methods', 'Mission', 'Modality', 'Modeling', 'Movement', 'NCI Scholars Program', 'Nature', 'Obesity', 'Operative Surgical Procedures', 'Overweight', 'Pathology', 'Persons', 'Physical activity', 'Prevention', 'Problem Solving', 'Public Health', 'Publications', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Running', 'Scientist', 'Stroke', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Walking', 'Work', 'base', 'big biomedical data', 'biomechanical model', 'clinical decision-making', 'cognitive function', 'cohesion', 'cost', 'data integration', 'data modeling', 'data sharing', 'experience', 'flexibility', 'health data', 'improved', 'improved outcome', 'industry partner', 'insight', 'massive open online courses', 'models and simulation', 'motor impairment', 'next generation', 'novel', 'novel strategies', 'prevent', 'programs', 'public health relevance', 'reduce symptoms', 'role model', 'sensor', 'social', 'social model', 'tool', 'visiting scholar']",NIBIB,STANFORD UNIVERSITY,U54,2016,68981,560644462,0.010724549121839193
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,9056632,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'education research', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2016,73173,338121506,0.013915772071026993
"Empiric Testing and enhancement of web-based abstract screening tool(Abstrackr) EMPIRICAL TESTING AND ENHANCEMENT OF WEB-BASED ABSTRACT SCREENING TOOL (ABSTRACKR)  In this year-long project, we aim to empirically assess the performance and efficiency of state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine and stakeholder-driven comparative effectiveness reviews. We have developed AbstrackrTM (hereon, Abstrackr), a human-guided computerized abstract screening tool that aims to reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. Abstrackr makes use of machine learning techniques, and is offered as a free web-based tool that enables management of the screening process.  We also aim to revise the web-interface of Abstrackr to make it more intuitive, user friendly, and add documentation and functionalities requested by users; and to revise Abstrackr’s back-end, which includes the way the software parses and analyses citations, fits machine learning models, and makes computations, to make it more efficient. These revisions will ensure that the tool becomes more robust, and that it remains usable for larger projects and for many teams.  The proposed work will be carried out by the developers of Abstrackr, comprising a highly experienced team of systematic review investigators and computer scientists at Brown University and the University of Texas at Austin, who have been working together for at least seven years. We will pursue dissemination of the findings of this assessment and of the revised tool through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its wider adoption by the Agency for Healthcare Research and Quality Evidence-based Practice Center Program, Cochrane Collaboration, and other groups conducting systematic reviews. We will also continue to make all code available online. Our aims are to: Aim 1. Empirically measure the efficiency and accuracy of the prediction algorithms in Abstrackr in the computer-assisted semi-automated screening of citations for eligibility in systematic reviews. Aim 2. Improve and add to the functionality of the Web-based Abstrackr software, based in part on enhancements suggested by a panel of identified heavy users. Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making and systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to assess the performance and efficiency of a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care, and to augment the functionality of its public implementation.",Empiric Testing and enhancement of web-based abstract screening tool(Abstrackr),9168247,R03HS024812,[' '],AHRQ,BROWN UNIVERSITY,R03,2016,99999,127562714,0.0012668500601453628
"Supporting Systematic Review Production with Article Similarity Network Visualization PROJECT SUMMARY Systematic reviews (SRs), or systematic reviews of literature, summarize evidence drawn from high quality studies, and are often the preferred source of evidence-based practice (EBP). However, conducting an SR is labor-intensive and time consuming, typically requiring several months to complete. It has been reported that more than ten thousands of SRs are needed to synthesize existing medical knowledge. An Article screening process is one of the most intensive and time consuming steps, which requires SR researchers to screen a large amount of references, ranging from hundreds to more than 10,000 articles, depending on the size of a SR. In the past 10 years, machine learning model training approaches24-29 were developed to accelerate the article selection process through automation. However, they are not widely used due to diffusion challenges.7,14 Major obstacles include 1) a training sample is required to generate the automation algorithm. If the training sample is biased, the article selection process will systematically fail; 2) the automation approach is not made available for non-computer science specialists, therefore SR researchers will not be able to “fine-tune” the automation algorithm for particular conditions in various SR topics; 3) As there is no global automation algorithm, the generalizability is significantly limited; 4) It is difficult to assess the actual workload saved, while finding every relevant article is required in SR. We propose a new approach to provide views of article relationships in an article network. This is different from other bibliometric networks constructing citation, co-author, or co-occurrence networks. Article network is a simple and logical concept: visualizing article relationships and distribution based on articles' similarities in titles, abstracts, keywords, publication types, etc. SR researchers can also alter the article distribution by adjusting the similarities. This approach does not aim to suggest an end-point of the screening process. Rather, it provides a view of distribution for included, excluded, and undecided articles. In the proposed research, we will integrate advanced techniques to sparsify article networks with mixed sparsification methods, and improve the quality and efficiency of large network visualization layouts by constructing a multi-level network structure and advanced force model. We aim to provide approaches to sparsify and visualize article networks with more than 10,000 articles. Our approach is highly generalizable that it can be used for any health science topics. By viewing the article distribution, SR researchers will be able to screen a large amount of literature more efficiently. This approach can be integrated into current SR technologies and used directly by SR researchers. The success of this project can support SR production on any health science topics, and thus streamline their ultimate application in EBP paradigms. PROJECT NARRATIVE Systematic reviews (SRs) provide the highest quality of research evidence for patient care. To accelerate the production of SRs, we will implement advanced visualization techniques to view article relationships and distribution with article networks and in a timely and human readable manner. The success of the project will support SR production and thus streamline their ultimate application to evidence-based practice.",Supporting Systematic Review Production with Article Similarity Network Visualization,9227858,R03HS025047,[' '],AHRQ,OHIO STATE UNIVERSITY,R03,2016,100000,241268189,-0.0009123299242990287
"Deep learning for representation of codes used for SEER-Medicare claims research ﻿    DESCRIPTION (provided by applicant):  We propose developing an algorithm and user-friendly software to better identify treatments using Medicare claims data. We will validate our approach using procedures listed in the Surveillance, Epidemiology, and End Results (SEER) database as a gold standard. In this way, we hope to better match procedures identified using Medicare claims data with SEER listed procedures.  The focus of this research is observational (i.e. non-randomized) data. Well-run randomized clinical trials can provide the best level of evidence of treatment effects. However, randomized trials in the United States have suffered from poor accrual for many interventions. Despite the fact that well-designed randomized clinical trials should be the gold standard, well-designed observational studies might be the only method of obtaining inferences concerning comparative effectiveness for some cancer interventions.  In cancer research, one of the most commonly used databases for observational research is the linked SEER-Medicare database. SEER-Medicare data has provided useful measurements of the effectiveness of a number of cancer therapies. Algorithms for identifying relevant treatment and diagnosis codes using Medicare data are often based on clinical reasoning and scientific evidence. One group of researchers, for example, developed an algorithm for identifying laparoscopic surgery among kidney cancer cases before claims codes for laparoscopic surgery were well developed. While such algorithms are useful for others pursuing similar investigations, there may still be substantial mismatch between treatment identified by the SEER cancer registry and treatment identified through Medicare claims. In this work, we propose developing a rigorous machine learning algorithm that can help researchers in better identifying treatments in Medicare claims data. Specifically, we will design a neural language modeling algorithm and implement a software system that finds vector representations of diagnosis and procedure codes.  We plan on using the neural language modeling algorithm to learn vector representations from SEER- Medicare claims data where related procedure and diagnosis codes are ""neighbors"" (i.e. closely related). We will investigate whether the codes we identify within neighborhoods correspond to the procedure codes used for published SEER-Medicare studies. We will then design a software assistant interface that will allow an investigator to explore which codes are related to a given seed of diagnosis or procedure codes. Finally, we will investigate the sensitivity and specificity of the algorithm by comparing procedures identified using Medicare claims with procedures listed in the SEER database. We will replicate analyses from a published SEER-Medicare paper to investigate if estimated treatment effects differ when using our novel algorithm compared to using the algorithm in the published paper.         PUBLIC HEALTH RELEVANCE: In cancer research, one of the most commonly used databases for observational research is the linked Surveillance, Epidemiology, and End Results (SEER)-Medicare database. To improve the identification of procedures when using Medicare claims data, we will design a software assistant interface that will allow an investigator to explore which codes are related to a given seed of diagnosis or procedure codes. This should improve the identification of procedures when using Medicare claims data, and make conclusions drawn from analyses using the database more reliable and consistent.            ",Deep learning for representation of codes used for SEER-Medicare claims research,9023921,R21CA202130,"['Algorithms', 'Cancer Intervention', 'Clinical', 'Code', 'Computer software', 'Data', 'Databases', 'Diagnosis', 'Effectiveness', 'Ethical Issues', 'Funding', 'Future', 'Gold', 'International Classification of Diseases', 'Intervention', 'Investigation', 'Language', 'Laparoscopic Surgical Procedures', 'Learning', 'Level of Evidence', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Medical', 'Medical Records', 'Medical Surveillance', 'Medicare', 'Medicare claim', 'Methods', 'Modeling', 'Natural Language Processing', 'Neighborhoods', 'Observational Study', 'Outcome', 'Paper', 'Patients', 'Procedures', 'Process', 'Proxy', 'Publishing', 'Randomized', 'Randomized Clinical Trials', 'Records', 'Renal carcinoma', 'Research', 'Research Personnel', 'Running', 'Seeds', 'Sensitivity and Specificity', 'Software Tools', 'Statistical Study', 'Terminology', 'Testing', 'Time', 'United States', 'Update', 'Work', 'abstracting', 'anticancer research', 'base', 'cancer therapy', 'comparative effectiveness', 'design', 'health disparity', 'improved', 'interest', 'malignant breast neoplasm', 'neoplasm registry', 'novel', 'public health relevance', 'randomized trial', 'relating to nervous system', 'software systems', 'treatment effect', 'usability', 'user friendly software', 'vector', 'volunteer']",NCI,RESEARCH INST OF FOX CHASE CAN CTR,R21,2016,178072,29585825,-0.02901089096380602
"Computer Studies of Protein Structure and Function DESCRIPTION (provided by applicant): The research described in the current proposal has the long-range goals of enabling the prediction of protein-protein interactions and the application of this knowledge to problems of biomedical relevance. An additional long-term goal is the fuller integration of Structural and Systems Biology. Specific Aims include: a) The development of three-dimensional structure-based methods to predict, on a genome-wide scale, whether and how two proteins interact. b) The integration of structural information with other sources of evidence as to protein-protein interactions. c) The application of the methods being developed to important biomedical problems. These research goals are motivated by a number of factors. First, cellular function is mediated by tens or even hundreds of thousands of protein-protein interactions yet these are generally hard to predict in advance or to measure accurately with high-throughput experimental techniques. A method that allows the computational prediction of such interactions would thus be of significant impact. Second, there are many more protein sequences than protein structures so it is necessary to find ways to amplify the information in protein databases if structure is to be fully integrated in genome scale research. The proposed research is intended to bridge this gap.  A central element of the approach to be taken is the use of structural alignments to reveal novel functional relationships between proteins. Since structure is better conserved than sequence, these alignments can reveal new information. A novel structure-based method is introduced which exploits homology models and remote geometric relationships to amplify structural information. The evidence that is obtained is then combined with non-structural sources of evidence using Bayesian networks to yield a probability of whether two proteins interact. An important element in the proposed research strategy is the recognition that structural modeling on a large scale is necessarily imprecise so that it is necessary to use low resolution ""scoring functions for a given model that do not depend sensitively on atomic detail. Bayesian methods then allow the extraction of a clear signal from the underlying noise.  The biological impact of the research will be enhanced through computational/experimental applications in areas where structural clues have the potential of discovering truly novel interactions. Validated predictions on adhesion proteins, nuclear hormone receptors and cytokine signaling proteins demonstrate the efficacy of the methodology. New applications to viral host interactions and to cancer signaling pathways are described. PUBLIC HEALTH RELEVANCE: The health relatedness of the proposed research arises from the fact that a better understanding of protein- protein interactions opens the way to a better understanding of the biology underlying human disease. Applications to interactions involved in viral-host interactions, and in cancer-related pathways serve to highlight this aspect of the proposed research.",Computer Studies of Protein Structure and Function,8972013,R01GM030518,"['Adhesions', 'Amino Acid Sequence', 'Area', 'Bayesian Method', 'Binding', 'Biological', 'Biology', 'Birds', 'Cell physiology', 'Chromosomes', 'Collaborations', 'Complex', 'Conserved Sequence', 'Cytokine Signaling', 'Data', 'Databases', 'Development', 'Elements', 'Epigenetic Process', 'Family', 'Family suidae', 'Funding', 'Gene Fusion', 'Genetic screening method', 'Genome', 'Goals', 'Health', 'Homology Modeling', 'Human', 'Human Genome', 'Indium', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Mediating', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Noise', 'Nuclear Hormone Receptors', 'Pathway interactions', 'Peptide Sequence Determination', 'Peptides', 'Phylogenetic Analysis', 'Probability', 'Process', 'Protein Region', 'Protein Structure Databases', 'Proteins', 'RNA Interference', 'Research', 'Resolution', 'Sequence Alignment', 'Sequence Homology', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'Signaling Protein', 'Site', 'Source', 'Specificity', 'Structural Models', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Tropism', 'Viral', 'Viral Proteins', 'Virus', 'Work', 'Yeasts', 'base', 'computer based statistical methods', 'computer infrastructure', 'computer studies', 'computerized tools', 'design', 'genome-wide', 'human disease', 'improved', 'influenzavirus', 'innovation', 'interest', 'method development', 'novel', 'predictive tools', 'programs', 'protein complex', 'protein function', 'protein protein interaction', 'protein structure', 'protein structure function', 'success', 'three dimensional structure', 'tool', 'tumor progression', 'tumorigenesis', 'virus host interaction']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2016,292404,558628098,0.04145083894710805
"Data-driven analysis of protein structure, function, and regulation PROJECT SUMMARY Proteins are capable of performing a wide variety of complex molecular functions and play a central role in all biological processes. A detailed and quantitative understanding of the relationship between a protein's sequence and it's biochemical properties would have a profound impact across all areas of biology, medicine, and biotechnology. We are developing data-driven approaches for dissecting the molecular basis of protein function. Our general framework involves designing informative libraries of protein sequences, experimentally mapping the relationship between sequence and function, and extracting detailed functional information from large sequence-function data sets. This work leverages emerging technologies and methods in DNA sequencing and synthesis, microfluidic screening, large-scale statistical learning, and optimization. We will develop generalizable platforms that can be applied to study a wide variety of enzymes and membrane transport proteins. PROJECT NARRATIVE A detailed and quantitative understanding of the relationship between a protein's sequence and it's biochemical properties would have a profound impact across all areas of biology, medicine, and biotechnology. This important capability would allow us to design new therapeutic and diagnostic proteins, and diagnose genetic diseases before they manifest symptoms. The goal of this proposal is to develop a general framework for dissecting the molecular basis of protein structure, function, and regulation.","Data-driven analysis of protein structure, function, and regulation",9143006,R35GM119854,"['Amino Acid Sequence', 'Area', 'Biochemical', 'Biological Process', 'Biology', 'Biotechnology', 'Complex', 'DNA Sequence', 'DNA biosynthesis', 'Data', 'Data Set', 'Diagnosis', 'Emerging Technologies', 'Enzymes', 'Goals', 'Hereditary Disease', 'Libraries', 'Machine Learning', 'Maps', 'Medicine', 'Membrane Transport Proteins', 'Methods', 'Microfluidics', 'Molecular', 'Peptide Sequence Determination', 'Play', 'Property', 'Proteins', 'Regulation', 'Role', 'Symptoms', 'Work', 'base', 'design', 'novel diagnostics', 'novel therapeutics', 'protein function', 'protein structure function', 'screening']",NIGMS,UNIVERSITY OF WISCONSIN-MADISON,R35,2016,359339,338121506,0.03237762551668807
"Molecular impact of mutations in monogenic disease and cancer ABSTRACT  Next generation genome scale sequencing of patients is now becoming routine for two classes of disease: rare  Mendelian traits and cancer. In favorable cases, these data allow identification of relevant mutations and thus  aid diagnosis and therapy. In both classes of disease, the most common type of mutation is missense -­ single  base  changes  that  result  in  an  amino  acid  substitution  in  a  protein.  Uncertainty  as  to  the  impact  of  these  mutations on in vivo protein activity has resulted in a very conservative approach to their interpretation in the  clinic,  so  causing  many  missed  opportunities  for  targeted  treatment.  The  goal  of  this  project  is  to  use  a  combination of three strategies to make the interpretation of these mutations much more applicable in the clinic.  There are already a large number of computational methods that attempt to determine the impact of missense  mutations on function, and there is substantial evidence that these have useful accuracy. The primary difficulty  is that the accuracy in any particular case is not reliably calibrated. Therefore, our first aim is to use a combination  of these methods to develop an approach focused on more reliable estimates for the probability of high impact  on  protein  function  (i.e.  more  confident  P  values).  The  second  aim  is  to  maximize  the  utilization  of  three-­ dimensional structural information, largely ignored by most computational methods. A large fraction of missense  mutations in these classes of disease act by destabilizing protein structure and knowledge of structure allows  these to be identified with much higher reliability. Also, structure provides a framework for detailed annotation  and comprehension of function. To facilitate the utilization of structure, we will implement a modeling platform  that leverages available experimental information to maximize the structural data available for analyzing mutation  impact.  An  important  aspect  of  the  platform  is  incorporation  of  methods  for  evaluating  the  reliability  of  the  structural features relevant to analysis of each mutation. In the third aim we will build specific functional models  for each protein of interest, integrating information from current databases, the literature, and community input,  so as to provide the richest possible background against which to judge the impact of mutations. Proteopedia, a  well established media wiki for proteins, will be used to provide an integrated view of text, data, and structure. A  key component of the information resource will be contributions from curators, who will provide annotation and  also solicit input from other experts. This aspect of the project builds on experience with other crowdsourcing  endeavors,  including  CASP,  CAGI  and  Proteopedia.    There  will  be  three  primary  outcomes  from  the  project:  First, improved reliability for the interpretation of missense mutations. Second, a prototype mutation annotation  procedure suitable for use in a clinical setting. Third, the resource will provide information of benefit to a range  of other scientists, thus facilitating the analysis of disease related mutations.      NARRATIVE  Genome  scale  DNA  sequencing  is  now  contributing  to  diagnosis  and  therapy  in  cases  of  rare  human  disease and cancer.  Full exploitation of these data is currently hampered by inadequate understanding  of which DNA changes affect protein function so as to contribute to disease. This project aims to develop  the methods and tools needed to remove that obstacle. ",Molecular impact of mutations in monogenic disease and cancer,9156099,R01GM120364,"['Address', 'Affect', 'Amino Acid Substitution', 'Clinic', 'Clinical', 'Communities', 'Comprehension', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Consensus', 'DNA', 'DNA Sequence', 'Data', 'Databases', 'Diagnosis', 'Disease', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mendelian disorder', 'Methods', 'Missense Mutation', 'Modeling', 'Molecular', 'Mutation', 'Mutation Analysis', 'Patients', 'Play', 'Probability', 'Procedures', 'Process', 'Proteins', 'Rare Diseases', 'Reporting', 'Resources', 'Role', 'Scientist', 'Structural Models', 'Structure', 'Tertiary Protein Structure', 'Text', 'Uncertainty', 'base', 'clinically relevant', 'crowdsourcing', 'data structure', 'experience', 'genome-wide', 'human disease', 'improved', 'in vivo', 'interest', 'learning strategy', 'next generation', 'primary outcome', 'protein function', 'protein protein interaction', 'protein structure', 'prototype', 'targeted treatment', 'tool', 'trait', 'wiki']",NIGMS,"UNIV OF MARYLAND, COLLEGE PARK",R01,2016,372928,58406802,0.006014092250400296
"Interactive Informatics Resource for Research-driven Cancer Proteomics DESCRIPTION (provided by applicant):  In 2013 over 1.6 million new cases of cancer are expected to be diagnosed and over 580,000 people are expected to die of the disease. Thus, continued research in the identification of new diagnostic and prognostic biomarkers of cancer is necessary. Although cancer is widely recognized as a genomic disease, the directives of the DNA-based drivers are executed at the level of proteins and their biological functions, and the application of potential protein level biomarkers remains a compelling vision. Thus, a large investment has been made by NCI and other research centers in high-throughput global proteomics experiments to mine for novel biomarkers of cancer. However, few of these markers have come to fruition. We believe that one of the major challenges to the discovery of robust protein- or pathway-biomarker candidates from these large and complex proteomics datasets is due to naive data analysis approaches that do not take into account the underlying complexity of the proteome (e.g., splice variants, post- translational modifications). State-of-the-art statistical algorithms to improve the tasks of quality assessment, peptide and protein quantification, and pathway modeling that are designed to account for the design of the experiment have been developed; however access to these methodologies by the larger community is hindered since they are in the prototype stage and typically require knowledge of statistical programming. Furthermore, the likelihood of these tools moving to robust software is low since they are developed within the context of existing grants that do not support the transition from prototype to software. For the field of clinical proteomics to successfully identif new mechanistic etiologies of cancer requires not only high quality data with respect to the instrument, but also high quality statistical analysis of the data. This project proposes new informatics technology in the form of a robust, interactive and cross- platform software environment that will enable biomedical and biological scientists to perform in-depth analyses of global proteomics data from the point of quality assessment and normalization of raw inferred abundances (e.g., peak area) to the identification of protein biomarkers and enriched pathways. The software will be designed in a single programming language (Java) to assure easy installation across platforms with wizard-based data entry and advanced data reporting. Java will also support the development of advanced graphical user interfaces for data presentation and interactive graphics with a modern look and feel. This approach will ensure that scientists outside of the development institution can develop modules to include in the software or extensions for data integration without challenges of re-compiling the application. The software modules to be developed under this project are Aim 1) peptide and protein level quality assessment and quantification, Aim 2) protein biomarker discovery via exploratory data analysis and machine learning, and Aim 3) pathway biomarker discovery through integration with the NCI Protein Interaction Database. PUBLIC HEALTH RELEVANCE:  For the past decade, cancer researchers have been utilizing global proteomics analyses to extensively categorize proteins and other molecular species in hopes of identifying distinctive features of cancer cells that not only explain the biology, but alo enable better patient care. Despite these investments, relatively few protein biomarkers have achieved clinical validation largely due to naive data analysis strategies used in the protein quantification and statistical validation of candidate biomarkers. This project will develop a robust user- friendly software environment that builds upon state-of-the-art statistical algorithms that are focused on addressing the underlying proteome complexity associated with cancer.",Interactive Informatics Resource for Research-driven Cancer Proteomics,9059049,U01CA184783,"['Accounting', 'Address', 'Advanced Development', 'Algorithms', 'Area', 'Biological', 'Biological Markers', 'Biological Process', 'Biology', 'Breast', 'Cancer Etiology', 'Clinical', 'Code', 'Collection', 'Communities', 'Complex', 'Computer software', 'DNA', 'Data', 'Data Analyses', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Ensure', 'Environment', 'Experimental Designs', 'Funding', 'Genes', 'Genomics', 'Genotype', 'Goals', 'Grant', 'Health', 'Imagery', 'Individual', 'Informatics', 'Institutes', 'Institution', 'Investments', 'Java', 'Knowledge', 'Label', 'Letters', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Metadata', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular', 'National Institute of Allergy and Infectious Disease', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Ovarian', 'Pathway interactions', 'Patient Care', 'Pattern', 'Peptide Mapping', 'Peptides', 'Phenotype', 'Post-Translational Protein Processing', 'Process', 'Prognostic Marker', 'Programming Languages', 'Protein Fragment', 'Proteins', 'Proteome', 'Proteomics', 'RNA Splicing', 'Research', 'Research Personnel', 'Resource Informatics', 'Resources', 'Sampling', 'Scientist', 'Source Code', 'Specificity', 'Staging', 'Statistical Algorithm', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Sum', 'Technology', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Variant', 'Vision', 'Visual', 'base', 'biomarker discovery', 'built environment', 'cancer cell', 'cancer proteomics', 'candidate marker', 'computerized data processing', 'computerized tools', 'data integration', 'design', 'diagnostic biomarker', 'experience', 'graphical user interface', 'improved', 'instrument', 'link protein', 'mathematical methods', 'model design', 'novel diagnostics', 'novel marker', 'potential biomarker', 'prognostic', 'programs', 'protein biomarkers', 'protein expression', 'prototype', 'research study', 'software development', 'statistics', 'tool', 'user friendly software']",NCI,BATTELLE PACIFIC NORTHWEST LABORATORIES,U01,2016,423583,19252132,-0.0014197853312406057
"An Open Source Precision Medicine Platform for Cloud Operating Systems ﻿    DESCRIPTION (provided by applicant):  Rapid improvements in DNA sequencing and synthesis have the potential to usher in a new era of precision medicine. To realize this vision, however, we must re-imagine the computational and storage infrastructure used to manage and extract actionable results from the massive data sets made possible by widely available advances in DNA sequencing and synthetic biology. In conjunction with the Global Alliance for Genomics and Health (GA4GH), we propose to build the Arvados platform so that a new ecosystem of clinical decision support applications will be able to navigate petabytes of global biomedical data and search millions of genomes in real-time (seconds). Our team has a proven track record of commercial success and high impact scientific research. Commercialization of this free and open-source software (FOSS) platform, which will be greatly accelerated by this grant, will permit organizations to seamlessly span on-premise & hosted cloud- operating systems and vastly simplify data-management & computation, all while facilitating compliance with institutional policies and regulatory requirements.         PUBLIC HEALTH RELEVANCE:  The delivery of healthcare based on molecular data specific to an individual patient (i.e. precision medicine) will require the creation of a new ecosystem of Clinical Decision Support (CDS) applications. This work will provide a platform that will make the development of such applications faster, easier, and less expensive.        ",An Open Source Precision Medicine Platform for Cloud Operating Systems,9140741,R44GM109737,"['Address', 'Adopted', 'Big Data', 'Bioinformatics', 'Businesses', 'Capital', 'Clinical', 'Clinical Decision Support Systems', 'Collaborations', 'Communities', 'Computer software', 'Contractor', 'DNA Sequence', 'DNA biosynthesis', 'Data', 'Data Set', 'Databases', 'Development', 'Distributed Systems', 'Ecosystem', 'Feedback', 'Fostering', 'Funding', 'Galaxy', 'Genome', 'Genomics', 'Grant', 'Health', 'Healthcare', 'Human', 'Industry', 'Information Technology', 'Institutional Policy', 'International', 'Internet', 'Language', 'Length', 'Letters', 'Machine Learning', 'Maintenance', 'Manuscripts', 'Measures', 'Medicine', 'Memory', 'Molecular', 'Operating System', 'Phase', 'Policies', 'Production', 'Publications', 'Reproducibility', 'Research', 'Research Infrastructure', 'Resources', 'Secure', 'Services', 'Small Business Innovation Research Grant', 'Source Code', 'System', 'Technology', 'Time', 'Training Support', 'Vision', 'Work', 'base', 'big biomedical data', 'cloud platform', 'commercialization', 'data management', 'genome sequencing', 'genomic data', 'health care delivery', 'individual patient', 'meetings', 'new technology', 'next generation sequencing', 'open source', 'operation', 'petabyte', 'portability', 'precision medicine', 'public health relevance', 'repository', 'screening', 'success', 'symposium', 'synthetic biology', 'terabyte', 'web services', 'whole genome']",NIGMS,"CUROVERSE, INC.",R44,2016,985339,0,0.008842026395686
"Reproducibility Assessment for Multivariate Assays DESCRIPTION (provided by applicant): This Small Business Innovation Research project addresses the problem of assessing reproducibility in analyzing high-throughput data. In feature selection for data with large numbers of features, it is well known that some features will appear to affect an outcome by chance, and that subsequent predictions based on these features may not be as successful as initial results would seem to indicate. Similarly, there are often multiple stages, and many parameters, involved in the multivariate assays de- signed to analyze high-throughput profiles. For example, good results achieved with a particular combination of settings for an instance of cross-validation may not generalize to other instances. The objective of this proposal is to extend new statistical methods for assessing reproducibility in replicate experiments to the context of machine learning, and demonstrate effectiveness in this application. The machine-learning methods to be investigated will include random forests, supervised principal components, lasso penalization and support vector machines. We will use simulated and real data from genomic applications to show the potential of this approach for providing reproducibility assessments that are not confounded with prespecified choices, for determining biologically relevant thresholds, for improving the accuracy of signal identification, and for identifying suboptimal results. Relevance. Although today's high-throughput technologies offer the possibility of revolutionizing clinical practice, the analytical tools availble for extracting information from this amount of data are not yet sufficiently developed for targeted exploration of the underlying biology. This project directly addresses the need to make what the FDA terms IVDMIA (In-Vitro Diagnostic Multivariate Index Assays) transparent, interpretable, and reproducible, and is thus an opportunity to improve analysis products and services provided to companies that identify, characterize, and validate biomarkers for clinical diagnostics and drug development decision points. The long-term goal of the proposed project is to develop a platform for biomarker discovery and integrative genomic analysis, with reproducibility assessment incorporated into multivariate assays. This will enable evaluation and improvement of approaches to detecting the biological factors that affect a particular outcome, and lead to more efficient and more effective methods for disease diagnosis, treatment monitoring, and therapeutic drug development. PUBLIC HEALTH RELEVANCE: Statistical models play a key role in medical research in uncovering information from data that leads to new diagnostics and therapies. However, development of standards for reliability in biomedical data mining has not kept up with the rapid pace at which new data types and modeling approaches are being devised. This proposal is for new methods for quantifying reproducibility in biomedical data analyses that will have a far-reaching impact on public health by streamlining protocols, reducing costs and offering more effective clinical support systems.",Reproducibility Assessment for Multivariate Assays,8828718,R43GM109503,"['Address', 'Affect', 'Algorithms', 'Area', 'Bioinformatics', 'Biological Assay', 'Biological Factors', 'Biological Markers', 'Biology', 'ChIP-seq', 'Clinical', 'Cloud Computing', 'Data', 'Data Analyses', 'Decision Trees', 'Development', 'Diagnostic', 'Dimensions', 'Effectiveness', 'Evaluation', 'Evolution', 'Genomics', 'Goals', 'Guidelines', 'Health', 'In Vitro', 'Investigation', 'Lasso', 'Lead', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Performance', 'Phase', 'Play', 'Protocols documentation', 'Public Health', 'Publishing', 'ROC Curve', 'Reproducibility', 'Research Project Grants', 'Scheme', 'Services', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Source', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Statistical Models', 'Support System', 'Techniques', 'Technology', 'Therapeutic', 'Trees', 'Validation', 'analytical tool', 'base', 'clinical practice', 'cost', 'data mining', 'design', 'disease diagnosis', 'drug development', 'follow-up', 'forest', 'high throughput technology', 'improved', 'indexing', 'novel diagnostics', 'research study']",NIGMS,INSILICOS,R43,2015,64005,0,-0.019607465879583074
"Mobility Data Integration to Insight     DESCRIPTION (provided by applicant): Mobility is essential for human health. Regular physical activity helps prevent heart disease and stroke, relieves symptoms of depression, and promotes weight loss. Unfortunately, many conditions, such as cerebral palsy, osteoarthritis, and obesity, limit mobility at an enormous personal and societal cost. While vast amounts of data are available from hundreds of research labs and millions of smartphones, there is a dearth of methods for analyzing this massive, heterogeneous dataset.  We propose to establish the National Center for Mobility Data Integration to Insight (the Mobilize Center) to overcome the data science challenges facing mobility big data and biomedical big data in general. Our preliminary work identified four bottlenecks in data science, which drive four Data Science Research Cores.  The Cores include Biomechanical Modeling, Statistical Learning, Behavioral and Social Modeling, and Integrative Modeling and Prediction. Our Cores will produce novel methods to integrate diverse modeling modalities and gain insight from noisy, sparse, heterogeneous, and time-varying big data. Our data-sharing consortia, with clinical, research, and industry partners, will provide mobility data for over ten million people.  Three Driving Biomedical Problems will focus and validate our data science research.  The Mobilize Center will disseminate our novel data science tools to thousands of researchers and create a sustainable data-sharing consortium. We will train tens of thousands of scientists to use data science methods in biomedicine through our in-person and online educational programs. We will establish a cohesive, vibrant, and sustainable National Center through the leadership of an experienced executive team and will help unify the BD2K consortia through our Biomedical Computation Review publication and the Simtk.org resource portal.  The Mobilize Center will lay the groundwork for the next generation of data science systems and revolutionize diagnosis and treatment for millions of people affected by limited mobility.         PUBLIC HEALTH RELEVANCE:  Regular physical activity is essential for human health, yet a broad range of conditions impair mobility. This project will transform human movement research by developing tools for data analysis and creating software that will advance research to prevent, diagnose, and reduce impairments that limit human movement.            ",Mobility Data Integration to Insight,8935802,U54EB020405,"['Affect', 'Area', 'Automobile Driving', 'Behavioral', 'Behavioral Model', 'Big Data', 'Biomechanics', 'Biomedical Computing', 'Biomedical Research', 'Body Weight decreased', 'Cellular Phone', 'Cerebral Palsy', 'Child', 'Classification', 'Clinical Research', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Degenerative polyarthritis', 'Diabetes Mellitus', 'Diagnosis', 'Educational workshop', 'Elderly', 'Ethics', 'Exercise', 'Fellowship', 'Fostering', 'Gait', 'Health', 'Heart Diseases', 'Human', 'Impairment', 'Individual', 'Injury', 'Joints', 'Leadership', 'Limb structure', 'Machine Learning', 'Medical center', 'Methods', 'Mission', 'Modality', 'Modeling', 'Monitor', 'Movement', 'NCI Scholars Program', 'Nature', 'Obesity', 'Operative Surgical Procedures', 'Outcome', 'Overweight', 'Pathology', 'Persons', 'Physical activity', 'Prevention', 'Problem Solving', 'Public Health', 'Publications', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Running', 'Science', 'Scientist', 'Stroke', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Visit', 'Walking', 'Work', 'base', 'biomechanical model', 'clinical decision-making', 'cognitive function', 'cohesion', 'cost', 'data integration', 'data modeling', 'data sharing', 'depressive symptoms', 'experience', 'flexibility', 'health data', 'improved', 'industry partner', 'insight', 'models and simulation', 'motor impairment', 'next generation', 'novel', 'novel strategies', 'prevent', 'programs', 'public health relevance', 'role model', 'sensor', 'social', 'social model', 'tool']",NIBIB,STANFORD UNIVERSITY,U54,2015,68981,560644462,0.010724549121839193
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,8935748,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Research', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Scientist', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2015,73173,338121506,0.013915772071026993
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,9133491,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,115911,560644462,0.00216607227130351
"Face De-Identification for Research and Clinical Use DESCRIPTION (provided by applicant): This application addresses NIH's call to promote data sharing and patient privacy. A major obstacle to sharing of recorded video has been the need to protect participants' identity. Similarly, concern about stigma is a reason that many people in need of mental health services (e.g., in the military) fail to do so. We propose a system to de-identify patients and research participants in video. Face de-identification transfers facial expression automatically from source face images, which are confidential, to target face images, which are not. The system safeguards face anonymity while preserving the facial expression of the original source video. The target video then can communicate the emotion, communicative intent, pain, and neurological or physiological status of the source person without displaying the source person's face. Face de-identification would enable video archive sharing among researchers and clinicians without compromising privacy or confidentiality. Moreover, a version of this system could potentially be used to preserve privacy and anonymity in internet-based interviews. Innovation. The project has four innovations. The approach (1) Removes identity information while retaining facial dynamics; thus preserving the information value of the face to communicate emotion, pain, and related states. (2) Accommodates subtle and spontaneous facial actions, rather than imitating only some predefined molar expressions (e.g., happy or sad). (3) Requires no training steps by target persons. (4) And requires no hand annotation of video. The system is entirely automatic. Approach. The software will take as input a video with the face of a subject (source) and automatically generate or output a video with the face de-identified. The project will use new machine learning and computer vision algorithms for transferring subtle facial expression from a source subject (original video) to a target subject, using only one frontal image of the target subject. A major novelty of the approach is to make the process completely automatic. The algorithm will be validated using commercially available software for face recognition and custom software for facial expression analysis. PUBLIC HEALTH RELEVANCE: Sharing of video recordings for research and clinical uses would significantly contribute to scientific discovery and patient diagnosis, treatment, and evaluation. We will develop and validate a fully automatic system that preserves facial expression in video while fully protecting face identity.",Face De-Identification for Research and Clinical Use,8908047,R21MH099487,"['Address', 'Age', 'Algorithms', 'Archives', 'Behavioral Sciences', 'Clinical', 'Code', 'Communication', 'Communities', 'Computer Vision Systems', 'Computer software', 'Confidentiality', 'Custom', 'Data Set', 'Detection', 'Diagnosis', 'Education', 'Emotions', 'Evaluation', 'Face', 'Facial Expression', 'Facial Expression Recognition', 'Goals', 'Gold', 'Hand', 'Health', 'Human', 'Image', 'Informed Consent', 'Intention', 'Internet', 'Interview', 'Judgment', 'Machine Learning', 'Maps', 'Measurement', 'Mental Health Services', 'Methods', 'Military Personnel', 'Modeling', 'Neurologic', 'Output', 'Pain', 'Participant', 'Patients', 'Perception', 'Personal Computers', 'Persons', 'Physiological', 'Privacy', 'Process', 'Research', 'Research Personnel', 'Running', 'Source', 'Step training', 'System', 'Testing', 'Training', 'Twin Multiple Birth', 'Video Recording', 'base', 'clinical practice', 'data sharing', 'graphical user interface', 'innovation', 'middle age', 'patient privacy', 'sex', 'social stigma', 'targeted imaging', 'user-friendly', 'young adult']",NIMH,CARNEGIE-MELLON UNIVERSITY,R21,2015,193512,30434536,-0.049367195057728804
"Computer Studies of Protein Structure and Function DESCRIPTION (provided by applicant): The research described in the current proposal has the long-range goals of enabling the prediction of protein-protein interactions and the application of this knowledge to problems of biomedical relevance. An additional long-term goal is the fuller integration of Structural and Systems Biology. Specific Aims include: a) The development of three-dimensional structure-based methods to predict, on a genome-wide scale, whether and how two proteins interact. b) The integration of structural information with other sources of evidence as to protein-protein interactions. c) The application of the methods being developed to important biomedical problems. These research goals are motivated by a number of factors. First, cellular function is mediated by tens or even hundreds of thousands of protein-protein interactions yet these are generally hard to predict in advance or to measure accurately with high-throughput experimental techniques. A method that allows the computational prediction of such interactions would thus be of significant impact. Second, there are many more protein sequences than protein structures so it is necessary to find ways to amplify the information in protein databases if structure is to be fully integrated in genome scale research. The proposed research is intended to bridge this gap.  A central element of the approach to be taken is the use of structural alignments to reveal novel functional relationships between proteins. Since structure is better conserved than sequence, these alignments can reveal new information. A novel structure-based method is introduced which exploits homology models and remote geometric relationships to amplify structural information. The evidence that is obtained is then combined with non-structural sources of evidence using Bayesian networks to yield a probability of whether two proteins interact. An important element in the proposed research strategy is the recognition that structural modeling on a large scale is necessarily imprecise so that it is necessary to use low resolution ""scoring functions for a given model that do not depend sensitively on atomic detail. Bayesian methods then allow the extraction of a clear signal from the underlying noise.  The biological impact of the research will be enhanced through computational/experimental applications in areas where structural clues have the potential of discovering truly novel interactions. Validated predictions on adhesion proteins, nuclear hormone receptors and cytokine signaling proteins demonstrate the efficacy of the methodology. New applications to viral host interactions and to cancer signaling pathways are described. PUBLIC HEALTH RELEVANCE: The health relatedness of the proposed research arises from the fact that a better understanding of protein- protein interactions opens the way to a better understanding of the biology underlying human disease. Applications to interactions involved in viral-host interactions, and in cancer-related pathways serve to highlight this aspect of the proposed research.",Computer Studies of Protein Structure and Function,8773597,R01GM030518,"['Adhesions', 'Amino Acid Sequence', 'Area', 'Bayesian Method', 'Binding', 'Biological', 'Biology', 'Birds', 'Cell physiology', 'Chromosomes', 'Collaborations', 'Complex', 'Conserved Sequence', 'Cytokine Signaling', 'Data', 'Databases', 'Development', 'Elements', 'Epigenetic Process', 'Family', 'Family suidae', 'Funding', 'Gene Fusion', 'Genetic screening method', 'Genome', 'Goals', 'Health', 'Homology Modeling', 'Human', 'Human Genome', 'Indium', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Mediating', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Noise', 'Nuclear Hormone Receptors', 'Pathway interactions', 'Peptide Sequence Determination', 'Peptides', 'Phylogenetic Analysis', 'Probability', 'Process', 'Protein Region', 'Protein Structure Databases', 'Proteins', 'RNA Interference', 'Research', 'Resolution', 'Sequence Alignment', 'Sequence Homology', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'Signaling Protein', 'Site', 'Source', 'Specificity', 'Structural Models', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Tropism', 'Viral', 'Viral Proteins', 'Virus', 'Work', 'Yeasts', 'base', 'computer based statistical methods', 'computer infrastructure', 'computer studies', 'computerized tools', 'design', 'genome-wide', 'human disease', 'improved', 'influenzavirus', 'innovation', 'interest', 'method development', 'novel', 'programs', 'protein complex', 'protein function', 'protein protein interaction', 'protein structure', 'protein structure function', 'success', 'three dimensional structure', 'tool', 'tumor progression', 'tumorigenesis', 'virus host interaction']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2015,291815,558628098,0.04145083894710805
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,9132876,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,413294,560644462,0.00216607227130351
"Interactive Informatics Resource for Research-driven Cancer Proteomics DESCRIPTION (provided by applicant):  In 2013 over 1.6 million new cases of cancer are expected to be diagnosed and over 580,000 people are expected to die of the disease. Thus, continued research in the identification of new diagnostic and prognostic biomarkers of cancer is necessary. Although cancer is widely recognized as a genomic disease, the directives of the DNA-based drivers are executed at the level of proteins and their biological functions, and the application of potential protein level biomarkers remains a compelling vision. Thus, a large investment has been made by NCI and other research centers in high-throughput global proteomics experiments to mine for novel biomarkers of cancer. However, few of these markers have come to fruition. We believe that one of the major challenges to the discovery of robust protein- or pathway-biomarker candidates from these large and complex proteomics datasets is due to naive data analysis approaches that do not take into account the underlying complexity of the proteome (e.g., splice variants, post- translational modifications). State-of-the-art statistical algorithms to improve the tasks of quality assessment, peptide and protein quantification, and pathway modeling that are designed to account for the design of the experiment have been developed; however access to these methodologies by the larger community is hindered since they are in the prototype stage and typically require knowledge of statistical programming. Furthermore, the likelihood of these tools moving to robust software is low since they are developed within the context of existing grants that do not support the transition from prototype to software. For the field of clinical proteomics to successfully identif new mechanistic etiologies of cancer requires not only high quality data with respect to the instrument, but also high quality statistical analysis of the data. This project proposes new informatics technology in the form of a robust, interactive and cross- platform software environment that will enable biomedical and biological scientists to perform in-depth analyses of global proteomics data from the point of quality assessment and normalization of raw inferred abundances (e.g., peak area) to the identification of protein biomarkers and enriched pathways. The software will be designed in a single programming language (Java) to assure easy installation across platforms with wizard-based data entry and advanced data reporting. Java will also support the development of advanced graphical user interfaces for data presentation and interactive graphics with a modern look and feel. This approach will ensure that scientists outside of the development institution can develop modules to include in the software or extensions for data integration without challenges of re-compiling the application. The software modules to be developed under this project are Aim 1) peptide and protein level quality assessment and quantification, Aim 2) protein biomarker discovery via exploratory data analysis and machine learning, and Aim 3) pathway biomarker discovery through integration with the NCI Protein Interaction Database. PUBLIC HEALTH RELEVANCE:  For the past decade, cancer researchers have been utilizing global proteomics analyses to extensively categorize proteins and other molecular species in hopes of identifying distinctive features of cancer cells that not only explain the biology, but alo enable better patient care. Despite these investments, relatively few protein biomarkers have achieved clinical validation largely due to naive data analysis strategies used in the protein quantification and statistical validation of candidate biomarkers. This project will develop a robust user- friendly software environment that builds upon state-of-the-art statistical algorithms that are focused on addressing the underlying proteome complexity associated with cancer.",Interactive Informatics Resource for Research-driven Cancer Proteomics,8847691,U01CA184783,"['Accounting', 'Address', 'Advanced Development', 'Algorithms', 'Area', 'Biological', 'Biological Markers', 'Biological Process', 'Biology', 'Breast', 'Cancer Etiology', 'Clinical', 'Code', 'Collection', 'Communities', 'Complex', 'Computer software', 'DNA', 'Data', 'Data Analyses', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Ensure', 'Environment', 'Experimental Designs', 'Funding', 'Genes', 'Genomics', 'Genotype', 'Goals', 'Grant', 'Health', 'Imagery', 'Individual', 'Informatics', 'Institutes', 'Institution', 'Investments', 'Java', 'Knowledge', 'Label', 'Letters', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular', 'National Institute of Allergy and Infectious Disease', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Ovarian', 'Pathway interactions', 'Patient Care', 'Pattern', 'Peptide Mapping', 'Peptides', 'Phenotype', 'Post-Translational Protein Processing', 'Process', 'Prognostic Marker', 'Programming Languages', 'Protein Fragment', 'Proteins', 'Proteome', 'Proteomics', 'RNA Splicing', 'Research', 'Research Personnel', 'Resource Informatics', 'Resources', 'Sampling', 'Scientist', 'Source Code', 'Specificity', 'Staging', 'Statistical Algorithm', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Sum', 'Technology', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Variant', 'Vision', 'Visual', 'base', 'built environment', 'cancer cell', 'cancer proteomics', 'candidate validation', 'computerized data processing', 'computerized tools', 'data integration', 'design', 'experience', 'graphical user interface', 'improved', 'instrument', 'link protein', 'mathematical methods', 'model design', 'novel', 'novel diagnostics', 'prognostic', 'programs', 'protein expression', 'prototype', 'research study', 'software development', 'statistics', 'tool', 'user friendly software']",NCI,BATTELLE PACIFIC NORTHWEST LABORATORIES,U01,2015,421696,19252132,-0.0014197853312406057
"The Cardiovascular Research Grid DESCRIPTION (provided by applicant):    The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinations of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotating ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and motion that can predict the early presence of developing heart disease in time for therapeutic intervention. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informatics system that allows clinical information to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG. RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8786588,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'High Performance Computing', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Therapeutic Intervention', 'Time', 'Ultrasonography', 'Work', 'base', 'cardiovascular imaging', 'cardiovascular visualization', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2015,2177431,807432003,0.011527074521106306
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,8836569,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,2687363,560644462,0.00216607227130351
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,8774800,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Clinical Trials', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Research', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Scientist', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2014,73173,338121506,0.013915772071026993
"Reproducibility Assessment for Multivariate Assays  Project Summary. This Small Business Innovation Research project addresses the problem of assessing reproducibility in analyzing high-throughput data. In feature selection for data with large numbers of fea- tures, it is well known that some features will appear to affect an outcome by chance, and that subsequent predictions based on these features may not be as successful as initial results would seem to indicate. Similarly, there are often multiple stages, and many parameters, involved in the multivariate assays de- signed to analyze high-throughput profiles. For example, good results achieved with a particular combina- tion of settings for an instance of cross-validation may not generalize to other instances. The objective of this proposal is to extend new statistical methods for assessing reproducibility in replicate experiments to the context of machine learning, and demonstrate effectiveness in this application. The machine-learning methods to be investigated will include random forests, supervised principal components, lasso penal- ization and support vector machines. We will use simulated and real data from genomic applications to show the potential of this approach for providing reproducibility assessments that are not confounded with prespecified choices, for determining biologically relevant thresholds, for improving the accuracy of signal identification, and for identifying suboptimal results. Relevance. Although today's high-throughput technologies offer the possibility of revolutionizing clinical practice, the analytical tools available for extracting information from this amount of data are not yet sufficiently developed for targeted exploration of the underlying biology. This project directly addresses the need to make what the FDA terms IVDMIA (In-Vitro Diagnostic Multivariate Index Assays) transparent, interpretable, and reproducible, and is thus an opportunity to improve analysis products and services provided to companies that identify, characterize, and validate biomarkers for clinical diagnostics and drug development decision points. The long-term goal of the proposed project is to develop a platform for biomarker discovery and integrative genomic analysis, with reproducibility assessment incorporated into multivariate assays. This will enable evaluation and improvement of approaches to detecting the biological factors that affect a particular outcome, and lead to more efficient and more effective methods for disease diagnosis, treatment monitoring, and therapeutic drug development. PUBLIC HEALTH RELEVANCE: Statistical models play a key role in medical research in uncovering information from data that leads to new diagnostics and therapies. However, development of standards for reliability in biomedical data mining has not kept up with the rapid pace at which new data types and modeling approaches are being devised. This proposal is for new methods for quantifying reproducibility in biomedical data analyses that will have a far-reaching impact on public health by streamlining protocols, reducing costs and offering more effective clinical support systems.            ",Reproducibility Assessment for Multivariate Assays,8647816,R43GM109503,"['Address', 'Affect', 'Algorithms', 'Area', 'Bioinformatics', 'Biological Assay', 'Biological Factors', 'Biological Markers', 'Biology', 'ChIP-seq', 'Clinical', 'Cloud Computing', 'Data', 'Data Analyses', 'Decision Trees', 'Development', 'Diagnostic', 'Dimensions', 'Effectiveness', 'Evaluation', 'Evolution', 'Genomics', 'Goals', 'Guidelines', 'In Vitro', 'Investigation', 'Lasso', 'Lead', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Performance', 'Phase', 'Play', 'Protocols documentation', 'Public Health', 'Publishing', 'ROC Curve', 'Reproducibility', 'Research Project Grants', 'Scheme', 'Services', 'Signal Transduction', 'Simulate', 'Small Business Innovation Research Grant', 'Source', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Statistical Models', 'Support System', 'Techniques', 'Technology', 'Therapeutic', 'Trees', 'Validation', 'analytical tool', 'base', 'clinical practice', 'cost', 'data mining', 'design', 'disease diagnosis', 'drug development', 'follow-up', 'forest', 'high throughput technology', 'improved', 'indexing', 'novel diagnostics', 'public health relevance', 'research study']",NIGMS,INSILICOS,R43,2014,131071,0,-0.019223590341765635
"The Cardiovascular Research Grid DESCRIPTION (provided by applicant):    The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinations of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotating ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and motion that can predict the early presence of developing heart disease in time for therapeutic intervention. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informatics system that allows clinical information to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG. RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8928685,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'High Performance Computing', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Therapeutic Intervention', 'Time', 'Ultrasonography', 'Work', 'base', 'cardiovascular imaging', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2014,159202,807432003,0.011527074521106306
"Face De-Identification for Research and Clinical Use     DESCRIPTION (provided by applicant): This application addresses NIH's call to promote data sharing and patient privacy. A major obstacle to sharing of recorded video has been the need to protect participants' identity. Similarly, concern about stigma is a reason that many people in need of mental health services (e.g., in the military) fail to do so. We propose a system to de-identify patients and research participants in video. Face de-identification transfers facial expression automatically from source face images, which are confidential, to target face images, which are not. The system safeguards face anonymity while preserving the facial expression of the original source video. The target video then can communicate the emotion, communicative intent, pain, and neurological or physiological status of the source person without displaying the source person's face. Face de-identification would enable video archive sharing among researchers and clinicians without compromising privacy or confidentiality. Moreover, a version of this system could potentially be used to preserve privacy and anonymity in internet-based interviews. Innovation. The project has four innovations. The approach (1) Removes identity information while retaining facial dynamics; thus preserving the information value of the face to communicate emotion, pain, and related states. (2) Accommodates subtle and spontaneous facial actions, rather than imitating only some predefined molar expressions (e.g., happy or sad). (3) Requires no training steps by target persons. (4) And requires no hand annotation of video. The system is entirely automatic. Approach. The software will take as input a video with the face of a subject (source) and automatically generate or output a video with the face de-identified. The project will use new machine learning and computer vision algorithms for transferring subtle facial expression from a source subject (original video) to a target subject, using only one frontal image of the target subject. A major novelty of the approach is to make the process completely automatic. The algorithm will be validated using commercially available software for face recognition and custom software for facial expression analysis.         PUBLIC HEALTH RELEVANCE: Sharing of video recordings for research and clinical uses would significantly contribute to scientific discovery and patient diagnosis, treatment, and evaluation. We will develop and validate a fully automatic system that preserves facial expression in video while fully protecting face identity.            ",Face De-Identification for Research and Clinical Use,8772435,R21MH099487,"['Address', 'Age', 'Algorithms', 'Archives', 'Behavioral Sciences', 'Clinical', 'Code', 'Communication', 'Communities', 'Computer Vision Systems', 'Computer software', 'Confidentiality', 'Custom', 'Data Set', 'Detection', 'Diagnosis', 'Education', 'Emotions', 'Evaluation', 'Face', 'Facial Expression', 'Facial Expression Recognition', 'Goals', 'Gold', 'Hand', 'Human', 'Image', 'Informed Consent', 'Intention', 'Internet', 'Interview', 'Judgment', 'Machine Learning', 'Maps', 'Measurement', 'Mental Health Services', 'Methods', 'Military Personnel', 'Modeling', 'Neurologic', 'Output', 'Pain', 'Participant', 'Patients', 'Perception', 'Personal Computers', 'Persons', 'Physiological', 'Privacy', 'Process', 'Research', 'Research Personnel', 'Running', 'Source', 'Step training', 'System', 'Testing', 'Training', 'Twin Multiple Birth', 'Video Recording', 'base', 'clinical practice', 'data sharing', 'graphical user interface', 'innovation', 'middle age', 'patient privacy', 'public health relevance', 'sex', 'social stigma', 'user-friendly', 'young adult']",NIMH,CARNEGIE-MELLON UNIVERSITY,R21,2014,194915,30434536,-0.049367195057728804
"Mobility Data Integration to Insight     DESCRIPTION (provided by applicant): Mobility is essential for human health. Regular physical activity helps prevent heart disease and stroke, relieves symptoms of depression, and promotes weight loss. Unfortunately, many conditions, such as cerebral palsy, osteoarthritis, and obesity, limit mobility at an enormous personal and societal cost. While vast amounts of data are available from hundreds of research labs and millions of smartphones, there is a dearth of methods for analyzing this massive, heterogeneous dataset.  We propose to establish the National Center for Mobility Data Integration to Insight (the Mobilize Center) to overcome the data science challenges facing mobility big data and biomedical big data in general. Our preliminary work identified four bottlenecks in data science, which drive four Data Science Research Cores.  The Cores include Biomechanical Modeling, Statistical Learning, Behavioral and Social Modeling, and Integrative Modeling and Prediction. Our Cores will produce novel methods to integrate diverse modeling modalities and gain insight from noisy, sparse, heterogeneous, and time-varying big data. Our data-sharing consortia, with clinical, research, and industry partners, will provide mobility data for over ten million people.  Three Driving Biomedical Problems will focus and validate our data science research.  The Mobilize Center will disseminate our novel data science tools to thousands of researchers and create a sustainable data-sharing consortium. We will train tens of thousands of scientists to use data science methods in biomedicine through our in-person and online educational programs. We will establish a cohesive, vibrant, and sustainable National Center through the leadership of an experienced executive team and will help unify the BD2K consortia through our Biomedical Computation Review publication and the Simtk.org resource portal.  The Mobilize Center will lay the groundwork for the next generation of data science systems and revolutionize diagnosis and treatment for millions of people affected by limited mobility.         PUBLIC HEALTH RELEVANCE:  Regular physical activity is essential for human health, yet a broad range of conditions impair mobility. This project will transform human movement research by developing tools for data analysis and creating software that will advance research to prevent, diagnose, and reduce impairments that limit human movement.            ",Mobility Data Integration to Insight,8775015,U54EB020405,"['Affect', 'Area', 'Automobile Driving', 'Behavioral', 'Behavioral Model', 'Big Data', 'Biomechanics', 'Biomedical Computing', 'Biomedical Research', 'Body Weight decreased', 'Cerebral Palsy', 'Child', 'Classification', 'Clinical Research', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Degenerative polyarthritis', 'Diabetes Mellitus', 'Diagnosis', 'Educational workshop', 'Elderly', 'Ethics', 'Exercise', 'Fellowship', 'Fostering', 'Gait', 'Health', 'Heart Diseases', 'Human', 'Impairment', 'Individual', 'Injury', 'Joints', 'Leadership', 'Limb structure', 'Machine Learning', 'Medical center', 'Methods', 'Mission', 'Modality', 'Modeling', 'Monitor', 'Movement', 'NCI Scholars Program', 'Nature', 'Obesity', 'Operative Surgical Procedures', 'Outcome', 'Overweight', 'Pathology', 'Persons', 'Physical activity', 'Prevention', 'Problem Solving', 'Public Health', 'Publications', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Running', 'Science', 'Scientist', 'Stroke', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Visit', 'Walking', 'Work', 'base', 'clinical decision-making', 'cognitive function', 'cohesion', 'cost', 'data integration', 'data modeling', 'data sharing', 'depressive symptoms', 'experience', 'flexibility', 'improved', 'industry partner', 'insight', 'models and simulation', 'next generation', 'novel', 'novel strategies', 'prevent', 'programs', 'public health relevance', 'role model', 'sensor', 'social', 'social model', 'tool']",NIBIB,STANFORD UNIVERSITY,U54,2014,209258,560644462,0.010724549121839193
"Computer Studies of Protein Structure and Function     DESCRIPTION (provided by applicant): The research described in the current proposal has the long-range goals of enabling the prediction of protein-protein interactions and the application of this knowledge to problems of biomedical relevance. An additional long-term goal is the fuller integration of Structural and Systems Biology. Specific Aims include: a) The development of three-dimensional structure-based methods to predict, on a genome-wide scale, whether and how two proteins interact. b) The integration of structural information with other sources of evidence as to protein-protein interactions. c) The application of the methods being developed to important biomedical problems. These research goals are motivated by a number of factors. First, cellular function is mediated by tens or even hundreds of thousands of protein-protein interactions yet these are generally hard to predict in advance or to measure accurately with high-throughput experimental techniques. A method that allows the computational prediction of such interactions would thus be of significant impact. Second, there are many more protein sequences than protein structures so it is necessary to find ways to amplify the information in protein databases if structure is to be fully integrated in genome scale research. The proposed research is intended to bridge this gap.  A central element of the approach to be taken is the use of structural alignments to reveal novel functional relationships between proteins. Since structure is better conserved than sequence, these alignments can reveal new information. A novel structure-based method is introduced which exploits homology models and remote geometric relationships to amplify structural information. The evidence that is obtained is then combined with non-structural sources of evidence using Bayesian networks to yield a probability of whether two proteins interact. An important element in the proposed research strategy is the recognition that structural modeling on a large scale is necessarily imprecise so that it is necessary to use low resolution ""scoring functions for a given model that do not depend sensitively on atomic detail. Bayesian methods then allow the extraction of a clear signal from the underlying noise.  The biological impact of the research will be enhanced through computational/experimental applications in areas where structural clues have the potential of discovering truly novel interactions. Validated predictions on adhesion proteins, nuclear hormone receptors and cytokine signaling proteins demonstrate the efficacy of the methodology. New applications to viral host interactions and to cancer signaling pathways are described.         PUBLIC HEALTH RELEVANCE: The health relatedness of the proposed research arises from the fact that a better understanding of protein- protein interactions opens the way to a better understanding of the biology underlying human disease. Applications to interactions involved in viral-host interactions, and in cancer-related pathways serve to highlight this aspect of the proposed research.            ",Computer Studies of Protein Structure and Function,8588329,R01GM030518,"['Adhesions', 'Amino Acid Sequence', 'Area', 'Bayesian Method', 'Binding', 'Biological', 'Biology', 'Birds', 'Cell physiology', 'Chromosomes', 'Collaborations', 'Complex', 'Conserved Sequence', 'Cytokine Signaling', 'Data', 'Databases', 'Development', 'Elements', 'Epigenetic Process', 'Family', 'Family suidae', 'Funding', 'Gene Fusion', 'Genetic screening method', 'Genome', 'Goals', 'Health', 'Homology Modeling', 'Human', 'Human Genome', 'Indium', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Mediating', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Noise', 'Nuclear Hormone Receptors', 'Pathway interactions', 'Peptide Sequence Determination', 'Peptides', 'Phylogenetic Analysis', 'Probability', 'Process', 'Protein Region', 'Protein Structure Databases', 'Proteins', 'RNA Interference', 'Research', 'Resolution', 'Sequence Alignment', 'Sequence Homology', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'Signaling Protein', 'Site', 'Source', 'Specificity', 'Structural Models', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Tropism', 'Viral', 'Viral Proteins', 'Virus', 'Work', 'Yeasts', 'base', 'computer based statistical methods', 'computer infrastructure', 'computer studies', 'computerized tools', 'design', 'genome-wide', 'human disease', 'improved', 'influenzavirus', 'innovation', 'interest', 'method development', 'novel', 'programs', 'protein complex', 'protein function', 'protein protein interaction', 'protein structure', 'protein structure function', 'public health relevance', 'success', 'three dimensional structure', 'tool', 'tumor progression', 'tumorigenesis', 'virus host interaction']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2014,291243,558628098,0.04145083894710805
"Refinement Methods for Protein Docking based on Exploring Multi-Dimensional Energ DESCRIPTION (provided by applicant): All successful state-of-the-art protein docking methods employ a so called multistage approach. At the first stage of such approaches a rough energy potential is used to score billions of conformations. At a second stage, thousands of conformations with the best scores are retained and clustered based on a certain similarity metric. Cluster centers correspond to putative predictions/models. Recent work by the proposing team demonstrated that greater prediction quality can be achieved by properly exploring these clusters through a process called refinement. This work resulted in the development of a prototype refinement approach - the Semi-Definite programming-based Underestimation method (SDU). The central goal of the project is to build on the SDU success and develop a new high-throughput refinement protocol able to produce predictions of near-crystallographic quality in the most computationally efficient manner. Efficiency will be achieved by leveraging the funnel-like shape that binding free energy potentials exhibit. The specific aims are: (1) the development of a new clustering method that can classify the conformations retained from a first-stage method into clusters suitable for the proposed refinement strategy; (2) the characterization of the structure of the multi-dimensional funnel corresponding to each cluster and the development of an efficient refinement strategy to explore this funnel; (3) the development of a side-chain positioning algorithm appropriate for docking by leveraging Markov random field theory; and (4) the dissemination of the algorithms developed through the release to the research community of a software package and an automated refinement server. It is anticipated that the computational efficiency gains of the proposed refinement protocol over alternative Monte Carlo methods will exceed two orders of magnitude, while, at the same time, significantly improve upon the accuracy achieved by earlier refinement approaches. A novelty of the proposed work is in its use of sophisticated machinery from the fields of optimization and decision theory specially tailored to the biophysical properties of the docking problem. Techniques from convex and combinatorial optimization, machine learning, and Markov random fields are brought to bear on the refinement stage of multistage protein docking approaches. An important element of the work is the systematic characterization of multi-dimensional binding energy funnels. The existence of such funnels has been long conjectured but it has not led to new docking approaches so far. The proposed algorithms essentially achieve this goal by devising efficient strategies to identify, characterize, and explore these funnels. PUBLIC HEALTH RELEVANCE: This work will substantially improve upon computational methods for characterizing and predicting protein- protein interactions. It will enable treating relatively weak protein complexes involving larger proteins than what is possible today. This will result in a better understanding of processes such as metabolic control, immune response, signal transduction, and gene regulation.",Refinement Methods for Protein Docking based on Exploring Multi-Dimensional Energ,8633467,R01GM093147,"['Accounting', 'Address', 'Adopted', 'Algorithms', 'Benchmarking', 'Binding', 'Biological', 'Combinatorial Optimization', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Data Quality', 'Decision Theory', 'Development', 'Dill', 'Dimensions', 'Discrimination', 'Docking', 'Electrostatics', 'Elements', 'Evaluation', 'Exhibits', 'Fourier Transform', 'Free Energy', 'Gene Expression Regulation', 'Generations', 'Geometry', 'Goals', 'Grant', 'Hand', 'Health', 'Immune response', 'Knowledge', 'Lead', 'Libraries', 'Ligands', 'Link', 'Machine Learning', 'Maintenance', 'Maps', 'Metabolic Control', 'Methods', 'Metric', 'Modeling', 'Molecular Conformation', 'Monte Carlo Method', 'Motion', 'Motivation', 'Movement', 'National Institute of General Medical Sciences', 'Paper', 'Pathway interactions', 'Plant Roots', 'Positioning Attribute', 'Potential Energy', 'Probability', 'Process', 'Proteins', 'Protocols documentation', 'Published Comment', 'Reporting', 'Research', 'Rotation', 'Sampling', 'Scoring Method', 'Shapes', 'Side', 'Signal Transduction', 'Simulate', 'Staging', 'Structure', 'Techniques', 'Time', 'Transduction Gene', 'Translating', 'Translations', 'Ursidae Family', 'Vertebral column', 'Work', 'Writing', 'base', 'biophysical properties', 'cost', 'experience', 'flexibility', 'improved', 'interest', 'programs', 'protein complex', 'protein folding', 'protein protein interaction', 'prototype', 'receptor', 'research study', 'screening', 'success', 'theories']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2014,316023,61050884,0.02648025775028853
"Very large datasets and new models to predict and design protein interactions PROJECT SUMMARY  Specific protein-protein interactions are responsible for organizing the cell, for processing biological signals and information, and for the chemistry of life. Thus, understanding biological mechanism relies on understanding the interactions that occur between proteins. An important long-term goal is to develop methods for reliably predicting and rationally modifying protein-protein interactions. Such capabilities would  provide insight into the molecular details of pathology and highlight opportunities for disease treatment. This proposal describes an integrated experimental/computational technology platform that will provide predictive models of protein interaction specificity. The experimental component involves constructing randomized libraries of proteins or peptides that will be sorted according to their affinities for binding a particular  receptor. The identities and binding affinities for very large numbers of library members will be decoded using high-throughput sequencing methods. The data, consisting of up to 107 {sequence, affinity} pairs per sequencing run, will be used as input to computational machine learning methods. Models will be generated that capture the relationship between sequence and interactions, and the predictive power of these models  will be tested experimentally. The work described in this proposal emphasizes technology development and application of the new platform to study two general types of protein complexes. First are interactions of short helical ligands with mid-sized globular proteins, here studied using anti-apoptotic Bcl-2 and Ca2+- binding EF-hand proteins. Second are interactions of short linear peptides with modular interaction  domains, here PDZ and SH3 domains. These four protein families mediate an enormous number of important molecular recognition events in human cells, and the resulting models will provide valuable support to study of their biological functions. This work will also provide a stringent test of the capabilities of the proposed technology, which can then be applied to a much wider variety of molecular complexes, e.g., protein-protein, protein-small molecule and protein-nucleic acid assemblies. Given the paucity of high-  throughput methods for accurately measuring protein-protein interactions, and the primitive capabilities of most computational models for predicting protein binding, the proposed technology platform has the potential to dramatically transform the study of protein interaction specificity. Relevance  Specific protein-protein interactions underlie all biological processes. Knowledge of interactions that occur in healthy vs. diseased tissues, coupled with methods for inhibiting such interactions, would dramatically expand opportunities to treat human disease. This proposal describes a new technology for advancing the measurement, prediction and design of protein complexes.",Very large datasets and new models to predict and design protein interactions,8722570,R01GM096466,"['Affinity', 'Apoptotic', 'BCL2 gene', 'Binding', 'Biological', 'Biological Assay', 'Biological Process', 'Calcium Binding', 'Cell physiology', 'Cells', 'Chemistry', 'Color', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Coupled', 'Data', 'Data Set', 'Disease', 'EF Hand Motifs', 'Event', 'Family', 'Fluorescence-Activated Cell Sorting', 'Goals', 'High-Throughput Nucleotide Sequencing', 'Human', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Libraries', 'Life', 'Ligands', 'Machine Learning', 'Measurement', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nucleic Acids', 'Pathology', 'Peptides', 'Performance', 'Phenotype', 'Property', 'Protein Binding', 'Protein Family', 'Proteins', 'Randomized', 'Running', 'SH3 Domains', 'Signal Transduction', 'Sorting - Cell Movement', 'Specificity', 'Staging', 'Surface', 'Technology', 'Testing', 'Tissues', 'Training', 'Work', 'Yeasts', 'base', 'combinatorial', 'design', 'globular protein', 'human disease', 'indexing', 'insight', 'member', 'molecular recognition', 'new technology', 'novel', 'novel strategies', 'predictive modeling', 'protein complex', 'protein protein interaction', 'receptor', 'screening', 'small molecule', 'technology development']",NIGMS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2014,394693,113554200,0.03634529869857145
"A Software Platform for Sensor-based Movement Disorder Recognition     DESCRIPTION (provided by applicant): The overall objective of this SBIR project is to develop a pre-commercial prototype system capable of continuously monitoring involuntary movement disorders from a wide spectrum of neurological conditions. The impact of this innovation will enhance the availability of advanced brain and behavior research tools [PA- 11-134] by providing a continuous means of tracking the presence and severity of movement disorders during normal daily activities. This project will transform our unique movement disorder recognition algorithms into custom software that analyzes movement disorders for specific neurological conditions. The information obtained from body worn sensors will provide an accurate and objective means for assessing the complex and changeable nature of movement disorders. This goal cannot by realized using the current method of self-report questionnaires.  The research strategy for Phase I will establish the merit and feasibility of this effort by developing an Application Generation (AG) software platform using a framework of configurable signal processing modules to generate custom applications for movement disorder analysis (Aim 1). This approach reduces the effort and enhances the flexibility of designing and testing software solutions for these applications. The AG Platform will be developed using C++ software to implement signal processing and machine-learning software modules that operate within a knowledge-based framework that we have previously developed. In Aim 2 we will utilize the AG platform to generate movement disorder analysis software to evaluate a challenging test-case application: freezing-of-gait in Parkinson's disease (PD). The goal is to attain performance metrics for freezing that are comparable to those we have achieved for tremor and dyskinesia in previous efforts.  Phase II will refine the capabilities of the AG platform developed in Phase I. We will augment it with the means to automatically design and train the machine learning algorithms, improve the user-interface, and provide options for viewing and summarizing the results. The improved AG platform will be used to develop customized disorder-analysis software that encompasses the full complement of movement disorders associated with PD (e.g. dystonia, bradykinesia, Parkinsonian gait, tremor, dyskinesia), as well as for other neurological condition, such as Essential Tremor (ET). Firmware will be developed for each custom application to efficiently integrate the analytic software with our existing Trigno wireless sensor data acquisition hardware, which needs to be streamlined for this application. This combined system will be evaluated under research use-case scenarios in Neurology. Phase II will deliver an ambulatory Movement Disorder Monitoring system that not only succeeds in providing state-of-the-art monitoring solutions for PD and Essential Tremor, but has proven technology to develop monitoring solutions for a wide variety of neurological conditions. Future development will transfer this technology to a clinical version of this system.         PUBLIC HEALTH RELEVANCE: The project is intended to improve the accuracy and reduce the burden of researchers and clinicians when assessing motor outcomes for patients with involuntary movement disorders. The proposed technology will provide a means for evaluating new treatment options, and expedite the delivery of care. The attainment of these goals should increase the effectiveness of research, the time required for these research advancements to reach the consumer, and help control the rising costs of clinical care among the estimated 45 million Americans who have neurological disorders resulting in involuntary movements. The resulting improvements in motor function will ultimately lead to greater independence and productivity for this growing segment of the population.            ",A Software Platform for Sensor-based Movement Disorder Recognition,8734495,R43NS083098,"['Affect', 'Algorithms', 'American', 'Bradykinesia', 'Clinical', 'Code', 'Communities', 'Complement', 'Complex', 'Computer software', 'Custom', 'Data', 'Development', 'Disease', 'Dyskinetic syndrome', 'Dystonia', 'Early Diagnosis', 'Essential Tremor', 'Freezing', 'Future', 'Gait', 'Generations', 'Goals', 'Health Care Sector', 'Health Professional', 'Home environment', 'Involuntary Movements', 'Lead', 'Learning Module', 'Left', 'Lower Extremity', 'Machine Learning', 'Methods', 'Metric', 'Miniaturization', 'Monitor', 'Motor', 'Movement Disorders', 'Nature', 'Neurologic', 'Neurology', 'Outcome', 'Parkinson Disease', 'Parkinsonian Disorders', 'Patient Monitoring', 'Patient Self-Report', 'Patients', 'Performance', 'Persons', 'Phase', 'Population', 'Process', 'Productivity', 'Quality of life', 'Questionnaires', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Severities', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Solutions', 'Surface', 'System', 'Technology', 'Technology Transfer', 'Testing', 'Time', 'Training', 'Tremor', 'Video Recording', 'Wireless Technology', 'Work', 'Writing', 'advanced system', 'base', 'brain behavior', 'care delivery', 'clinical care', 'cost', 'data acquisition', 'design', 'disorder control', 'effectiveness research', 'flexibility', 'handheld mobile device', 'human subject', 'improved', 'innovation', 'knowledge base', 'motor disorder', 'nervous system disorder', 'novel therapeutics', 'prototype', 'public health relevance', 'research study', 'response', 'sensor', 'signal processing', 'software development', 'tool']",NINDS,"ALTEC, INC.",R43,2014,417876,1532918,-0.0028535091336852948
"Interactive Informatics Resource for Research-driven Cancer Proteomics     DESCRIPTION (provided by applicant):  In 2013 over 1.6 million new cases of cancer are expected to be diagnosed and over 580,000 people are expected to die of the disease. Thus, continued research in the identification of new diagnostic and prognostic biomarkers of cancer is necessary. Although cancer is widely recognized as a genomic disease, the directives of the DNA-based drivers are executed at the level of proteins and their biological functions, and the application of potential protein level biomarkers remains a compelling vision. Thus, a large investment has been made by NCI and other research centers in high-throughput global proteomics experiments to mine for novel biomarkers of cancer. However, few of these markers have come to fruition. We believe that one of the major challenges to the discovery of robust protein- or pathway-biomarker candidates from these large and complex proteomics datasets is due to naive data analysis approaches that do not take into account the underlying complexity of the proteome (e.g., splice variants, post- translational modifications). State-of-the-art statistical algorithms to improve the tasks of quality assessment, peptide and protein quantification, and pathway modeling that are designed to account for the design of the experiment have been developed; however access to these methodologies by the larger community is hindered since they are in the prototype stage and typically require knowledge of statistical programming. Furthermore, the likelihood of these tools moving to robust software is low since they are developed within the context of existing grants that do not support the transition from prototype to software. For the field of clinical proteomics to successfully identif new mechanistic etiologies of cancer requires not only high quality data with respect to the instrument, but also high quality statistical analysis of the data. This project proposes new informatics technology in the form of a robust, interactive and cross- platform software environment that will enable biomedical and biological scientists to perform in-depth analyses of global proteomics data from the point of quality assessment and normalization of raw inferred abundances (e.g., peak area) to the identification of protein biomarkers and enriched pathways. The software will be designed in a single programming language (Java) to assure easy installation across platforms with wizard-based data entry and advanced data reporting. Java will also support the development of advanced graphical user interfaces for data presentation and interactive graphics with a modern look and feel. This approach will ensure that scientists outside of the development institution can develop modules to include in the software or extensions for data integration without challenges of re-compiling the application. The software modules to be developed under this project are Aim 1) peptide and protein level quality assessment and quantification, Aim 2) protein biomarker discovery via exploratory data analysis and machine learning, and Aim 3) pathway biomarker discovery through integration with the NCI Protein Interaction Database.         PUBLIC HEALTH RELEVANCE:  For the past decade, cancer researchers have been utilizing global proteomics analyses to extensively categorize proteins and other molecular species in hopes of identifying distinctive features of cancer cells that not only explain the biology, but alo enable better patient care. Despite these investments, relatively few protein biomarkers have achieved clinical validation largely due to naive data analysis strategies used in the protein quantification and statistical validation of candidate biomarkers. This project will develop a robust user- friendly software environment that builds upon state-of-the-art statistical algorithms that are focused on addressing the underlying proteome complexity associated with cancer.            ",Interactive Informatics Resource for Research-driven Cancer Proteomics,8685758,U01CA184783,"['Accounting', 'Address', 'Advanced Development', 'Algorithms', 'Area', 'Biological', 'Biological Markers', 'Biological Process', 'Biology', 'Breast', 'Cancer Etiology', 'Clinical', 'Code', 'Collection', 'Communities', 'Complex', 'Computer software', 'DNA', 'Data', 'Data Analyses', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Ensure', 'Environment', 'Experimental Designs', 'Funding', 'Genes', 'Genomics', 'Genotype', 'Goals', 'Grant', 'Imagery', 'Individual', 'Informatics', 'Institutes', 'Institution', 'Investments', 'Java', 'Knowledge', 'Label', 'Letters', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular', 'National Institute of Allergy and Infectious Disease', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Ovarian', 'Pathway interactions', 'Patient Care', 'Pattern', 'Peptide Mapping', 'Peptides', 'Phenotype', 'Post-Translational Protein Processing', 'Process', 'Prognostic Marker', 'Programming Languages', 'Protein Fragment', 'Proteins', 'Proteome', 'Proteomics', 'RNA Splicing', 'Research', 'Research Personnel', 'Resource Informatics', 'Resources', 'Sampling', 'Scientist', 'Source Code', 'Specificity', 'Staging', 'Statistical Algorithm', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Sum', 'Technology', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Variant', 'Vision', 'Visual', 'base', 'cancer cell', 'cancer proteomics', 'candidate validation', 'computerized data processing', 'computerized tools', 'data integration', 'design', 'experience', 'graphical user interface', 'improved', 'instrument', 'link protein', 'mathematical methods', 'model design', 'novel', 'novel diagnostics', 'prognostic', 'programs', 'protein expression', 'prototype', 'public health relevance', 'research study', 'software development', 'statistics', 'tool', 'user friendly software']",NCI,BATTELLE PACIFIC NORTHWEST LABORATORIES,U01,2014,419855,19252132,-0.0014197853312406057
"The Cardiovascular Research Grid DESCRIPTION (provided by applicant):    The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinations of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotating ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and motion that can predict the early presence of developing heart disease in time for therapeutic intervention. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informatics system that allows clinical information to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG.  RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8588958,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'High Performance Computing', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Therapeutic Intervention', 'Time', 'Ultrasonography', 'Work', 'base', 'cardiovascular imaging', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2014,2177431,807432003,0.011527074521106306
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8640966,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2014,2699376,560644462,0.00216607227130351
"Computer Studies of Protein Structure and Function     DESCRIPTION (provided by applicant): The research described in the current proposal has the long-range goals of enabling the prediction of protein-protein interactions and the application of this knowledge to problems of biomedical relevance. An additional long-term goal is the fuller integration of Structural and Systems Biology. Specific Aims include: a) The development of three-dimensional structure-based methods to predict, on a genome-wide scale, whether and how two proteins interact. b) The integration of structural information with other sources of evidence as to protein-protein interactions. c) The application of the methods being developed to important biomedical problems. These research goals are motivated by a number of factors. First, cellular function is mediated by tens or even hundreds of thousands of protein-protein interactions yet these are generally hard to predict in advance or to measure accurately with high-throughput experimental techniques. A method that allows the computational prediction of such interactions would thus be of significant impact. Second, there are many more protein sequences than protein structures so it is necessary to find ways to amplify the information in protein databases if structure is to be fully integrated in genome scale research. The proposed research is intended to bridge this gap.  A central element of the approach to be taken is the use of structural alignments to reveal novel functional relationships between proteins. Since structure is better conserved than sequence, these alignments can reveal new information. A novel structure-based method is introduced which exploits homology models and remote geometric relationships to amplify structural information. The evidence that is obtained is then combined with non-structural sources of evidence using Bayesian networks to yield a probability of whether two proteins interact. An important element in the proposed research strategy is the recognition that structural modeling on a large scale is necessarily imprecise so that it is necessary to use low resolution ""scoring functions for a given model that do not depend sensitively on atomic detail. Bayesian methods then allow the extraction of a clear signal from the underlying noise.  The biological impact of the research will be enhanced through computational/experimental applications in areas where structural clues have the potential of discovering truly novel interactions. Validated predictions on adhesion proteins, nuclear hormone receptors and cytokine signaling proteins demonstrate the efficacy of the methodology. New applications to viral host interactions and to cancer signaling pathways are described.         PUBLIC HEALTH RELEVANCE: The health relatedness of the proposed research arises from the fact that a better understanding of protein- protein interactions opens the way to a better understanding of the biology underlying human disease. Applications to interactions involved in viral-host interactions, and in cancer-related pathways serve to highlight this aspect of the proposed research.            ",Computer Studies of Protein Structure and Function,8437358,R01GM030518,"['Adhesions', 'Amino Acid Sequence', 'Area', 'Bayesian Method', 'Binding', 'Biological', 'Biology', 'Birds', 'Cell physiology', 'Chromosomes', 'Collaborations', 'Complex', 'Conserved Sequence', 'Cytokine Signaling', 'Data', 'Databases', 'Development', 'Elements', 'Epigenetic Process', 'Family', 'Family suidae', 'Funding', 'Fungal Genome', 'Gene Fusion', 'Genetic screening method', 'Genome', 'Goals', 'Health', 'Homology Modeling', 'Human', 'Human Genome', 'Indium', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Mediating', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Noise', 'Nuclear Hormone Receptors', 'Pathway interactions', 'Peptide Sequence Determination', 'Peptides', 'Phylogenetic Analysis', 'Probability', 'Process', 'Protein Region', 'Protein Structure Databases', 'Proteins', 'RNA Interference', 'Research', 'Resolution', 'Sequence Alignment', 'Sequence Homology', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'Signaling Protein', 'Site', 'Source', 'Specificity', 'Structural Models', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Tropism', 'Viral', 'Viral Proteins', 'Virus', 'Work', 'base', 'computer based statistical methods', 'computer infrastructure', 'computer studies', 'computerized tools', 'design', 'genome-wide', 'human disease', 'improved', 'influenzavirus', 'innovation', 'interest', 'method development', 'novel', 'programs', 'protein complex', 'protein function', 'protein protein interaction', 'protein structure', 'protein structure function', 'public health relevance', 'success', 'three dimensional structure', 'tool', 'tumor progression', 'tumorigenesis', 'virus host interaction']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2013,290550,558628098,0.04145083894710805
"Refinement Methods for Protein Docking based on Exploring Multi-Dimensional Energ DESCRIPTION (provided by applicant): All successful state-of-the-art protein docking methods employ a so called multistage approach. At the first stage of such approaches a rough energy potential is used to score billions of conformations. At a second stage, thousands of conformations with the best scores are retained and clustered based on a certain similarity metric. Cluster centers correspond to putative predictions/models. Recent work by the proposing team demonstrated that greater prediction quality can be achieved by properly exploring these clusters through a process called refinement. This work resulted in the development of a prototype refinement approach - the Semi-Definite programming-based Underestimation method (SDU). The central goal of the project is to build on the SDU success and develop a new high-throughput refinement protocol able to produce predictions of near-crystallographic quality in the most computationally efficient manner. Efficiency will be achieved by leveraging the funnel-like shape that binding free energy potentials exhibit. The specific aims are: (1) the development of a new clustering method that can classify the conformations retained from a first-stage method into clusters suitable for the proposed refinement strategy; (2) the characterization of the structure of the multi-dimensional funnel corresponding to each cluster and the development of an efficient refinement strategy to explore this funnel; (3) the development of a side-chain positioning algorithm appropriate for docking by leveraging Markov random field theory; and (4) the dissemination of the algorithms developed through the release to the research community of a software package and an automated refinement server. It is anticipated that the computational efficiency gains of the proposed refinement protocol over alternative Monte Carlo methods will exceed two orders of magnitude, while, at the same time, significantly improve upon the accuracy achieved by earlier refinement approaches. A novelty of the proposed work is in its use of sophisticated machinery from the fields of optimization and decision theory specially tailored to the biophysical properties of the docking problem. Techniques from convex and combinatorial optimization, machine learning, and Markov random fields are brought to bear on the refinement stage of multistage protein docking approaches. An important element of the work is the systematic characterization of multi-dimensional binding energy funnels. The existence of such funnels has been long conjectured but it has not led to new docking approaches so far. The proposed algorithms essentially achieve this goal by devising efficient strategies to identify, characterize, and explore these funnels. PUBLIC HEALTH RELEVANCE: This work will substantially improve upon computational methods for characterizing and predicting protein- protein interactions. It will enable treating relatively weak protein complexes involving larger proteins than what is possible today. This will result in a better understanding of processes such as metabolic control, immune response, signal transduction, and gene regulation.",Refinement Methods for Protein Docking based on Exploring Multi-Dimensional Energ,8450066,R01GM093147,"['Accounting', 'Address', 'Adopted', 'Algorithms', 'Benchmarking', 'Binding', 'Biological', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Data Quality', 'Decision Theory', 'Development', 'Dill', 'Dimensions', 'Discrimination', 'Docking', 'Electrostatics', 'Elements', 'Evaluation', 'Exhibits', 'Fourier Transform', 'Free Energy', 'Gene Expression Regulation', 'Generations', 'Goals', 'Grant', 'Hand', 'Health', 'Immune response', 'Knowledge', 'Lead', 'Libraries', 'Ligands', 'Link', 'Machine Learning', 'Maintenance', 'Maps', 'Metabolic Control', 'Methods', 'Metric', 'Modeling', 'Molecular Conformation', 'Monte Carlo Method', 'Motion', 'Motivation', 'Movement', 'National Institute of General Medical Sciences', 'Paper', 'Pathway interactions', 'Plant Roots', 'Positioning Attribute', 'Potential Energy', 'Probability', 'Process', 'Property', 'Proteins', 'Protocols documentation', 'Published Comment', 'Reporting', 'Research', 'Rotation', 'Sampling', 'Scoring Method', 'Shapes', 'Side', 'Signal Transduction', 'Simulate', 'Staging', 'Structure', 'Techniques', 'Time', 'Transduction Gene', 'Translating', 'Translations', 'Ursidae Family', 'Vertebral column', 'Work', 'Writing', 'base', 'combinatorial', 'cost', 'experience', 'flexibility', 'improved', 'interest', 'programs', 'protein complex', 'protein folding', 'protein protein interaction', 'prototype', 'receptor', 'research study', 'screening', 'success', 'theories']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2013,304962,61050884,0.02648025775028853
"Very large datasets and new models to predict and design protein interactions PROJECT SUMMARY  Specific protein-protein interactions are responsible for organizing the cell, for processing biological signals and information, and for the chemistry of life. Thus, understanding biological mechanism relies on understanding the interactions that occur between proteins. An important long-term goal is to develop methods for reliably predicting and rationally modifying protein-protein interactions. Such capabilities would  provide insight into the molecular details of pathology and highlight opportunities for disease treatment. This proposal describes an integrated experimental/computational technology platform that will provide predictive models of protein interaction specificity. The experimental component involves constructing randomized libraries of proteins or peptides that will be sorted according to their affinities for binding a particular  receptor. The identities and binding affinities for very large numbers of library members will be decoded using high-throughput sequencing methods. The data, consisting of up to 107 {sequence, affinity} pairs per sequencing run, will be used as input to computational machine learning methods. Models will be generated that capture the relationship between sequence and interactions, and the predictive power of these models  will be tested experimentally. The work described in this proposal emphasizes technology development and application of the new platform to study two general types of protein complexes. First are interactions of short helical ligands with mid-sized globular proteins, here studied using anti-apoptotic Bcl-2 and Ca2+- binding EF-hand proteins. Second are interactions of short linear peptides with modular interaction  domains, here PDZ and SH3 domains. These four protein families mediate an enormous number of important molecular recognition events in human cells, and the resulting models will provide valuable support to study of their biological functions. This work will also provide a stringent test of the capabilities of the proposed technology, which can then be applied to a much wider variety of molecular complexes, e.g., protein-protein, protein-small molecule and protein-nucleic acid assemblies. Given the paucity of high-  throughput methods for accurately measuring protein-protein interactions, and the primitive capabilities of most computational models for predicting protein binding, the proposed technology platform has the potential to dramatically transform the study of protein interaction specificity. Relevance  Specific protein-protein interactions underlie all biological processes. Knowledge of interactions that occur in healthy vs. diseased tissues, coupled with methods for inhibiting such interactions, would dramatically expand opportunities to treat human disease. This proposal describes a new technology for advancing the measurement, prediction and design of protein complexes.",Very large datasets and new models to predict and design protein interactions,8538461,R01GM096466,"['Affinity', 'Apoptotic', 'BCL2 gene', 'Binding', 'Biological', 'Biological Assay', 'Biological Process', 'Calcium Binding', 'Cell physiology', 'Cells', 'Chemistry', 'Color', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Coupled', 'Data', 'Data Set', 'Disease', 'EF Hand Motifs', 'Event', 'Family', 'Fluorescence-Activated Cell Sorting', 'Goals', 'Human', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Libraries', 'Life', 'Ligands', 'Machine Learning', 'Measurement', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nucleic Acids', 'Pathology', 'Peptides', 'Performance', 'Phenotype', 'Property', 'Protein Binding', 'Protein Family', 'Proteins', 'Randomized', 'Running', 'SH3 Domains', 'Signal Transduction', 'Sorting - Cell Movement', 'Specificity', 'Staging', 'Surface', 'Technology', 'Testing', 'Tissues', 'Training', 'Work', 'Yeasts', 'base', 'combinatorial', 'design', 'globular protein', 'human disease', 'indexing', 'insight', 'member', 'molecular recognition', 'new technology', 'novel', 'novel strategies', 'predictive modeling', 'protein complex', 'protein protein interaction', 'receptor', 'screening', 'small molecule', 'technology development']",NIGMS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2013,387824,113554200,0.03634529869857145
"A Software Platform for Sensor-based Movement Disorder Recognition     DESCRIPTION (provided by applicant): The overall objective of this SBIR project is to develop a pre-commercial prototype system capable of continuously monitoring involuntary movement disorders from a wide spectrum of neurological conditions. The impact of this innovation will enhance the availability of advanced brain and behavior research tools [PA- 11-134] by providing a continuous means of tracking the presence and severity of movement disorders during normal daily activities. This project will transform our unique movement disorder recognition algorithms into custom software that analyzes movement disorders for specific neurological conditions. The information obtained from body worn sensors will provide an accurate and objective means for assessing the complex and changeable nature of movement disorders. This goal cannot by realized using the current method of self-report questionnaires.  The research strategy for Phase I will establish the merit and feasibility of this effort by developing an Application Generation (AG) software platform using a framework of configurable signal processing modules to generate custom applications for movement disorder analysis (Aim 1). This approach reduces the effort and enhances the flexibility of designing and testing software solutions for these applications. The AG Platform will be developed using C++ software to implement signal processing and machine-learning software modules that operate within a knowledge-based framework that we have previously developed. In Aim 2 we will utilize the AG platform to generate movement disorder analysis software to evaluate a challenging test-case application: freezing-of-gait in Parkinson's disease (PD). The goal is to attain performance metrics for freezing that are comparable to those we have achieved for tremor and dyskinesia in previous efforts.  Phase II will refine the capabilities of the AG platform developed in Phase I. We will augment it with the means to automatically design and train the machine learning algorithms, improve the user-interface, and provide options for viewing and summarizing the results. The improved AG platform will be used to develop customized disorder-analysis software that encompasses the full complement of movement disorders associated with PD (e.g. dystonia, bradykinesia, Parkinsonian gait, tremor, dyskinesia), as well as for other neurological condition, such as Essential Tremor (ET). Firmware will be developed for each custom application to efficiently integrate the analytic software with our existing Trigno wireless sensor data acquisition hardware, which needs to be streamlined for this application. This combined system will be evaluated under research use-case scenarios in Neurology. Phase II will deliver an ambulatory Movement Disorder Monitoring system that not only succeeds in providing state-of-the-art monitoring solutions for PD and Essential Tremor, but has proven technology to develop monitoring solutions for a wide variety of neurological conditions. Future development will transfer this technology to a clinical version of this system.         PUBLIC HEALTH RELEVANCE: The project is intended to improve the accuracy and reduce the burden of researchers and clinicians when assessing motor outcomes for patients with involuntary movement disorders. The proposed technology will provide a means for evaluating new treatment options, and expedite the delivery of care. The attainment of these goals should increase the effectiveness of research, the time required for these research advancements to reach the consumer, and help control the rising costs of clinical care among the estimated 45 million Americans who have neurological disorders resulting in involuntary movements. The resulting improvements in motor function will ultimately lead to greater independence and productivity for this growing segment of the population.            ",A Software Platform for Sensor-based Movement Disorder Recognition,8521782,R43NS083098,"['Affect', 'Algorithms', 'American', 'Bradykinesia', 'Clinical', 'Code', 'Communities', 'Complement', 'Complex', 'Computer software', 'Custom', 'Data', 'Development', 'Devices', 'Disease', 'Dyskinetic syndrome', 'Dystonia', 'Early Diagnosis', 'Essential Tremor', 'Freezing', 'Future', 'Gait', 'Generations', 'Goals', 'Health Care Sector', 'Health Professional', 'Home environment', 'Involuntary Movements', 'Lead', 'Left', 'Lower Extremity', 'Machine Learning', 'Methods', 'Metric', 'Miniaturization', 'Monitor', 'Motor', 'Movement Disorders', 'Nature', 'Neurologic', 'Neurology', 'Outcome', 'Parkinson Disease', 'Parkinsonian Disorders', 'Patient Monitoring', 'Patient Self-Report', 'Patients', 'Performance', 'Persons', 'Phase', 'Population', 'Process', 'Productivity', 'Quality of life', 'Questionnaires', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Severities', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Solutions', 'Surface', 'System', 'Technology', 'Technology Transfer', 'Testing', 'Time', 'Training', 'Tremor', 'Video Recording', 'Wireless Technology', 'Work', 'Writing', 'advanced system', 'base', 'brain behavior', 'care delivery', 'clinical care', 'computerized data processing', 'cost', 'data acquisition', 'design', 'disorder control', 'effectiveness research', 'flexibility', 'human subject', 'improved', 'innovation', 'knowledge base', 'motor disorder', 'nervous system disorder', 'novel therapeutics', 'prototype', 'public health relevance', 'research study', 'response', 'sensor', 'software development', 'tool']",NINDS,"ALTEC, INC.",R43,2013,424766,1532918,-0.0028535091336852948
"Automatic 3D Quantification of Synapse Distribution in Complex Dendritic Arbor     DESCRIPTION (provided by applicant): The subcellular distribution of synapses is critical for the assembly, function, and plasticity of the nervous system and plays a role in its disorders. Underlying molecular mechanisms, however, remain largely unknown. While advanced multidimensional images, in conjunction with single-cell genetic techniques, have afforded an unprecedented opportunity to understand synapse development at a new level, there is a knowledge gap in our capacity to effectively quantify subcellular synapses from large quantities of three-dimensional images. This is a significant problem and has hampered large-scale studies of the molecular mechanisms of synapse development, especially in neurons with complex arbor-such as Purkinje cells in mammals and lobula plate tangential cells (LPTC) in Drosophila-where existing approaches do not yield complete or robust synapse quantification for the entire dendritic tree and do not scale to efficient genetic screening. The objective of thi project is to bridge this gap by providing tools for quantitative investigation of subcellular synapse distribution and its molecular mechanisms using three-dimensional microscopy images. Specifically, our highly cross- disciplinary team will pursue two aims: (1) Develop automatic algorithms to analyze and quantify synapse distribution in the entire dendritic tree of neurons with complex arbor. Holistic and objective description of synapse density will enable automatic detection of mutant patterns. (2) Develop automatic algorithms to analyze and quantify synapse distribution in different parts of the entire dendritic tree of neurons with complex arbor. Efficient quantification at distinct subcellular locations will assist discovery of novel regulators for different subcellular parts. As a test case, we will use synapse distribution n Drosophila LPTC neurons, which are amenable to both genome-wide genetic screens and genetic manipulations with single-neuron resolution. We will develop reliable methods to characterize the density of inhibitory GABAergic and excitatory cholinergic synapses from three-dimensional fluorescence confocal images. Our algorithms will lead to the next level of mechanistic understanding that controls the subcellular distribution of inhibitory and excitatory synapses, and enable a wide range of quantitative analyses for other types of neurons with similar complexity. Powerful multichannel co-analysis and machine learning approaches will be used to improve synapse detection and subcellular compartment extraction for overcoming challenges in 3D confocal image, including staining artifacts and anisotropic resolution. Algorithms will be developed using a model-guided methodology that emphasizes efficiency for large volume 3D images during genetic screening. Pattern-recognition methods will be used to speed up proofreading of the synapse quantification results. A novel ordering strategy will be adapted for neurons of complex dendritic arbor to quantify subcellular synapses in a functionally meaningful way. The project will produce a set of open-source, extensible tools for automatic synapse quantification and proofreading, with friendly graphical-user interfaces, to serve the neuroscience community.         PUBLIC HEALTH RELEVANCE: The underlying molecular mechanisms for the subcellular distribution of synapses remain largely unknown, which hinders the discovery of novel therapies for many neurological disorders. By developing new, efficient automatic algorithms and open-source tools for quantifying synapses in neurons, this research intends to advance the capacity to effectively analyze large quantities of three-dimensional neuronal images, especially those of complex dendritic arbor. The work will impact public health by enabling a better understanding of disease mechanisms, which is the critical first step toward new treatments, and supports NIH's goal to advance understanding of fundamental biology to uncover the causes of specific diseases.            ",Automatic 3D Quantification of Synapse Distribution in Complex Dendritic Arbor,8574710,R15MH099569,"['Academic Research Enhancement Awards', 'Algorithms', 'Area', 'Biology', 'Cells', 'Collaborations', 'Communities', 'Complex', 'Data', 'Dendrites', 'Detection', 'Development', 'Disease', 'Drosophila genus', 'Excitatory Synapse', 'Fluorescence', 'Generations', 'Genetic', 'Genetic Screening', 'Genetic Techniques', 'Goals', 'Hippocampus (Brain)', 'Image', 'Image Analysis', 'Inhibitory Synapse', 'Investigation', 'Knowledge', 'Lead', 'Location', 'Machine Learning', 'Mammals', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Microscopic', 'Microscopy', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Morphology', 'Nervous system structure', 'Neurons', 'Neurosciences', 'Pattern', 'Pattern Recognition', 'Play', 'Public Health', 'Purkinje Cells', 'Pyramidal Cells', 'Research', 'Resolution', 'Role', 'Speed', 'Staging', 'Staining method', 'Stains', 'Surface', 'Synapses', 'System', 'Techniques', 'Testing', 'Three-Dimensional Image', 'Three-Dimensional Imaging', 'Trees', 'Variant', 'Work', 'base', 'cholinergic synapse', 'density', 'falls', 'genetic manipulation', 'genome-wide', 'graduate student', 'graphical user interface', 'high throughput technology', 'improved', 'in vivo', 'innovation', 'interdisciplinary approach', 'mutant', 'nervous system disorder', 'novel', 'open source', 'public health relevance', 'tool', 'undergraduate student', 'user-friendly']",NIMH,NORTHERN ILLINOIS UNIVERSITY,R15,2013,461165,2020585,-0.023725363441931043
"The Cardiovascular Research Grid DESCRIPTION (provided by applicant):    The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinations of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotating ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and motion that can predict the early presence of developing heart disease in time for therapeutic intervention. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informatics system that allows clinical information to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG.  RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8424997,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'High Performance Computing', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Therapeutic Intervention', 'Time', 'Ultrasonography', 'Work', 'base', 'cardiovascular imaging', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2013,2177404,807432003,0.011527074521106306
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8447583,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2013,2703817,560644462,0.00216607227130351
"Very large datasets and new models to predict and design protein interactions PROJECT SUMMARY  Specific protein-protein interactions are responsible for organizing the cell, for processing biological signals and information, and for the chemistry of life. Thus, understanding biological mechanism relies on understanding the interactions that occur between proteins. An important long-term goal is to develop methods for reliably predicting and rationally modifying protein-protein interactions. Such capabilities would  provide insight into the molecular details of pathology and highlight opportunities for disease treatment. This proposal describes an integrated experimental/computational technology platform that will provide predictive models of protein interaction specificity. The experimental component involves constructing randomized libraries of proteins or peptides that will be sorted according to their affinities for binding a particular  receptor. The identities and binding affinities for very large numbers of library members will be decoded using high-throughput sequencing methods. The data, consisting of up to 107 {sequence, affinity} pairs per sequencing run, will be used as input to computational machine learning methods. Models will be generated that capture the relationship between sequence and interactions, and the predictive power of these models  will be tested experimentally. The work described in this proposal emphasizes technology development and application of the new platform to study two general types of protein complexes. First are interactions of short helical ligands with mid-sized globular proteins, here studied using anti-apoptotic Bcl-2 and Ca2+- binding EF-hand proteins. Second are interactions of short linear peptides with modular interaction  domains, here PDZ and SH3 domains. These four protein families mediate an enormous number of important molecular recognition events in human cells, and the resulting models will provide valuable support to study of their biological functions. This work will also provide a stringent test of the capabilities of the proposed technology, which can then be applied to a much wider variety of molecular complexes, e.g., protein-protein, protein-small molecule and protein-nucleic acid assemblies. Given the paucity of high-  throughput methods for accurately measuring protein-protein interactions, and the primitive capabilities of most computational models for predicting protein binding, the proposed technology platform has the potential to dramatically transform the study of protein interaction specificity. Relevance  Specific protein-protein interactions underlie all biological processes. Knowledge of interactions that occur in healthy vs. diseased tissues, coupled with methods for inhibiting such interactions, would dramatically expand opportunities to treat human disease. This proposal describes a new technology for advancing the measurement, prediction and design of protein complexes.",Very large datasets and new models to predict and design protein interactions,8527960,R01GM096466,"['Affinity', 'Apoptotic', 'Binding', 'Biological', 'Biological Assay', 'Biological Process', 'Calcium Binding', 'Cell physiology', 'Cells', 'Chemistry', 'Color', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Coupled', 'Data', 'Data Set', 'Disease', 'EF Hand Motifs', 'Event', 'Family', 'Fluorescence-Activated Cell Sorting', 'Goals', 'Human', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Libraries', 'Life', 'Ligands', 'Machine Learning', 'Measurement', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nucleic Acids', 'Pathology', 'Peptides', 'Performance', 'Phenotype', 'Property', 'Protein Binding', 'Protein Family', 'Proteins', 'Randomized', 'Running', 'SH3 Domains', 'Screening procedure', 'Signal Transduction', 'Sorting - Cell Movement', 'Specificity', 'Staging', 'Surface', 'Technology', 'Testing', 'Tissues', 'Training', 'Work', 'Yeasts', 'base', 'combinatorial', 'design', 'globular protein', 'human disease', 'indexing', 'insight', 'member', 'molecular recognition', 'new technology', 'novel', 'novel strategies', 'predictive modeling', 'protein complex', 'protein protein interaction', 'receptor', 'small molecule', 'technology development']",NIGMS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2012,46595,113554200,0.03634529869857145
"Integrated Cheminformatics Resource for Orphan Neurodegenerative Diseases    DESCRIPTION (provided by applicant): In this project, we seek to continue our development of new innovative computational tools, apply them to model relevant protein targets of orphan neurodegenerative diseases, and build a publicly accessible web resource to host these tools and models. These goals will be achieved via two Specific Aims. Aim-1 is to develop and validate two new structure-based computational tools. The first tool (Shape4) is a fast, structure-based virtual screening method designed to search large multi-conformer molecular databases for potential ligands for a protein target. It is based on this chief hypothesis: a ligand molecule's topographical shape and pharmacophore features should be complementary to those of its protein binding site. Novel computational geometry and shape modeling algorithms will be employed to fulfill the shape / pharmacophore matching tasks. The second tool (SB-PPK) generates structure-based descriptors for organic molecules. The descriptors so generated depend not only on the structure of an organic molecule, but also on the binding site features of the target protein. Thus, these new descriptors overcome the drawbacks of traditional molecular descriptors that depend only on the structures of organic molecules, regardless of what the target protein is. The new descriptors will be employed in conjunction with QSAR (quantitative structure activity relationship) modeling workflow to develop predictive models for selected protein targets (Aim-2a). Specifically, the two new methods developed in Aim-1 will be applied to build predictive models for targets from phosphodiesterase (PDE) and histone deacetylase (HDAC) families: PDE- 4, PDE-5, HDAC-7 and HDAC-8. We will then deploy the validated models via a web portal (Aim-2b) to benefit the research community of orphan neurodegenerative diseases. We aim to develop and apply innovative computational tools to study the protein targets of orphan neurodegenerative diseases, and to establish an open-access informatics resource to support the drug discovery efforts in these disease areas. We will ultimately contribute to alleviating the pain of orphan neurodegenerative disease patients.          n/a",Integrated Cheminformatics Resource for Orphan Neurodegenerative Diseases,8209233,SC3GM086265,"['Academia', 'Address', 'Affect', 'Algorithms', 'Applications Grants', 'Appointment', 'Area', 'Benchmarking', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biotechnology', 'Brain', 'Chemical Structure', 'Chemicals', 'Chicago', 'Collaborations', 'Communities', 'Complement', 'Computer Simulation', 'Computer Vision Systems', 'Computing Methodologies', 'Data', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Disease', 'Drug Design', 'Drug Industry', 'Ensure', 'Family', 'Foundations', 'Funding', 'Future', 'Genomics', 'Goals', 'Grant', 'Histone Deacetylase', 'Huntington Disease', 'Illinois', 'Individual', 'Industry', 'Informatics', 'Internet', 'Letters', 'Libraries', 'Ligands', 'Marketing', 'Medicine', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Molecular Models', 'Names', 'Neurodegenerative Disorders', 'North Carolina', 'Orphan', 'Pain', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Productivity', 'Protein Binding', 'Protein Family', 'Proteins', 'Protocols documentation', 'Publications', 'Quantitative Structure-Activity Relationship', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Support', 'Research Training', 'Resource Informatics', 'Resources', 'Role', 'Science', 'Scientist', 'Screening procedure', 'Shapes', 'Solid', 'Speed', 'Spinal Muscular Atrophy', 'Staging', 'Structure', 'Structure-Activity Relationship', 'Techniques', 'Technology', 'Testing', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Validation', 'Vision', 'Work', 'base', 'career', 'cheminformatics', 'computer infrastructure', 'computer science', 'computerized tools', 'computing resources', 'conformer', 'design', 'drug candidate', 'drug discovery', 'innovation', 'method development', 'model development', 'molecular modeling', 'novel', 'pharmacophore', 'phosphodiesterase IV', 'phosphodiesterase V', 'phosphoric diester hydrolase', 'predictive modeling', 'research study', 'success', 'three dimensional structure', 'tool', 'virtual', 'web-accessible']",NIGMS,NORTH CAROLINA CENTRAL UNIVERSITY,SC3,2012,98010,7249069,0.0010431091651653826
"Structural bioinformatics software for epitope selection and antibody engineering    DESCRIPTION (provided by applicant): Human health has benefited tremendously from the therapeutic application of monoclonal antibodies (mAb), treating painful and devastating diseases such as rheumatoid arthritis and cancer, among others. However, mAb development is a laborious and time consuming process. The health benefits gained from faster mAb development are clear, creating a great need for tools to guide scientists toward discovering the most promising antigenic targets-particularly with regard to B-cell epitopes (the part of an antigen recognized by an antibody). The critical barrier to progress in this domain is the inability to deduce the conformational characteristics of protein sequence in the absence of known structure for predicting linear B-cell epitopes-the largest, most diverse, and pharmaceutically valuable class of known epitopes. The general criticism of existing prediction methods is that they are inaccurate and do not address the conformational nature of B-cell epitopes.  DNASTAR proposes to create a software pipeline that guides the prediction of B-cell epitopes, models the dynamic structural interface between a monoclonal antibody and its experimentally identified antigen, and screens in silico site-directed mutations to engineer more potent antibodies with enhanced binding affinity. The Phase I goal is to improve the prediction of antigenic peptides from target protein sequences and experimental or predicted structures. Toward this goal, DNASTAR has established collaborations with experts in monoclonal antibody production, 3D structure prediction, and protein structure and dynamics, including access to their experimental methods, data, and software tools. Our predictive models will benefit from three key innovations: 1) a superior data set and professional insights into monoclonal antibody production, 2) the introduction of state of the art 3D structure prediction for training our epitope predictors, and 3) the first use of structure-based protein dynamics in B-cell epitope prediction.  At the conclusion of Phase I, we will deliver an enhanced sequence-only B-cell epitope prediction model when compared to current top prediction methods (Aim 1) and a superior sequence and structure-based epitope prediction model using 3D structure prediction and protein dynamics (Aim 2). In creating these models, we will account for the chemical and physical properties of a protein sequence and the biophysics that mediate protein-protein interactions, including solvent accessibility, hydrogen bonding, residue flexibility, binding nuclei, and geometric contours of the molecular surface. The proposed software pipeline will be built upon Protean 3D, our new molecular structure and simulation viewer, and will elevate the technical capability of a broad range of experimental scientists to estimate key antigenic structural properties from proteins without known structure-all on their desktop computer. Upon achieving these aims, scientists will recognize that it is no longer adequate to describe B-cell epitopes using amino acid frequencies or propensity scales alone.      PUBLIC HEALTH RELEVANCE: Monoclonal antibodies are invaluable tools for diagnosing and treating human diseases. Unfortunately, the experimental methods used today to identify the most promising immunogenic targets are time consuming and less than totally effective. By taking the novel approach of incorporating both protein sequence information and structural features derived from high quality 3D structure predictions within our desktop computer software product, we propose to advance the ability of a broad range of life scientists to properly predict B-cell epitopes (the part of an antigen recognized by an antibody) applicable to their area of interest. This will accelerate the discovery of new monoclonal antibody pharmaceuticals, leading to improved human health across many diseases.           Monoclonal antibodies are invaluable tools for diagnosing and treating human diseases. Unfortunately, the experimental methods used today to identify the most promising immunogenic targets are time consuming and less than totally effective. By taking the novel approach of incorporating both protein sequence information and structural features derived from high quality 3D structure predictions within our desktop computer software product, we propose to advance the ability of a broad range of life scientists to properly predict B-cell epitopes (the part of an antigen recognized by an antibody) applicable to their area of interest. This will accelerate the discovery of new monoclonal antibody pharmaceuticals, leading to improved human health across many diseases.         ",Structural bioinformatics software for epitope selection and antibody engineering,8251785,R43GM100520,"['Accounting', 'Address', 'Affinity', 'Amino Acid Sequence', 'Amino Acids', 'Antibodies', 'Antibody Affinity', 'Antibody Binding Sites', 'Antibody Formation', 'Antigens', 'Area', 'Autoimmune Diseases', 'B-Lymphocyte Epitopes', 'Base Sequence', 'Binding', 'Bioinformatics', 'Biological', 'Biophysics', 'Cell Nucleus', 'Characteristics', 'Chemicals', 'Collaborations', 'Complement', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Engineering', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Health Benefit', 'Human', 'Hydrogen Bonding', 'Immunology', 'Life', 'Machine Learning', 'Malignant Neoplasms', 'Marketing', 'Measures', 'Mediating', 'Methods', 'Metric', 'Modeling', 'Molecular', 'Molecular Structure', 'Monoclonal Antibodies', 'Mutation', 'Nature', 'Pain', 'Peptide Sequence Determination', 'Peptides', 'Performance', 'Pharmacologic Substance', 'Phase', 'Post-Translational Protein Processing', 'Process', 'Property', 'Protein Dynamics', 'Protein Engineering', 'Proteins', 'Receiver Operating Characteristics', 'Rheumatoid Arthritis', 'Scientist', 'Site', 'Software Tools', 'Solvents', 'Structure', 'Surface', 'Therapeutic', 'Time', 'Training', 'antibody engineering', 'base', 'chemical property', 'design', 'flexibility', 'human disease', 'immunogenic', 'improved', 'innovation', 'insight', 'interest', 'monoclonal antibody production', 'novel', 'novel strategies', 'physical property', 'predictive modeling', 'programs', 'protein protein interaction', 'protein structure prediction', 'research study', 'simulation', 'success', 'three dimensional structure', 'three-dimensional modeling', 'tool']",NIGMS,"DNASTAR, INC.",R43,2012,156708,2499981,0.016888917870930966
"Position Sensitive P-Mer Frequency Clustering with Applications to Classification    DESCRIPTION (provided by applicant):    Position Sensitive P-Mer Frequency Clustering with  Applications to Classification and Differentiation Recent genomic sequencing advances, such as next generation sequencing, and projects like the Human Microbiome Project create extremely large genomic databases. Even though the length of any specific sequence may be much shorter than that of the complete DNA sequence of an organism, looking at enormous libraries of sequences, such as 16S rRNA, presents an equally (if not greater) computational challenge. In traditional genomic analysis, only one sequence may be analyzed at a time. When dealing with metagenomics, thousands (or more) sequences need to be analyzed at the same time. However, to study such problems as environmental biological diversity and human microbiome diversity this is exactly what is needed. Current techniques have several shortcomings which need to be addressed. Techniques involving sequence alignment are typically based on selection of one representative sequence (as is typically done when looking at 16S rRNA data) which introduces selection bias. Genomic databases involving multiple copies of 16S per organism across thousands of organisms, will soon grow too large to practically process just using computationally expensive alignment methods to match sequences, but faster alignment-free methods currently do not provide the needed accuracy and sensitivity. As a complement to existing methods we introduce a novel class of fast high-throughput algorithms based on quasi-alignment using position specific p-mer frequency clustering. Organisms are represented by a directed graph structure that summarizes the ordering between clusters of p-mer frequency histograms at different positions in sequences. This model can be learned using all available 16S copies of an organism and thus eliminates selection bias. Due to the added position information, these algorithms can be used for species (and even strain) classification facilitating the study of strain diversity within species. Our prototype implementation of this new technique shows that it is able to produce compact profiles which can be efficiently stored and used for large scale classification and differentiation down to the strain level. Since the technique incorporates high-throughput data stream clustering, a proven technique in high performance computing, it scales well for very large scale DNA/RNA sequence data as well as massive sets of short sequence snippets collected during metagenomic research. In this project we will develop a suite of tools, profile models, and scoring techniques to model RNA/DNA sequences providing applications of organism classification, and intra/inter-organism similarity/diversity. Our approach provides both the specificity needed to perform strain classification and still avoid the computational overhead of alignment. It is important to note that this is accomplished through dynamic online machine learning techniques without human intervention.           Recent advances in Metagenomics and the Human Microbiome provide a complex landscape for dealing with a multitude of genomes all at once. One of the many challenges in this field is classification of the genomes present in the sample. Effective metagenomic classification and diversity analysis require complex representations of taxa. The significance of our research is that we develop a suite of tools, based on novel alignment free techniques that will be applied to environmental metagenomics samples as well as human microbiome samples. Providing such methods to rapidly classify organisms using our new approach on a laptop computer instead of several multi-processor servers will facilitate the rapid development of microbiome-based health screening in the near future.            ",Position Sensitive P-Mer Frequency Clustering with Applications to Classification,8320160,R21HG005912,"['Address', 'Algorithms', 'Biodiversity', 'Classification', 'Complement', 'Complex', 'Computational Technique', 'Computers', 'DNA', 'DNA Sequence', 'Data', 'Databases', 'Development', 'Effectiveness', 'Family', 'Frequencies', 'Future', 'Genome', 'Genomics', 'Grant', 'Graph', 'Habitats', 'Health', 'High Performance Computing', 'Human', 'Human Microbiome', 'Intervention', 'Lead', 'Learning', 'Length', 'Libraries', 'Link', 'Machine Learning', 'Metagenomics', 'Methods', 'Mining', 'Modeling', 'Online Systems', 'Organism', 'Positioning Attribute', 'Probability', 'Process', 'Property', 'RNA', 'RNA Sequences', 'Research', 'Ribosomal RNA', 'Sampling', 'Screening procedure', 'Selection Bias', 'Sequence Alignment', 'Sequence Analysis', 'Specificity', 'Stream', 'Structure', 'Taxon', 'Techniques', 'Testing', 'Time', 'Update', 'Work', 'base', 'computing resources', 'cost', 'improved', 'laptop', 'metagenome', 'microbial', 'microbiome', 'next generation', 'novel', 'novel strategies', 'prototype', 'research study', 'statistics', 'success', 'tool', 'user-friendly', 'web site']",NHGRI,SOUTHERN METHODIST UNIVERSITY,R21,2012,204974,2836411,0.00621263889902227
"Ontology-based Information Network to Support Vaccine Research  Project Summary (Abstract):  Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.  Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,8311060,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'abstracting', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'knowledge base', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2012,264994,641965656,0.020731948121314597
"Refinement Methods for Protein Docking based on Exploring Multi-Dimensional Energ DESCRIPTION (provided by applicant): All successful state-of-the-art protein docking methods employ a so called multistage approach. At the first stage of such approaches a rough energy potential is used to score billions of conformations. At a second stage, thousands of conformations with the best scores are retained and clustered based on a certain similarity metric. Cluster centers correspond to putative predictions/models. Recent work by the proposing team demonstrated that greater prediction quality can be achieved by properly exploring these clusters through a process called refinement. This work resulted in the development of a prototype refinement approach - the Semi-Definite programming-based Underestimation method (SDU). The central goal of the project is to build on the SDU success and develop a new high-throughput refinement protocol able to produce predictions of near-crystallographic quality in the most computationally efficient manner. Efficiency will be achieved by leveraging the funnel-like shape that binding free energy potentials exhibit. The specific aims are: (1) the development of a new clustering method that can classify the conformations retained from a first-stage method into clusters suitable for the proposed refinement strategy; (2) the characterization of the structure of the multi-dimensional funnel corresponding to each cluster and the development of an efficient refinement strategy to explore this funnel; (3) the development of a side-chain positioning algorithm appropriate for docking by leveraging Markov random field theory; and (4) the dissemination of the algorithms developed through the release to the research community of a software package and an automated refinement server. It is anticipated that the computational efficiency gains of the proposed refinement protocol over alternative Monte Carlo methods will exceed two orders of magnitude, while, at the same time, significantly improve upon the accuracy achieved by earlier refinement approaches. A novelty of the proposed work is in its use of sophisticated machinery from the fields of optimization and decision theory specially tailored to the biophysical properties of the docking problem. Techniques from convex and combinatorial optimization, machine learning, and Markov random fields are brought to bear on the refinement stage of multistage protein docking approaches. An important element of the work is the systematic characterization of multi-dimensional binding energy funnels. The existence of such funnels has been long conjectured but it has not led to new docking approaches so far. The proposed algorithms essentially achieve this goal by devising efficient strategies to identify, characterize, and explore these funnels. PUBLIC HEALTH RELEVANCE: This work will substantially improve upon computational methods for characterizing and predicting protein- protein interactions. It will enable treating relatively weak protein complexes involving larger proteins than what is possible today. This will result in a better understanding of processes such as metabolic control, immune response, signal transduction, and gene regulation.",Refinement Methods for Protein Docking based on Exploring Multi-Dimensional Energ,8240452,R01GM093147,"['Accounting', 'Address', 'Adopted', 'Algorithms', 'Benchmarking', 'Binding', 'Biological', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Data Quality', 'Decision Theory', 'Development', 'Dill', 'Dimensions', 'Discrimination', 'Docking', 'Electrostatics', 'Elements', 'Evaluation', 'Exhibits', 'Fourier Transform', 'Free Energy', 'Gene Expression Regulation', 'Generations', 'Goals', 'Grant', 'Hand', 'Health', 'Immune response', 'Knowledge', 'Lead', 'Libraries', 'Ligands', 'Link', 'Machine Learning', 'Maintenance', 'Maps', 'Metabolic Control', 'Methods', 'Metric', 'Modeling', 'Molecular Conformation', 'Monte Carlo Method', 'Motion', 'Motivation', 'Movement', 'National Institute of General Medical Sciences', 'Paper', 'Pathway interactions', 'Plant Roots', 'Positioning Attribute', 'Potential Energy', 'Probability', 'Process', 'Property', 'Proteins', 'Protocols documentation', 'Published Comment', 'Reporting', 'Research', 'Rotation', 'Sampling', 'Scoring Method', 'Screening procedure', 'Shapes', 'Side', 'Signal Transduction', 'Simulate', 'Staging', 'Structure', 'Techniques', 'Time', 'Transduction Gene', 'Translating', 'Translations', 'Ursidae Family', 'Vertebral column', 'Work', 'Writing', 'base', 'combinatorial', 'cost', 'experience', 'flexibility', 'improved', 'interest', 'programs', 'protein complex', 'protein folding', 'protein protein interaction', 'prototype', 'receptor', 'research study', 'success', 'theories']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2012,308407,61050884,0.02648025775028853
"Very large datasets and new models to predict and design protein interactions PROJECT SUMMARY  Specific protein-protein interactions are responsible for organizing the cell, for processing biological signals and information, and for the chemistry of life. Thus, understanding biological mechanism relies on understanding the interactions that occur between proteins. An important long-term goal is to develop methods for reliably predicting and rationally modifying protein-protein interactions. Such capabilities would  provide insight into the molecular details of pathology and highlight opportunities for disease treatment. This proposal describes an integrated experimental/computational technology platform that will provide predictive models of protein interaction specificity. The experimental component involves constructing randomized libraries of proteins or peptides that will be sorted according to their affinities for binding a particular  receptor. The identities and binding affinities for very large numbers of library members will be decoded using high-throughput sequencing methods. The data, consisting of up to 107 {sequence, affinity} pairs per sequencing run, will be used as input to computational machine learning methods. Models will be generated that capture the relationship between sequence and interactions, and the predictive power of these models  will be tested experimentally. The work described in this proposal emphasizes technology development and application of the new platform to study two general types of protein complexes. First are interactions of short helical ligands with mid-sized globular proteins, here studied using anti-apoptotic Bcl-2 and Ca2+- binding EF-hand proteins. Second are interactions of short linear peptides with modular interaction  domains, here PDZ and SH3 domains. These four protein families mediate an enormous number of important molecular recognition events in human cells, and the resulting models will provide valuable support to study of their biological functions. This work will also provide a stringent test of the capabilities of the proposed technology, which can then be applied to a much wider variety of molecular complexes, e.g., protein-protein, protein-small molecule and protein-nucleic acid assemblies. Given the paucity of high-  throughput methods for accurately measuring protein-protein interactions, and the primitive capabilities of most computational models for predicting protein binding, the proposed technology platform has the potential to dramatically transform the study of protein interaction specificity. Relevance  Specific protein-protein interactions underlie all biological processes. Knowledge of interactions that occur in healthy vs. diseased tissues, coupled with methods for inhibiting such interactions, would dramatically expand opportunities to treat human disease. This proposal describes a new technology for advancing the measurement, prediction and design of protein complexes.",Very large datasets and new models to predict and design protein interactions,8328742,R01GM096466,"['Affinity', 'Apoptotic', 'Binding', 'Biological', 'Biological Assay', 'Biological Process', 'Calcium Binding', 'Cell physiology', 'Cells', 'Chemistry', 'Color', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Coupled', 'Data', 'Data Set', 'Disease', 'EF Hand Motifs', 'Event', 'Family', 'Fluorescence-Activated Cell Sorting', 'Goals', 'Human', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Libraries', 'Life', 'Ligands', 'Machine Learning', 'Measurement', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nucleic Acids', 'Pathology', 'Peptides', 'Performance', 'Phenotype', 'Property', 'Protein Binding', 'Protein Family', 'Proteins', 'Randomized', 'Running', 'SH3 Domains', 'Screening procedure', 'Signal Transduction', 'Sorting - Cell Movement', 'Specificity', 'Staging', 'Surface', 'Technology', 'Testing', 'Tissues', 'Training', 'Work', 'Yeasts', 'base', 'combinatorial', 'design', 'globular protein', 'human disease', 'indexing', 'insight', 'member', 'molecular recognition', 'new technology', 'novel', 'novel strategies', 'predictive modeling', 'protein complex', 'protein protein interaction', 'receptor', 'small molecule', 'technology development']",NIGMS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2012,395872,113554200,0.03634529869857145
"The Cardiovascular Research Grid    DESCRIPTION (provided by applicant):       The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinafions of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotafing ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and mofion that can predict the early presence of developing heart disease in fime for therapeufic intervenfion. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informafics system that allows clinical informafion to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG.  RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease. (End of Abstract)          The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store,  manage, and analyze data on the structure and function of the cardiovascular system in health and disease.  The CVRG Project has developed and deployed unique technology that is now being used in a broad range  of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to  explore and analvze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8240702,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Time', 'Ultrasonography', 'Work', 'abstracting', 'base', 'cardiovascular imaging', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2012,2194299,807432003,0.010048100289156366
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8337800,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Screening procedure', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2012,2751015,560644462,0.00216607227130351
"Integrated Cheminformatics Resource for Orphan Neurodegenerative Diseases    DESCRIPTION (provided by applicant): In this project, we seek to continue our development of new innovative computational tools, apply them to model relevant protein targets of orphan neurodegenerative diseases, and build a publicly accessible web resource to host these tools and models. These goals will be achieved via two Specific Aims. Aim-1 is to develop and validate two new structure-based computational tools. The first tool (Shape4) is a fast, structure-based virtual screening method designed to search large multi-conformer molecular databases for potential ligands for a protein target. It is based on this chief hypothesis: a ligand molecule's topographical shape and pharmacophore features should be complementary to those of its protein binding site. Novel computational geometry and shape modeling algorithms will be employed to fulfill the shape / pharmacophore matching tasks. The second tool (SB-PPK) generates structure-based descriptors for organic molecules. The descriptors so generated depend not only on the structure of an organic molecule, but also on the binding site features of the target protein. Thus, these new descriptors overcome the drawbacks of traditional molecular descriptors that depend only on the structures of organic molecules, regardless of what the target protein is. The new descriptors will be employed in conjunction with QSAR (quantitative structure activity relationship) modeling workflow to develop predictive models for selected protein targets (Aim-2a). Specifically, the two new methods developed in Aim-1 will be applied to build predictive models for targets from phosphodiesterase (PDE) and histone deacetylase (HDAC) families: PDE- 4, PDE-5, HDAC-7 and HDAC-8. We will then deploy the validated models via a web portal (Aim-2b) to benefit the research community of orphan neurodegenerative diseases. We aim to develop and apply innovative computational tools to study the protein targets of orphan neurodegenerative diseases, and to establish an open-access informatics resource to support the drug discovery efforts in these disease areas. We will ultimately contribute to alleviating the pain of orphan neurodegenerative disease patients.          n/a",Integrated Cheminformatics Resource for Orphan Neurodegenerative Diseases,8019584,SC3GM086265,"['Academia', 'Address', 'Affect', 'Algorithms', 'Applications Grants', 'Appointment', 'Area', 'Benchmarking', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biotechnology', 'Brain', 'Chemical Structure', 'Chemicals', 'Chicago', 'Collaborations', 'Communities', 'Complement', 'Computer Simulation', 'Computer Vision Systems', 'Computing Methodologies', 'Data', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Disease', 'Drug Design', 'Drug Industry', 'Ensure', 'Family', 'Foundations', 'Funding', 'Future', 'Genomics', 'Goals', 'Grant', 'Histone Deacetylase', 'Huntington Disease', 'Illinois', 'Individual', 'Industry', 'Informatics', 'Internet', 'Letters', 'Libraries', 'Ligands', 'Marketing', 'Medicine', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Molecular Models', 'Names', 'Neurodegenerative Disorders', 'North Carolina', 'Orphan', 'Pain', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Productivity', 'Protein Binding', 'Protein Family', 'Proteins', 'Protocols documentation', 'Publications', 'Quantitative Structure-Activity Relationship', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Support', 'Research Training', 'Resource Informatics', 'Resources', 'Role', 'Science', 'Scientist', 'Screening procedure', 'Shapes', 'Solid', 'Speed', 'Spinal Muscular Atrophy', 'Staging', 'Structure', 'Structure-Activity Relationship', 'Techniques', 'Technology', 'Testing', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Validation', 'Vision', 'Work', 'base', 'career', 'cheminformatics', 'computer infrastructure', 'computer science', 'computerized tools', 'computing resources', 'conformer', 'design', 'drug candidate', 'drug discovery', 'innovation', 'method development', 'model development', 'molecular modeling', 'novel', 'pharmacophore', 'phosphodiesterase IV', 'phosphodiesterase V', 'phosphoric diester hydrolase', 'predictive modeling', 'research study', 'success', 'three dimensional structure', 'tool', 'virtual', 'web-accessible']",NIGMS,NORTH CAROLINA CENTRAL UNIVERSITY,SC3,2011,98010,7249069,0.0010431091651653826
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,8115129,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2011,136978,17640378,0.004617724439534644
"Position Sensitive P-Mer Frequency Clustering with Applications to Classification    DESCRIPTION (provided by applicant):    Position Sensitive P-Mer Frequency Clustering with  Applications to Classification and Differentiation Recent genomic sequencing advances, such as next generation sequencing, and projects like the Human Microbiome Project create extremely large genomic databases. Even though the length of any specific sequence may be much shorter than that of the complete DNA sequence of an organism, looking at enormous libraries of sequences, such as 16S rRNA, presents an equally (if not greater) computational challenge. In traditional genomic analysis, only one sequence may be analyzed at a time. When dealing with metagenomics, thousands (or more) sequences need to be analyzed at the same time. However, to study such problems as environmental biological diversity and human microbiome diversity this is exactly what is needed. Current techniques have several shortcomings which need to be addressed. Techniques involving sequence alignment are typically based on selection of one representative sequence (as is typically done when looking at 16S rRNA data) which introduces selection bias. Genomic databases involving multiple copies of 16S per organism across thousands of organisms, will soon grow too large to practically process just using computationally expensive alignment methods to match sequences, but faster alignment-free methods currently do not provide the needed accuracy and sensitivity. As a complement to existing methods we introduce a novel class of fast high-throughput algorithms based on quasi-alignment using position specific p-mer frequency clustering. Organisms are represented by a directed graph structure that summarizes the ordering between clusters of p-mer frequency histograms at different positions in sequences. This model can be learned using all available 16S copies of an organism and thus eliminates selection bias. Due to the added position information, these algorithms can be used for species (and even strain) classification facilitating the study of strain diversity within species. Our prototype implementation of this new technique shows that it is able to produce compact profiles which can be efficiently stored and used for large scale classification and differentiation down to the strain level. Since the technique incorporates high-throughput data stream clustering, a proven technique in high performance computing, it scales well for very large scale DNA/RNA sequence data as well as massive sets of short sequence snippets collected during metagenomic research. In this project we will develop a suite of tools, profile models, and scoring techniques to model RNA/DNA sequences providing applications of organism classification, and intra/inter-organism similarity/diversity. Our approach provides both the specificity needed to perform strain classification and still avoid the computational overhead of alignment. It is important to note that this is accomplished through dynamic online machine learning techniques without human intervention.      PUBLIC HEALTH RELEVANCE:    Recent advances in Metagenomics and the Human Microbiome provide a complex landscape for dealing with a multitude of genomes all at once. One of the many challenges in this field is classification of the genomes present in the sample. Effective metagenomic classification and diversity analysis require complex representations of taxa. The significance of our research is that we develop a suite of tools, based on novel alignment free techniques that will be applied to environmental metagenomics samples as well as human microbiome samples. Providing such methods to rapidly classify organisms using our new approach on a laptop computer instead of several multi-processor servers will facilitate the rapid development of microbiome-based health screening in the near future.                 Recent advances in Metagenomics and the Human Microbiome provide a complex landscape for dealing with a multitude of genomes all at once. One of the many challenges in this field is classification of the genomes present in the sample. Effective metagenomic classification and diversity analysis require complex representations of taxa. The significance of our research is that we develop a suite of tools, based on novel alignment free techniques that will be applied to environmental metagenomics samples as well as human microbiome samples. Providing such methods to rapidly classify organisms using our new approach on a laptop computer instead of several multi-processor servers will facilitate the rapid development of microbiome-based health screening in the near future.            ",Position Sensitive P-Mer Frequency Clustering with Applications to Classification,8192895,R21HG005912,"['Address', 'Algorithms', 'Biodiversity', 'Classification', 'Complement', 'Complex', 'Computational Technique', 'Computers', 'DNA', 'DNA Sequence', 'Data', 'Databases', 'Development', 'Effectiveness', 'Family', 'Frequencies', 'Future', 'Genome', 'Genomics', 'Grant', 'Graph', 'Habitats', 'Health', 'High Performance Computing', 'Human', 'Human Microbiome', 'Intervention', 'Lead', 'Learning', 'Length', 'Libraries', 'Link', 'Machine Learning', 'Metagenomics', 'Methods', 'Mining', 'Modeling', 'Online Systems', 'Organism', 'Positioning Attribute', 'Probability', 'Process', 'Property', 'RNA', 'RNA Sequences', 'Research', 'Ribosomal RNA', 'Sampling', 'Screening procedure', 'Selection Bias', 'Sequence Alignment', 'Sequence Analysis', 'Specificity', 'Stream', 'Structure', 'Taxon', 'Techniques', 'Testing', 'Time', 'Update', 'Work', 'base', 'computing resources', 'cost', 'improved', 'laptop', 'microbial', 'microbiome', 'next generation', 'novel', 'novel strategies', 'prototype', 'research study', 'statistics', 'success', 'tool', 'user-friendly', 'web site']",NHGRI,SOUTHERN METHODIST UNIVERSITY,R21,2011,180669,2836411,0.007244681973756254
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,8079474,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'allograft rejection', 'base', 'biobank', 'cancer microarray', 'cancer type', 'design', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'transplant database', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2011,250488,340417756,-0.006401210586271298
"Ontology-based Information Network to Support Vaccine Research    DESCRIPTION (provided by applicant): Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.            Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,8120230,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'knowledge base', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2011,264994,641965656,0.020879586806042995
"Refinement Methods for Protein Docking based on Exploring Multi-Dimensional Energ    DESCRIPTION (provided by applicant):  All successful state-of-the-art protein docking methods employ a so called multistage approach. At the first stage of such approaches a rough energy potential is used to score billions of conformations. At a second stage, thousands of conformations with the best scores are retained and clustered based on a certain similarity metric. Cluster centers correspond to putative predictions/models. Recent work by the proposing team demonstrated that greater prediction quality can be achieved by properly exploring these clusters through a process called refinement. This work resulted in the development of a prototype refinement approach - the Semi-Definite programming-based Underestimation method (SDU).  The central goal of the project is to build on the SDU success and develop a new high-throughput refinement protocol able to produce predictions of near-crystallographic quality in the most computationally efficient manner. Efficiency will be achieved by leveraging the funnel-like shape that binding free energy potentials exhibit. The specific aims are: (1) the development of a new clustering method that can classify the conformations retained from a first-stage method into clusters suitable for the proposed refinement strategy; (2) the characterization of the structure of the multi-dimensional funnel corresponding to each cluster and the development of an efficient refinement strategy to explore this funnel; (3) the development of a side-chain positioning algorithm appropriate for docking by leveraging Markov random field theory; and (4) the dissemination of the algorithms developed through the release to the research community of a software package and an automated refinement server. It is anticipated that the computational efficiency gains of the proposed refinement protocol over alternative Monte Carlo methods will exceed two orders of magnitude, while, at the same time, significantly improve upon the accuracy achieved by earlier refinement approaches.  A novelty of the proposed work is in its use of sophisticated machinery from the fields of optimization and decision theory specially tailored to the biophysical properties of the docking problem. Techniques from convex and combinatorial optimization, machine learning, and Markov random fields are brought to bear on the refinement stage of multistage protein docking approaches. An important element of the work is the systematic characterization of multi-dimensional binding energy funnels. The existence of such funnels has been long conjectured but it has not led to new docking approaches so far. The proposed algorithms essentially achieve this goal by devising efficient strategies to identify, characterize, and explore these funnels.      PUBLIC HEALTH RELEVANCE: This work will substantially improve upon computational methods for characterizing and predicting protein- protein interactions. It will enable treating relatively weak protein complexes involving larger proteins than what is possible today. This will result in a better understanding of processes such as metabolic control, immune response, signal transduction, and gene regulation.          Project Narrative   This work will substantially improve upon computational methods for characterizing and predicting protein-  protein interactions. It will enable treating relatively weak protein complexes involving larger proteins than what  is possible today. This will result in a better understanding of processes such as metabolic control, immune  response, signal transduction, and gene regulation.",Refinement Methods for Protein Docking based on Exploring Multi-Dimensional Energ,8042533,R01GM093147,"['Accounting', 'Address', 'Adopted', 'Algorithms', 'Benchmarking', 'Binding', 'Biological', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Data Quality', 'Decision Theory', 'Development', 'Dill', 'Dimensions', 'Discrimination', 'Docking', 'Electrostatics', 'Elements', 'Evaluation', 'Exhibits', 'Fourier Transform', 'Free Energy', 'Gene Expression Regulation', 'Generations', 'Goals', 'Grant', 'Hand', 'Immune response', 'Knowledge', 'Lead', 'Libraries', 'Ligands', 'Link', 'Machine Learning', 'Maintenance', 'Maps', 'Metabolic Control', 'Methods', 'Metric', 'Modeling', 'Molecular Conformation', 'Monte Carlo Method', 'Motion', 'Motivation', 'Movement', 'National Institute of General Medical Sciences', 'Paper', 'Pathway interactions', 'Plant Roots', 'Positioning Attribute', 'Potential Energy', 'Probability', 'Process', 'Property', 'Proteins', 'Protocols documentation', 'Published Comment', 'Reporting', 'Research', 'Rotation', 'Sampling', 'Scoring Method', 'Screening procedure', 'Shapes', 'Side', 'Signal Transduction', 'Simulate', 'Staging', 'Structure', 'Techniques', 'Time', 'Transduction Gene', 'Translating', 'Translations', 'Ursidae Family', 'Vertebral column', 'Work', 'Writing', 'base', 'combinatorial', 'cost', 'experience', 'flexibility', 'improved', 'interest', 'programs', 'protein complex', 'protein folding', 'protein protein interaction', 'prototype', 'public health relevance', 'receptor', 'research study', 'success', 'theories']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2011,315155,61050884,0.029844014101979698
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.           Project Narrative The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.",Scalable Learning with Ensemble Techniques and Parallel Computing,8045486,R44GM083965,"['Adoption', 'Algorithms', 'Architecture', 'Biological Sciences', 'Biomedical Research', 'Classification', 'Communication', 'Communities', 'Community Financing', 'Companions', 'Complex', 'Computer software', 'Consult', 'Crowding', 'Data', 'Data Set', 'Databases', 'Detection', 'Diagnosis', 'Disease', 'Effectiveness', 'Emerging Technologies', 'Ensure', 'Fostering', 'Foundations', 'Future', 'Generations', 'Goals', 'Graph', 'Grouping', 'Health', 'Imagery', 'Knowledge', 'Knowledge Discovery', 'Language', 'Learning', 'Libraries', 'Machine Learning', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Performance', 'Phase', 'Prevention', 'Problem Solving', 'Program Development', 'Randomized', 'Research', 'Research Infrastructure', 'Research Personnel', 'Running', 'Scheme', 'Simulate', 'Software Design', 'Software Tools', 'Speed', 'Structure', 'Techniques', 'Technology', 'Testing', 'Training', 'Validation', 'Voting', 'Work', 'base', 'computer cluster', 'computerized tools', 'data mining', 'design', 'forest', 'improved', 'innovation', 'new technology', 'next generation', 'parallel computing', 'programs', 'prototype', 'research and development', 'response', 'software development', 'sound', 'statistics', 'theories', 'tool']",NIGMS,INSILICOS,R44,2011,374673,0,0.023874599890017157
"Clinical Cytometry Analysis Software with Automated Gating    DESCRIPTION (provided by applicant): Flow cytometry is used to rapidly gather large quantities of data on cell type and function. The manual process of classifying hundreds of thousands of cells forms a bottleneck in diagnostics, high-throughput screening, clinical trials, and large-scale research experiments. The process currently requires a trained technician to identify populations on a digital graph of the data by manually drawing regions. As the complexity of the data increases, this gating task becomes more lengthy and laborious, and it is increasingly clear that minimizing human processing is essential to increasing both throughput and consistency. In clinical tests and diagnostic environments, automated gating would eliminate a complex set of human instructions and decisions in the Standard Operating Procedure (SOP), thereby reducing error and speeding results to the doctor. In many cases, the software will be able to recognize the need for additional tests before the doctor has an opportunity to look at the first report. Currently no software is available to perform complex multi-parameter analyses in an automated and rigorously validated manner. FlowDx will fill an important gap in the evolution of the technology and pave the way for ever larger phenotypic studies and for the translation of this research process to a clinical environment. Specific Aims 1) Fully define the experimental protocol, whereby a researcher can compare two or more classifications of identical data sets to study the differences, biases and effectiveness of human and algorithmic classifiers. 2) Describe and evaluate metrics that compare the performance of classification algorithms. 3) Conduct analytical experiments on our identified use cases, illustrating the potential of this technique to affect clinical analysis. 4) Iteratively implement the tools to automate these experiments, improve the experimental capabilities, and collaborate in new use cases. These aims will be satisfied while maintaining quantitative standards of software quality, establishing measurements in system uptime, throughput and robustness to set the baseline for subsequent iterations.      PUBLIC HEALTH RELEVANCE: FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project 1) Fits the ""translational medicine"" model of the NIH Roadmap 2) Reduces error in the diagnosis of cancer and other diseases 3) Speeds results to physicians. Patients learn the outcome more quickly. Therapeutic intervention is faster. 4) Accommodates large-scale research by allowing greater volumes of complex data to be much more quickly examined, compared, and quantified 5) Reduces the expense of cell analysis by as much as 50% 6) Conforms to 21CFR Part 11 guidance           Narrative FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project  � Fits the ""translational medicine"" model of the NIH Roadmap  � Reduces error in the diagnosis of cancer and other diseases  � Speeds results to physicians. Patients learn the outcome more quickly.  Therapeutic intervention is faster.  � Accommodates large-scale research by allowing greater volumes of complex data  to be much more quickly examined, compared, and quantified  � Reduces the expense of cell analysis by as much as 50%  � Conforms to 21CFR Part 11 guidance",Clinical Cytometry Analysis Software with Automated Gating,8139155,R44RR024094,"['AIDS/HIV problem', 'Affect', 'Algorithms', 'Architecture', 'Authorization documentation', 'Automation', 'Biological Assay', 'Biological Neural Networks', 'Biomedical Research', 'Cell physiology', 'Cells', 'Characteristics', 'Classification', 'Client', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Code', 'Complex', 'Computer software', 'Computers', 'Consensus', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Documentation', 'Effectiveness', 'Environment', 'Evolution', 'Flow Cytometry', 'Foundations', 'Graph', 'Grouping', 'Hospitals', 'Human', 'Institution', 'Instruction', 'Label', 'Language', 'Learning', 'Machine Learning', 'Magnetism', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Methods', 'Metric', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Procedures', 'Process', 'Protocols documentation', 'Quality Control', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scientist', 'Security', 'Services', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Trees', 'United States National Institutes of Health', 'Universities', 'Work', 'abstracting', 'cancer diagnosis', 'cell type', 'commercial application', 'data integrity', 'design', 'digital', 'encryption', 'high throughput screening', 'improved', 'operation', 'patient privacy', 'public health relevance', 'repository', 'research study', 'response', 'software systems', 'technological innovation', 'tool', 'translational medicine']",NCRR,"TREE STAR, INC.",R44,2011,449663,0,-0.014972860222862892
"The Cardiovascular Research Grid    DESCRIPTION (provided by applicant):       The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinafions of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotafing ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and mofion that can predict the early presence of developing heart disease in fime for therapeufic intervenfion. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informafics system that allows clinical informafion to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG.  RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease. (End of Abstract)          The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store,  manage, and analyze data on the structure and function of the cardiovascular system in health and disease.  The CVRG Project has developed and deployed unique technology that is now being used in a broad range  of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to  explore and analvze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8017600,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Time', 'Ultrasonography', 'Work', 'abstracting', 'base', 'cardiovascular imaging', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2011,2241978,807432003,0.010048100289156366
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.      PUBLIC HEALTH RELEVANCE:  Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,            Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8242999,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Screening procedure', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2011,2416667,560644462,-0.004229723831366165
"Development of a Research-Ready Pregnancy and Newborn Biobank in California    DESCRIPTION (provided by applicant): Development of a Research-Ready Pregnancy and Newborn Biobank in California Population-based biobanks are a critical resource for identifying disease mechanisms and developing screening tests for biomarkers associated with certain disorders. The California Department of Public Health has been banking newborn specimens statewide since 1982 (N~14 million) and maternal prenatal specimens for a portion of the state since 2000 (N~1 million), creating one of the largest, if not the largest single biological specimen banks with linked data in the world. With the fast pace of new knowledge in genetics and laboratory methods, the demand for specimens and data from researchers around the world now far surpasses the Department's ability to fill them. The goal of this infrastructure development project is to create an efficient, high throughput, low cost newborn screening and prenatal/maternal screening specimen biobank and linked data base that could be used by large numbers of researchers around the world for a wide range of studies through the following aims: (1) establishment of highly efficient protocols and procurement and integration of automated systems for pulling and processing specimens; (2) development of an integrated specimen tracking system into the Department's existing web-based Screening Information System; (3) development of a computerized system to track application requests for specimens and data; and (4) development of a linked screening program-vital records database that is organized into a life course, client based system. These aims will be accomplished through expansion of the Department's award-winning Screening Integration System to include web-based tracking of specimens and research requests, and use of an innovative machine-learning record matching application for high-performance linkages. After the 2 year grant period is completed, the California Research Ready Biospecimen Bank will be able to provide researchers with valuable biological specimens in a timely, cost-effective manner, thereby enabling a dramatic expansion of epidemiological research nationwide. The continuity of the system will be ensured by codifying human subjects-sensitive policies and procedures into Departmental regulations and by charging researchers modest fees for specimens, data and other research services.      PUBLIC HEALTH RELEVANCE: Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.           Program Narrative Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.",Development of a Research-Ready Pregnancy and Newborn Biobank in California,7945336,RC2HD065514,"['Area', 'Award', 'Biological', 'Biological Markers', 'Biological Specimen Banks', 'California', 'Case-Control Studies', 'Cessation of life', 'Charge', 'Client', 'Data', 'Data Files', 'Databases', 'Development', 'Disease', 'Ensure', 'Epidemiology', 'Family Study', 'Fees', 'Fetal Death', 'Funding', 'Future', 'Genetic', 'Goals', 'Government', 'Grant', 'Information Systems', 'Infusion procedures', 'Knowledge', 'Laboratories', 'Laws', 'Life Cycle Stages', 'Link', 'Live Birth', 'Machine Learning', 'Methods', 'Multiple Pregnancy', 'Neonatal Screening', 'Newborn Infant', 'Online Systems', 'Performance', 'Policies', 'Population', 'Pregnancy', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Public Health', 'Records', 'Regulation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Screening procedure', 'Services', 'Specimen', 'Specimen Handling', 'Study Subject', 'System', 'Systems Integration', 'Testing', 'Time', 'Woman', 'abstracting', 'base', 'biobank', 'cohort', 'computerized', 'cost', 'human subject', 'infrastructure development', 'innovation', 'population based', 'prenatal', 'programs']",NICHD,SEQUOIA FOUNDATION,RC2,2010,1,0,-0.016572470360205052
"Integrated Cheminformatics Resource for Orphan Neurodegenerative Diseases    DESCRIPTION (provided by applicant): In this project, we seek to continue our development of new innovative computational tools, apply them to model relevant protein targets of orphan neurodegenerative diseases, and build a publicly accessible web resource to host these tools and models. These goals will be achieved via two Specific Aims. Aim-1 is to develop and validate two new structure-based computational tools. The first tool (Shape4) is a fast, structure-based virtual screening method designed to search large multi-conformer molecular databases for potential ligands for a protein target. It is based on this chief hypothesis: a ligand molecule's topographical shape and pharmacophore features should be complementary to those of its protein binding site. Novel computational geometry and shape modeling algorithms will be employed to fulfill the shape / pharmacophore matching tasks. The second tool (SB-PPK) generates structure-based descriptors for organic molecules. The descriptors so generated depend not only on the structure of an organic molecule, but also on the binding site features of the target protein. Thus, these new descriptors overcome the drawbacks of traditional molecular descriptors that depend only on the structures of organic molecules, regardless of what the target protein is. The new descriptors will be employed in conjunction with QSAR (quantitative structure activity relationship) modeling workflow to develop predictive models for selected protein targets (Aim-2a). Specifically, the two new methods developed in Aim-1 will be applied to build predictive models for targets from phosphodiesterase (PDE) and histone deacetylase (HDAC) families: PDE- 4, PDE-5, HDAC-7 and HDAC-8. We will then deploy the validated models via a web portal (Aim-2b) to benefit the research community of orphan neurodegenerative diseases. We aim to develop and apply innovative computational tools to study the protein targets of orphan neurodegenerative diseases, and to establish an open-access informatics resource to support the drug discovery efforts in these disease areas. We will ultimately contribute to alleviating the pain of orphan neurodegenerative disease patients.          n/a",Integrated Cheminformatics Resource for Orphan Neurodegenerative Diseases,7753228,SC3GM086265,"['Academia', 'Address', 'Affect', 'Algorithms', 'Applications Grants', 'Appointment', 'Area', 'Benchmarking', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biotechnology', 'Brain', 'Chemical Structure', 'Chemicals', 'Chicago', 'Collaborations', 'Communities', 'Complement', 'Computer Simulation', 'Computer Vision Systems', 'Computing Methodologies', 'Data', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Disease', 'Drug Design', 'Drug Industry', 'Ensure', 'Family', 'Foundations', 'Funding', 'Future', 'Genomics', 'Goals', 'Grant', 'Histone Deacetylase', 'Huntington Disease', 'Illinois', 'Individual', 'Industry', 'Informatics', 'Internet', 'Letters', 'Libraries', 'Ligands', 'Marketing', 'Medicine', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Molecular Models', 'Names', 'Neurodegenerative Disorders', 'North Carolina', 'Orphan', 'Pain', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Productivity', 'Protein Binding', 'Protein Family', 'Proteins', 'Protocols documentation', 'Publications', 'Quantitative Structure-Activity Relationship', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Support', 'Research Training', 'Resource Informatics', 'Resources', 'Role', 'Science', 'Scientist', 'Screening procedure', 'Shapes', 'Solid', 'Speed', 'Spinal Muscular Atrophy', 'Staging', 'Structure', 'Structure-Activity Relationship', 'Techniques', 'Technology', 'Testing', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Validation', 'Vision', 'Work', 'base', 'career', 'cheminformatics', 'computer infrastructure', 'computer science', 'computerized tools', 'computing resources', 'conformer', 'design', 'drug candidate', 'drug discovery', 'innovation', 'method development', 'model development', 'molecular modeling', 'novel', 'pharmacophore', 'phosphodiesterase IV', 'phosphodiesterase V', 'phosphoric diester hydrolase', 'predictive modeling', 'research study', 'success', 'three dimensional structure', 'tool', 'virtual', 'web-accessible']",NIGMS,NORTH CAROLINA CENTRAL UNIVERSITY,SC3,2010,98700,7249069,0.0010431091651653826
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,7858165,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'allograft rejection', 'base', 'biobank', 'cancer microarray', 'cancer type', 'design', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'transplant database', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2010,253269,340417756,-0.006401210586271298
"Ontology-based Information Network to Support Vaccine Research    DESCRIPTION (provided by applicant): Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.            Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,7935464,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'knowledge base', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2010,267671,641965656,0.020879586806042995
"Building motif lexicons    DESCRIPTION (provided by applicant):  With the complete sequencing of numerous genomes and the annotation of proteomes, one of the next major challenges in biology is to understand the functions and integration of the encoded proteins (a NIH Roadmap area of emphasis). Deciphering protein function is a very time consuming, expensive process, as reflected by the disproportionately low percentage of proteins with well-established functions. One approach for extrapolating established functions to new proteins is to predict short motifs. Short motifs target proteins for post-translational modification, trafficking to cellular compartments, and binding to other proteins or molecules. Our cross-disciplinary team has built Minimotif Miner (MnM), a short motif database and platform- independent web-tool that identifies motif consensus sequences in protein queries and thus potential new protein functions (http://mnm.engr.uconn.edu/). MnM can also be used to develop new hypotheses of how specific mutations cause human disease and to identify putative targets for the development of therapeutic drugs, antibiotics, insecticides, and antiviral agents. Despite the utility of MnM and other motif resources, prediction of functional motifs still has two major limitations, which we address in this proposal. 1) To reduce the false-positive prediction of motifs, we have created a new language that allows us to consider the 3- dimensional structural conservation of motifs. For each motif, we will build specific motif definitions by combining experimental data with data from motif structures in the Protein Data Bank. We will also determine the sequence permutations that can form the observed motif structure by using molecular dynamic simulations. 2) To build a more comprehensive motif database we will use artificial intelligence to mine PubMed. The expert system will use automated literature screening, document summarization, and motif identification efficiency score to extract the majority of known motifs from PubMed. Addressing these limitations will vastly increase the utility of short motif prediction.              n/a",Building motif lexicons,7825413,R01GM079689,"['3-Dimensional', 'Adaptor Signaling Protein', 'Address', 'Amino Acids', 'Antibiotics', 'Antiviral Agents', 'Area', 'Artificial Intelligence', 'Binding', 'Biochemical', 'Biology', 'Code', 'Consensus', 'Consensus Sequence', 'Data', 'Databases', 'Expert Systems', 'Genome', 'Growth Factor', 'Human', 'Insecticides', 'Internet', 'Language', 'Letters', 'Literature', 'Measures', 'Mining', 'Molecular Conformation', 'Mutation', 'Nomenclature', 'Paper', 'Performance', 'Post-Translational Protein Processing', 'Process', 'Protein Binding', 'Proteins', 'Proteome', 'PubMed', 'Reading', 'Research Personnel', 'Resources', 'Screening procedure', 'Series', 'Signal Transduction', 'Structure', 'Time', 'United States National Institutes of Health', 'base', 'data mining', 'human disease', 'indexing', 'molecular dynamics', 'platform-independent', 'programs', 'protein function', 'protein structure', 'src Homology Region 2 Domain', 'therapeutic development', 'three dimensional structure', 'tool', 'trafficking']",NIGMS,UNIVERSITY OF NEVADA LAS VEGAS,R01,2010,273781,10239584,0.03282286667460869
"Improve predictions of structure and function by PredictProtein    DESCRIPTION (provided by applicant):       SUMMARY: Over 25,000 researchers in the US and over 50,000 in 120 other countries have exploited the PredictProtein (PP) Internet server to analyze proteins by homology-transfer and by eye novo predictions of protein structure and function. Here, we propose technical and scientific solutions that will improve the functionality of PP and its extension portal META-PP. Many technical changes will remain hidden to users and are required to increase the maintainability, scalability, and portability of these servers. New Graphical User Interfaces are one proposed solution that will visibly impact the service. The scientific solutions address two related tasks pertaining to the prediction of structure and function. The first is to predict the effect of mutations. We propose the development of novel machine learning-based methods to distinguish between mutations that affect structure, function, or have no apparent phenotype. Our final method will be applied to the screening of SNP data from our experimental colleagues at Columbia, as well as to the prediction of SNP effects in public databases. The second major task is the identification of natively unstructured regions and their functional classification. Proteins that do not adopt regular structures in isolation are increasingly becoming an important research area; they may provide a key to the evolution of complexity from prokaryotes to eukaryotes. We propose the development of a machine learning-based identification of features specific to this important class of molecules. We also plan to attack the problem from a very different angle by using predictions of interaction densities inside proteins. The resulting novel tools will allow a proteome-wide analysis of the role of these molecules. All methods will be made available through PP.      RELEVANCE: Information about protein structure adds an entire dimension to protein analysis and genome annotation. This addition is often essential to infer function even for natively unstructured proteins. The PredictProtein server is unique in its combination and exploitation of evolution, structure, and function; many thousands of theoretical, experimental, and clinical researches have benefited from this. The long-term goal of the research proposed here is to improve our ability to use the evolutionary record of amino acid substitutions, i.e. to ultimately understand the amino acid ""language"". The short-term goal is to address two tasks that are closely related to human diseases, namely the distinction between silent and important mutations and the mapping of unstructured proteins onto networks and diseases.          n/a",Improve predictions of structure and function by PredictProtein,7842572,R01LM007329,"['Address', 'Adopted', 'Affect', 'Amino Acid Substitution', 'Amino Acids', 'Area', 'Biology', 'Budgets', 'Classification', 'Clinical Research', 'Code', 'Communities', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Devices', 'Dimensions', 'Disease', 'Eukaryota', 'Evolution', 'Eye', 'Funding', 'Genome', 'Genomics', 'Goals', 'Grant', 'Induced Mutation', 'Internet', 'Investigation', 'Language', 'Linux', 'Machine Learning', 'Maintenance', 'Maps', 'Medical', 'Membrane', 'Methods', 'Mutation', 'Nucleotides', 'Output', 'Phenotype', 'Plug-in', 'Point Mutation', 'Progress Reports', 'Prokaryotic Cells', 'Property', 'Protein Analysis', 'Proteins', 'Proteome', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Screening procedure', 'Sequence Analysis', 'Services', 'Solutions', 'Structure', 'Testing', 'Training', 'base', 'data modeling', 'density', 'design', 'graphical user interface', 'human disease', 'improved', 'novel', 'portability', 'protein function', 'protein protein interaction', 'protein structure', 'protein structure function', 'protein structure prediction', 'tool']",NLM,COLUMBIA UNIV NEW YORK MORNINGSIDE,R01,2010,327180,61807989,0.028968897504906423
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.          n/a",Scalable Learning with Ensemble Techniques and Parallel Computing,8013208,R44GM083965,"['Adoption', 'Algorithms', 'Architecture', 'Arts', 'Biological Sciences', 'Biomedical Research', 'Cations', 'Classification', 'Communication', 'Communities', 'Companions', 'Complex', 'Computer software', 'Consult', 'Data', 'Data Set', 'Databases', 'Detection', 'Diagnosis', 'Disease', 'Effectiveness', 'Emerging Technologies', 'Ensure', 'Fostering', 'Foundations', 'Future', 'Generations', 'Goals', 'Graph', 'Grouping', 'Imagery', 'Knowledge', 'Language', 'Learning', 'Libraries', 'Machine Learning', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Performance', 'Phase', 'Prevention', 'Problem Solving', 'Program Development', 'Randomized', 'Research', 'Research Infrastructure', 'Research Personnel', 'Running', 'Scheme', 'Simulate', 'Software Design', 'Software Tools', 'Speed', 'Structure', 'Techniques', 'Technology', 'Testing', 'Training', 'Voting', 'Work', 'base', 'computer cluster', 'computerized tools', 'data mining', 'design', 'forest', 'improved', 'innovation', 'next generation', 'parallel computing', 'programs', 'prototype', 'public health relevance', 'research and development', 'response', 'software development', 'sound', 'statistics', 'theories', 'tool']",NIGMS,INSILICOS,R44,2010,376899,0,0.024523114682297353
"COMPUTATIONAL THINKING - Combining multiple types of reasoning to infer plausible In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING - Combining multiple types of reasoning to infer plausible,8170612,76201000023C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Medical Informatics', 'Patients', 'Population', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'System', 'Thinking', 'biomedical scientist', 'flexibility', 'innovation', 'novel strategies', 'prototype', 'rapid growth']",NLM,"CYCORP, INC.",N03,2010,377967,0,0.0023472384415915695
"COMPUTTIONAL THINKING-An Evidence-Based, Open-Database Approach to Diagnostic Dec In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a","COMPUTTIONAL THINKING-An Evidence-Based, Open-Database Approach to Diagnostic Dec",8173645,76201000026C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Face', 'Family', 'Funding', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Population', 'Prevention', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'cost effectiveness', 'design', 'evidence base', 'flexibility', 'improved', 'innovation', 'novel strategies', 'rapid growth']",NLM,"SIMULCONSULT, INC.",N03,2010,377991,0,0.004032942555906599
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,7933715,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2010,388462,17640378,0.004617724439534644
"Accessible Handling of Misclassified or Missing Binary Variables in CER Studies    DESCRIPTION (provided by applicant): Common but often overlooked threats to the validity of comparative effectiveness research (CER) studies include the misclassification or missingness of binary variables that are crucial to the ultimate analysis of the data. These variables potentially include the outcome of interest in standard or repeated measures logistic regression models, the factor (exposure) of interest, or an important confounder of the association under study. This proposal seeks to facilitate the investigation of the resulting biases to which a given CER analysis may be subject, and to provide study design-based remedial measures via which validity can be restored. The focus is upon statistical methods for conducting sensitivity analyses, as well as methods designed to make efficient use of supplemental data sources. The latter include validation data (in the case of misclassification), and so-called reassessment data (in the case of potentially informative missingness). A primary consideration throughout includes the incorporation of subject-specific covariates into the model of interest, as well as into models for the underlying misclassification or missingness process. Another primary goal is to establish a relatively consistent likelihood-based framework for all proposed analyses incorporating supplemental data, and to provide user-friendly programs utilizing common statistical software in order to make the methods broadly and readily accessible to those conducting CER. While not limited to specific applications, the proposed research draws motivation from and lends itself to illustration via two real-world studies. The first is the HIV Epidemiology Research Study (HERS), an observational cohort study in which the binary diagnosis of bacterial vaginosis was made at repeated visits via both error-prone and sophisticated assay techniques. The second is an emergency department-based ophthalmologic study in which non-dilated ocular fundus photography will be used for diagnosing serious ocular conditions, and will be compared against existing standard diagnostic methods. Both studies involve internal validation data to facilitate corrections for misclassification based on a fallible diagnostic method, and both are also subject to missing outcome and/or predictor data.      PUBLIC HEALTH RELEVANCE: The goal of this project is to provide statistical methods to aid comparative effectiveness research (CER) investigators with common problems encountered in data analysis. The problems upon which the project focuses come about when binary (""yes/no"") data are subject to being incorrectly measured (misclassified), or when they are sometimes not observed (missing) for reasons that might relate to information about subjects in the study. The intention is to provide CER investigators with methods that are relatively easy to use, yet effective and powerful for combating these challenges to valid data analysis.           PROJECT NARRATIVE The goal of this project is to provide statistical methods to aid comparative effectiveness research (CER) investigators with common problems encountered in data analysis. The problems upon which the project focuses come about when binary (""yes/no"") data are subject to being incorrectly measured (misclassified), or when they are sometimes not observed (missing) for reasons that might relate to information about subjects in the study. The intention is to provide CER investigators with methods that are relatively easy to use, yet effective and powerful for combating these challenges to valid data analysis.",Accessible Handling of Misclassified or Missing Binary Variables in CER Studies,8037394,RC4NR012527,"['Accident and Emergency department', 'Address', 'Bacterial Vaginosis', 'Biological Assay', 'Clinical', 'Cohort Studies', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Collection', 'Data Sources', 'Diagnosis', 'Diagnostic Procedure', 'Epidemiologic Studies', 'Epidemiology', 'Equation', 'Fostering', 'Fundus photography', 'Goals', 'HIV', 'Intention', 'Investigation', 'Literature', 'Logistic Regressions', 'Measures', 'Methods', 'Modeling', 'Motivation', 'Outcome', 'Participant', 'Process', 'Research', 'Research Design', 'Research Personnel', 'Resource Allocation', 'Sampling', 'Series', 'Statistical Methods', 'Techniques', 'Time', 'Validation', 'Visit', 'abstracting', 'analytical method', 'base', 'case control', 'combat', 'comparative effectiveness', 'cost', 'design', 'effectiveness research', 'interest', 'programs', 'research study', 'user-friendly']",NINR,EMORY UNIVERSITY,RC4,2010,441691,507546965,-0.0014621934099211188
"Clinical Cytometry Analysis Software with Automated Gating    DESCRIPTION (provided by applicant): Flow cytometry is used to rapidly gather large quantities of data on cell type and function. The manual process of classifying hundreds of thousands of cells forms a bottleneck in diagnostics, high-throughput screening, clinical trials, and large-scale research experiments. The process currently requires a trained technician to identify populations on a digital graph of the data by manually drawing regions. As the complexity of the data increases, this gating task becomes more lengthy and laborious, and it is increasingly clear that minimizing human processing is essential to increasing both throughput and consistency. In clinical tests and diagnostic environments, automated gating would eliminate a complex set of human instructions and decisions in the Standard Operating Procedure (SOP), thereby reducing error and speeding results to the doctor. In many cases, the software will be able to recognize the need for additional tests before the doctor has an opportunity to look at the first report. Currently no software is available to perform complex multi-parameter analyses in an automated and rigorously validated manner. FlowDx will fill an important gap in the evolution of the technology and pave the way for ever larger phenotypic studies and for the translation of this research process to a clinical environment. Specific Aims 1) Fully define the experimental protocol, whereby a researcher can compare two or more classifications of identical data sets to study the differences, biases and effectiveness of human and algorithmic classifiers. 2) Describe and evaluate metrics that compare the performance of classification algorithms. 3) Conduct analytical experiments on our identified use cases, illustrating the potential of this technique to affect clinical analysis. 4) Iteratively implement the tools to automate these experiments, improve the experimental capabilities, and collaborate in new use cases. These aims will be satisfied while maintaining quantitative standards of software quality, establishing measurements in system uptime, throughput and robustness to set the baseline for subsequent iterations.      PUBLIC HEALTH RELEVANCE: FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project 1) Fits the ""translational medicine"" model of the NIH Roadmap 2) Reduces error in the diagnosis of cancer and other diseases 3) Speeds results to physicians. Patients learn the outcome more quickly. Therapeutic intervention is faster. 4) Accommodates large-scale research by allowing greater volumes of complex data to be much more quickly examined, compared, and quantified 5) Reduces the expense of cell analysis by as much as 50% 6) Conforms to 21CFR Part 11 guidance           Narrative FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project  � Fits the ""translational medicine"" model of the NIH Roadmap  � Reduces error in the diagnosis of cancer and other diseases  � Speeds results to physicians. Patients learn the outcome more quickly.  Therapeutic intervention is faster.  � Accommodates large-scale research by allowing greater volumes of complex data  to be much more quickly examined, compared, and quantified  � Reduces the expense of cell analysis by as much as 50%  � Conforms to 21CFR Part 11 guidance",Clinical Cytometry Analysis Software with Automated Gating,7999420,R44RR024094,"['Acquired Immunodeficiency Syndrome', 'Affect', 'Algorithms', 'Architecture', 'Authorization documentation', 'Automation', 'Biological Assay', 'Biological Neural Networks', 'Biomedical Research', 'Cells', 'Characteristics', 'Classification', 'Client', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Code', 'Complex', 'Computer software', 'Computers', 'Consensus', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Documentation', 'Effectiveness', 'Environment', 'Evolution', 'Flow Cytometry', 'Foundations', 'Graph', 'Grouping', 'HIV', 'Hospitals', 'Human', 'Institution', 'Instruction', 'Label', 'Language', 'Learning', 'Machine Learning', 'Magnetism', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Methods', 'Metric', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Procedures', 'Process', 'Protocols documentation', 'Quality Control', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Security', 'Services', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Trees', 'United States National Institutes of Health', 'Universities', 'Work', 'abstracting', 'cancer diagnosis', 'cell type', 'commercial application', 'data integrity', 'design', 'digital', 'encryption', 'high throughput screening', 'improved', 'operation', 'patient privacy', 'public health relevance', 'repository', 'research study', 'response', 'software systems', 'technological innovation', 'tool', 'translational medicine']",NCRR,"TREE STAR, INC.",R44,2010,449663,0,-0.014972860222862892
"Proteomic Phosphopeptide Chip Technology for Protein Profiling    DESCRIPTION (provided by applicant):   This R21/R33 grant application is in response to NIH/NCI RFA-CA-07-005 for ""Advanced Proteomic Platforms and Computational Sciences for the NCI Clinical Proteomic Technologies Initiative"". Cancers are malignant growths caused by misregulated and uncontrolled cell division; these abnormal cellular activities are typically accompanied by unusual protein expression profiles1. Existing proteomics technologies cannot meet the demand of reliable, sensitive, accurate determinations of these disease-related molecular profiles; to achieve reproducible quality, high throughput, and affordable proteomic analyses, many challenging technological hurdles must first be addressed. We propose developing a proteomic phosphopeptide (PPEP) microchip technology platform that profiles proteins carrying phosphopeptide binding domains (PPBDs); using profiles generated in these experiments, along with predicative computational modeling based on both experimental data and a comprehensive PPEP and PPBD interaction database, we will demonstrate specific and quantitative measurements related to protein functions for proteins of significant biological importance. The strength of our proposed work lies in the integration of an already established array technology and a highly promising bioinformatics platform. The methods developed will enable many researchers to rapidly and vigorously develop peptide arrays for quantitative measurement of the proteins in the biological systems of their own interest or to use standard domain-optimized peptide arrays to systematically profile biological samples of basic research or clinical importance. Over the long term, the methods we develop will be used to establish domain-recognition systems for other types of domain-carrying proteins measurements. These capabilities of reliable measurement of proteins as a function of disease states are essential for cancer research, diagnosis and treatment.             n/a",Proteomic Phosphopeptide Chip Technology for Protein Profiling,7906880,R33CA126209,"['Address', 'Antibodies', 'Applications Grants', 'Basic Science', 'Binding', 'Binding Proteins', 'Biochemical Reaction', 'Bioinformatics', 'Biological', 'Cell division', 'Clinical', 'Computational Science', 'Computer Simulation', 'Computer software', 'DNA', 'Data', 'Data Analyses', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Ensure', 'Epitopes', 'Experimental Designs', 'Growth', 'Information Resources', 'Link', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Methods', 'Microfluidic Microchips', 'Microfluidics', 'Modeling', 'Molecular Profiling', 'PTPN11 gene', 'Peptide Mapping', 'Peptide Signal Sequences', 'Peptides', 'Phase', 'Phosphopeptides', 'Phosphorylation Site', 'Phosphotransferases', 'Process', 'Protein Binding', 'Protein Kinase', 'Proteins', 'Proteomics', 'Publishing', 'Research Personnel', 'Resource Informatics', 'Sampling', 'Signal Transduction', 'Software Tools', 'Staging', 'System', 'Technology', 'Testing', 'Time', 'Work', 'Writing', 'anticancer research', 'base', 'biological systems', 'database design', 'density', 'design', 'interest', 'meetings', 'microchip', 'novel', 'programs', 'protein aminoacid sequence', 'protein expression', 'protein function', 'protein profiling', 'research study', 'response', 'src Homology Region 2 Domain', 'technology development', 'tool', 'web-accessible']",NCI,UNIVERSITY OF HOUSTON,R33,2010,498935,31980265,0.02201046504312122
"Robust Classification Methods for Categorical Regression    DESCRIPTION (provided by applicant): Improving statistical methods to provide better classification performance and new analytical capabilities for categorical regression would be invaluable to the medical and health care research communities. Categorical regression models (e.g., binary logistic, multinomial logistic) are used extensively to identify patterns of alcohol-related symptoms, screen for disorders, and assess policies. In addition, such models are used extensively in other areas of research such as mental illness, cancer, traumatic injuries, and AIDS-related pathologies. However, many such models are developed with inadequate support to fully analyze and exploit the intrinsically probabilistic nature of their results. This is of critical importance as health researchers, clinicians, and administrators are often faced with classification decisions using categorical regression models to identify unacceptable risks, adequate outcomes, and acceptable guidelines for screening, diagnoses, treatment, and quality of care. Commercially available statistical software does not offer sophisticated methods for robust estimation of posterior probabilities in the presence of model misspecification, missing covariates, and nonignorable missing data generating processes. Such robust missing data handling methods provide natural mechanisms for dealing with verification bias and modeling correlated, longitudinal, or survey data with complex sampling designs. Moreover, commercially available statistical software does not provide automated methods for using estimated posterior probabilities to make optimal classification decisions with respect to different optimality criteria. In particular, automated features such as optimizing multiple decision criteria (allocation rules) that trade off specificity against sensitivity, decision threshold confidence intervals, statistical tests for evaluating correct specification of posterior probabilities, statistical tests for comparing competing classifier thresholds, and methods for multi-outcome classification and inference are not readily available. Phase II research will extend Phase I findings for binary logistic regression to develop and implement automated robust classification methods for multinomial logistic regression modeling, which also applies to the larger class of nonlinear categorical regression models that output posterior probabilities. The Phase II software prototype will provide: 1) new user-selectable robust decision threshold estimators, 2) robust confidence intervals on decision threshold estimators, 3) new classifier threshold comparison tests, 4) new outcome probability specification tests, 5) efficient missing data handling methods in the presence of nonignorable nonresponse data, and 6) second-order analytic and simulation-based Bayesian methods for improved small sample and rare event outcome probability estimation. These new methodologies will be integrated into a prototype user-friendly software package, evaluated with extensive simulation studies, and then applied to real world classification problems encountered in: alcohol, mental illness (depression, bipolar, schizophrenia), cancer (prostate), trauma (emergency room), and infectious disease (AIDS) through collaborations with domain experts in those respective fields. In summary, Phase II research will establish the essential technical foundation for Phase III commercialization with the objective of providing a suite of new classification analysis methods as an advanced statistical tool that improves epidemiologic, clinical, and public health research.                 n/a",Robust Classification Methods for Categorical Regression,7917387,R44CA139607,"['Accident and Emergency department', 'Achievement', 'Address', 'Administrator', 'Agreement', 'Alcohols', 'Algorithms', 'Area', 'Bayesian Method', 'Behavior', 'Bipolar Depression', 'Classification', 'Clinical', 'Clinical Investigator', 'Collaborations', 'Communicable Diseases', 'Communities', 'Complex', 'Computer software', 'Confidence Intervals', 'Data', 'Data Analyses', 'Data Set', 'Decision Analysis', 'Decision Making', 'Development', 'Diagnosis', 'Disease', 'Empirical Research', 'Engineering', 'Epidemiologic Studies', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Foundations', 'Goals', 'Guidelines', 'Health', 'Health Services Research', 'Healthcare', 'Industry', 'Information Resources Management', 'Injury', 'Jordan', 'Journals', 'Knowledge', 'Literature', 'Logistic Regressions', 'Logistics', 'Malignant Neoplasms', 'Malignant neoplasm of prostate', 'Medical', 'Mental Health', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Outcome', 'Output', 'Pathology', 'Pattern', 'Peer Review', 'Performance', 'Phase', 'Policies', 'Preparation', 'Probability', 'Process', 'Publishing', 'Quality of Care', 'Relative (related person)', 'Research', 'Research Personnel', 'Risk', 'Robin bird', 'Sampling', 'Schizophrenia', 'Screening procedure', 'Sensitivity and Specificity', 'Simulate', 'Software Tools', 'Specific qualifier value', 'Specificity', 'Statistical Methods', 'Surveys', 'Symptoms', 'Technology', 'Testing', 'Trauma', 'anticancer research', 'base', 'commercialization', 'computerized data processing', 'density', 'design', 'graphical user interface', 'improved', 'innovation', 'phase 1 study', 'phase 2 study', 'prototype', 'public health research', 'simulation', 'software development', 'theories', 'tool', 'user friendly software']",NCI,MARTINGALE RESEARCH CORPORATION,R44,2010,990520,0,0.006766502232110227
"Building motif lexicons    DESCRIPTION (provided by applicant):  With the complete sequencing of numerous genomes and the annotation of proteomes, one of the next major challenges in biology is to understand the functions and integration of the encoded proteins (a NIH Roadmap area of emphasis). Deciphering protein function is a very time consuming, expensive process, as reflected by the disproportionately low percentage of proteins with well-established functions. One approach for extrapolating established functions to new proteins is to predict short motifs. Short motifs target proteins for post-translational modification, trafficking to cellular compartments, and binding to other proteins or molecules. Our cross-disciplinary team has built Minimotif Miner (MnM), a short motif database and platform- independent web-tool that identifies motif consensus sequences in protein queries and thus potential new protein functions (http://mnm.engr.uconn.edu/). MnM can also be used to develop new hypotheses of how specific mutations cause human disease and to identify putative targets for the development of therapeutic drugs, antibiotics, insecticides, and antiviral agents. Despite the utility of MnM and other motif resources, prediction of functional motifs still has two major limitations, which we address in this proposal. 1) To reduce the false-positive prediction of motifs, we have created a new language that allows us to consider the 3- dimensional structural conservation of motifs. For each motif, we will build specific motif definitions by combining experimental data with data from motif structures in the Protein Data Bank. We will also determine the sequence permutations that can form the observed motif structure by using molecular dynamic simulations. 2) To build a more comprehensive motif database we will use artificial intelligence to mine PubMed. The expert system will use automated literature screening, document summarization, and motif identification efficiency score to extract the majority of known motifs from PubMed. Addressing these limitations will vastly increase the utility of short motif prediction.              n/a",Building motif lexicons,7617090,R01GM079689,"['3-Dimensional', 'Adaptor Signaling Protein', 'Address', 'Amino Acids', 'Antibiotics', 'Antiviral Agents', 'Area', 'Artificial Intelligence', 'Binding', 'Biochemical', 'Biology', 'Code', 'Consensus', 'Consensus Sequence', 'Data', 'Databases', 'Expert Systems', 'Genome', 'Growth Factor', 'Human', 'Insecticides', 'Internet', 'Language', 'Letters', 'Literature', 'Measures', 'Mining', 'Molecular Conformation', 'Mutation', 'Nomenclature', 'Paper', 'Performance', 'Post-Translational Protein Processing', 'Process', 'Protein Binding', 'Proteins', 'Proteome', 'PubMed', 'Reading', 'Research Personnel', 'Resources', 'Screening procedure', 'Series', 'Signal Transduction', 'Structure', 'Time', 'United States National Institutes of Health', 'base', 'data mining', 'human disease', 'indexing', 'molecular dynamics', 'platform-independent', 'programs', 'protein function', 'protein structure', 'src Homology Region 2 Domain', 'therapeutic development', 'three dimensional structure', 'tool', 'trafficking']",NIGMS,UNIVERSITY OF CONNECTICUT SCH OF MED/DNT,R01,2009,60937,65553150,0.03282286667460869
"Integrated Cheminformatics Resource for Orphan Neurodegenerative Diseases    DESCRIPTION (provided by applicant): In this project, we seek to continue our development of new innovative computational tools, apply them to model relevant protein targets of orphan neurodegenerative diseases, and build a publicly accessible web resource to host these tools and models. These goals will be achieved via two Specific Aims. Aim-1 is to develop and validate two new structure-based computational tools. The first tool (Shape4) is a fast, structure-based virtual screening method designed to search large multi-conformer molecular databases for potential ligands for a protein target. It is based on this chief hypothesis: a ligand molecule's topographical shape and pharmacophore features should be complementary to those of its protein binding site. Novel computational geometry and shape modeling algorithms will be employed to fulfill the shape / pharmacophore matching tasks. The second tool (SB-PPK) generates structure-based descriptors for organic molecules. The descriptors so generated depend not only on the structure of an organic molecule, but also on the binding site features of the target protein. Thus, these new descriptors overcome the drawbacks of traditional molecular descriptors that depend only on the structures of organic molecules, regardless of what the target protein is. The new descriptors will be employed in conjunction with QSAR (quantitative structure activity relationship) modeling workflow to develop predictive models for selected protein targets (Aim-2a). Specifically, the two new methods developed in Aim-1 will be applied to build predictive models for targets from phosphodiesterase (PDE) and histone deacetylase (HDAC) families: PDE- 4, PDE-5, HDAC-7 and HDAC-8. We will then deploy the validated models via a web portal (Aim-2b) to benefit the research community of orphan neurodegenerative diseases. We aim to develop and apply innovative computational tools to study the protein targets of orphan neurodegenerative diseases, and to establish an open-access informatics resource to support the drug discovery efforts in these disease areas. We will ultimately contribute to alleviating the pain of orphan neurodegenerative disease patients.          n/a",Integrated Cheminformatics Resource for Orphan Neurodegenerative Diseases,7559157,SC3GM086265,"['Academia', 'Address', 'Affect', 'Algorithms', 'Applications Grants', 'Appointment', 'Area', 'Benchmarking', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biotechnology', 'Brain', 'Chemical Structure', 'Chemicals', 'Chicago', 'Collaborations', 'Communities', 'Complement', 'Computer Simulation', 'Computer Vision Systems', 'Computing Methodologies', 'Data', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Disease', 'Drug Design', 'Drug Industry', 'Ensure', 'Family', 'Foundations', 'Funding', 'Future', 'Genomics', 'Goals', 'Grant', 'Histone Deacetylase', 'Huntington Disease', 'Illinois', 'Individual', 'Industry', 'Informatics', 'Internet', 'Letters', 'Libraries', 'Ligands', 'Marketing', 'Medicine', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Molecular Models', 'Names', 'Neurodegenerative Disorders', 'North Carolina', 'Orphan', 'Pain', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Productivity', 'Protein Binding', 'Protein Family', 'Proteins', 'Protocols documentation', 'Publications', 'Quantitative Structure-Activity Relationship', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Support', 'Research Training', 'Resource Informatics', 'Resources', 'Role', 'Science', 'Scientist', 'Screening procedure', 'Shapes', 'Solid', 'Speed', 'Spinal Muscular Atrophy', 'Staging', 'Structure', 'Structure-Activity Relationship', 'Techniques', 'Technology', 'Testing', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Validation', 'Vision', 'Work', 'base', 'career', 'cheminformatics', 'computer infrastructure', 'computer science', 'computerized tools', 'computing resources', 'conformer', 'design', 'drug candidate', 'drug discovery', 'innovation', 'method development', 'model development', 'molecular modeling', 'novel', 'pharmacophore', 'phosphodiesterase IV', 'phosphodiesterase V', 'phosphoric diester hydrolase', 'predictive modeling', 'research study', 'success', 'three dimensional structure', 'tool', 'virtual', 'web-accessible']",NIGMS,NORTH CAROLINA CENTRAL UNIVERSITY,SC3,2009,98400,7249069,0.0010431091651653826
"Building motif lexicons    DESCRIPTION (provided by applicant):  With the complete sequencing of numerous genomes and the annotation of proteomes, one of the next major challenges in biology is to understand the functions and integration of the encoded proteins (a NIH Roadmap area of emphasis). Deciphering protein function is a very time consuming, expensive process, as reflected by the disproportionately low percentage of proteins with well-established functions. One approach for extrapolating established functions to new proteins is to predict short motifs. Short motifs target proteins for post-translational modification, trafficking to cellular compartments, and binding to other proteins or molecules. Our cross-disciplinary team has built Minimotif Miner (MnM), a short motif database and platform- independent web-tool that identifies motif consensus sequences in protein queries and thus potential new protein functions (http://mnm.engr.uconn.edu/). MnM can also be used to develop new hypotheses of how specific mutations cause human disease and to identify putative targets for the development of therapeutic drugs, antibiotics, insecticides, and antiviral agents. Despite the utility of MnM and other motif resources, prediction of functional motifs still has two major limitations, which we address in this proposal. 1) To reduce the false-positive prediction of motifs, we have created a new language that allows us to consider the 3- dimensional structural conservation of motifs. For each motif, we will build specific motif definitions by combining experimental data with data from motif structures in the Protein Data Bank. We will also determine the sequence permutations that can form the observed motif structure by using molecular dynamic simulations. 2) To build a more comprehensive motif database we will use artificial intelligence to mine PubMed. The expert system will use automated literature screening, document summarization, and motif identification efficiency score to extract the majority of known motifs from PubMed. Addressing these limitations will vastly increase the utility of short motif prediction.              n/a",Building motif lexicons,7983414,R01GM079689,"['3-Dimensional', 'Adaptor Signaling Protein', 'Address', 'Amino Acids', 'Antibiotics', 'Antiviral Agents', 'Area', 'Artificial Intelligence', 'Binding', 'Biochemical', 'Biology', 'Code', 'Consensus', 'Consensus Sequence', 'Data', 'Databases', 'Expert Systems', 'Genome', 'Growth Factor', 'Human', 'Insecticides', 'Internet', 'Language', 'Letters', 'Literature', 'Measures', 'Mining', 'Molecular Conformation', 'Mutation', 'Nomenclature', 'Paper', 'Performance', 'Post-Translational Protein Processing', 'Process', 'Protein Binding', 'Proteins', 'Proteome', 'PubMed', 'Reading', 'Research Personnel', 'Resources', 'Screening procedure', 'Series', 'Signal Transduction', 'Structure', 'Time', 'United States National Institutes of Health', 'base', 'data mining', 'human disease', 'indexing', 'molecular dynamics', 'platform-independent', 'programs', 'protein function', 'protein structure', 'src Homology Region 2 Domain', 'therapeutic development', 'three dimensional structure', 'tool', 'trafficking']",NIGMS,UNIVERSITY OF NEVADA LAS VEGAS,R01,2009,215112,10239584,0.03282286667460869
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,7666186,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Allografting', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Classification', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'base', 'biobank', 'cancer microarray', 'cancer type', 'design', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'transplant database', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2009,256073,340417756,-0.006401210586271298
"Ontology-based Information Network to Support Vaccine Research    DESCRIPTION (provided by applicant): Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.            Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,7735790,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2009,270375,641965656,0.020879586806042995
"Improve predictions of structure and function by PredictProtein    DESCRIPTION (provided by applicant):       SUMMARY: Over 25,000 researchers in the US and over 50,000 in 120 other countries have exploited the PredictProtein (PP) Internet server to analyze proteins by homology-transfer and by eye novo predictions of protein structure and function. Here, we propose technical and scientific solutions that will improve the functionality of PP and its extension portal META-PP. Many technical changes will remain hidden to users and are required to increase the maintainability, scalability, and portability of these servers. New Graphical User Interfaces are one proposed solution that will visibly impact the service. The scientific solutions address two related tasks pertaining to the prediction of structure and function. The first is to predict the effect of mutations. We propose the development of novel machine learning-based methods to distinguish between mutations that affect structure, function, or have no apparent phenotype. Our final method will be applied to the screening of SNP data from our experimental colleagues at Columbia, as well as to the prediction of SNP effects in public databases. The second major task is the identification of natively unstructured regions and their functional classification. Proteins that do not adopt regular structures in isolation are increasingly becoming an important research area; they may provide a key to the evolution of complexity from prokaryotes to eukaryotes. We propose the development of a machine learning-based identification of features specific to this important class of molecules. We also plan to attack the problem from a very different angle by using predictions of interaction densities inside proteins. The resulting novel tools will allow a proteome-wide analysis of the role of these molecules. All methods will be made available through PP.      RELEVANCE: Information about protein structure adds an entire dimension to protein analysis and genome annotation. This addition is often essential to infer function even for natively unstructured proteins. The PredictProtein server is unique in its combination and exploitation of evolution, structure, and function; many thousands of theoretical, experimental, and clinical researches have benefited from this. The long-term goal of the research proposed here is to improve our ability to use the evolutionary record of amino acid substitutions, i.e. to ultimately understand the amino acid ""language"". The short-term goal is to address two tasks that are closely related to human diseases, namely the distinction between silent and important mutations and the mapping of unstructured proteins onto networks and diseases.          n/a",Improve predictions of structure and function by PredictProtein,7625220,R01LM007329,"['Address', 'Adopted', 'Affect', 'Amino Acid Substitution', 'Amino Acids', 'Area', 'Biology', 'Budgets', 'Classification', 'Clinical Research', 'Code', 'Communities', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Devices', 'Dimensions', 'Disease', 'Eukaryota', 'Evolution', 'Eye', 'Funding', 'Genome', 'Genomics', 'Goals', 'Grant', 'Induced Mutation', 'Internet', 'Investigation', 'Language', 'Linux', 'Machine Learning', 'Maintenance', 'Maps', 'Medical', 'Membrane', 'Methods', 'Mutation', 'Nucleotides', 'Output', 'Phenotype', 'Plug-in', 'Point Mutation', 'Progress Reports', 'Prokaryotic Cells', 'Property', 'Protein Analysis', 'Proteins', 'Proteome', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Screening procedure', 'Sequence Analysis', 'Services', 'Solutions', 'Structure', 'Testing', 'Training', 'base', 'data modeling', 'density', 'design', 'graphical user interface', 'human disease', 'improved', 'novel', 'portability', 'protein function', 'protein protein interaction', 'protein structure', 'protein structure function', 'protein structure prediction', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2009,330166,558628098,0.028968897504906423
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,7786337,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2009,362692,17640378,0.004617724439534644
"Proteomic Phosphopeptide Chip Technology for Protein Profiling    DESCRIPTION (provided by applicant):   This R21/R33 grant application is in response to NIH/NCI RFA-CA-07-005 for ""Advanced Proteomic Platforms and Computational Sciences for the NCI Clinical Proteomic Technologies Initiative"". Cancers are malignant growths caused by misregulated and uncontrolled cell division; these abnormal cellular activities are typically accompanied by unusual protein expression profiles1. Existing proteomics technologies cannot meet the demand of reliable, sensitive, accurate determinations of these disease-related molecular profiles; to achieve reproducible quality, high throughput, and affordable proteomic analyses, many challenging technological hurdles must first be addressed. We propose developing a proteomic phosphopeptide (PPEP) microchip technology platform that profiles proteins carrying phosphopeptide binding domains (PPBDs); using profiles generated in these experiments, along with predicative computational modeling based on both experimental data and a comprehensive PPEP and PPBD interaction database, we will demonstrate specific and quantitative measurements related to protein functions for proteins of significant biological importance. The strength of our proposed work lies in the integration of an already established array technology and a highly promising bioinformatics platform. The methods developed will enable many researchers to rapidly and vigorously develop peptide arrays for quantitative measurement of the proteins in the biological systems of their own interest or to use standard domain-optimized peptide arrays to systematically profile biological samples of basic research or clinical importance. Over the long term, the methods we develop will be used to establish domain-recognition systems for other types of domain-carrying proteins measurements. These capabilities of reliable measurement of proteins as a function of disease states are essential for cancer research, diagnosis and treatment.             n/a",Proteomic Phosphopeptide Chip Technology for Protein Profiling,7676045,R33CA126209,"['Address', 'Antibodies', 'Applications Grants', 'Basic Science', 'Binding', 'Binding Proteins', 'Biochemical Reaction', 'Bioinformatics', 'Biological', 'Cell division', 'Clinical', 'Computational Science', 'Computer Simulation', 'Computer software', 'DNA', 'Data', 'Data Analyses', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Ensure', 'Epitopes', 'Experimental Designs', 'Growth', 'Information Resources', 'Link', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Methods', 'Microfluidic Microchips', 'Microfluidics', 'Modeling', 'Molecular Profiling', 'PTPN11 gene', 'Peptide Mapping', 'Peptide Signal Sequences', 'Peptides', 'Phase', 'Phosphopeptides', 'Phosphorylation Site', 'Phosphotransferases', 'Process', 'Protein Binding', 'Protein Kinase', 'Proteins', 'Proteomics', 'Publishing', 'Research Personnel', 'Resource Informatics', 'Sampling', 'Signal Transduction', 'Software Tools', 'Staging', 'System', 'Technology', 'Testing', 'Time', 'Work', 'Writing', 'anticancer research', 'base', 'biological systems', 'database design', 'density', 'design', 'interest', 'meetings', 'microchip', 'novel', 'programs', 'protein aminoacid sequence', 'protein expression', 'protein function', 'protein profiling', 'research study', 'response', 'src Homology Region 2 Domain', 'technology development', 'tool', 'web-accessible']",NCI,UNIVERSITY OF HOUSTON,R33,2009,494433,31980265,0.02201046504312122
"BUILDING AND VALIDATING LOCATION PROTEOMICS DATABASES The primary goal of this proposal is to collect high-resolution information on the distribution of proteins within mammalian cells and to link it to nucleotide and protein sequences. It builds on extensive prior work on development of protein tagging methods by the co-PIs and on development of software systems for automated analysis of subcellular patterns in fluorescence microscope images by the PI. 25,000 independent cell lines expressing GFP protein fusions will be created in NIH 3T3 cells using high-throughput CD-tagging (protein-trapping) methods. As the cell lines are created, high-resolution fluorescence microscope images will be collected using fluorescence microscopy and the gene and protein tagged in each cell line will be determined by high-throughput molecular analysis methods. The images will be subjected to automated, computerized image analysis to group proteins with statistically indistinguishable patterns. The determined location for each protein will be compared to whatever information is available from protein databases, journal articles and location predictors. Each assigned location will be accompanied by a confidence estimate derived from combining these sources. In addition, the images for each protein group will be used to build generative models that can synthesize new protein distributions statistically equivalent to the original images. The ability to synthesize distributions will provide an important structural framework for systems biology modeling of cell behavior in normal and disease states. n/a",BUILDING AND VALIDATING LOCATION PROTEOMICS DATABASES,7813483,R01GM075205,"['Active Learning', 'Address', 'American', 'Automation', 'Behavior', 'Biological Assay', 'Biological Markers', 'Cell Culture Techniques', 'Cell Line', 'Cells', 'Coin', 'Databases', 'Dependency', 'Disease', 'Drug Delivery Systems', 'Employment', 'Fill-It', 'Fluorescence Microscopy', 'Funding', 'Genes', 'Goals', 'Grant', 'Heart', 'Image', 'Image Analysis', 'Knowledge', 'Learning', 'Location', 'Machine Learning', 'Measures', 'Methods', 'Microscopy', 'Modeling', 'Molecular Bank', 'Mutation', 'NIH 3T3 Cells', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Principal Investigator', 'Proteins', 'Proteome', 'Proteomics', 'Recovery', 'Resolution', 'Robotics', 'Running', 'Screening procedure', 'Statistical Models', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'United States National Institutes of Health', 'Work', 'abstracting', 'base', 'cell behavior', 'drug discovery', 'predictive modeling', 'programs', 'protein function', 'public health relevance', 'research study', 'response', 'two-dimensional']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2009,510300,30434536,0.004243388664506061
"Metabolomic Assessment of Estrogenic Endocrine Disruptor    DESCRIPTION (provided by applicant)     Estrogenic endocrine disruptors (EEDs) are a group of structurally diverse compounds that include pharmaceuticals, dietary supplements, industrial chemicals and environmental contaminants.  They can elicit a number of adverse health effects such as hormone dependent cancers, reproductive tract abnormalities, compromised reproductive fitness, and impaired cognitive abilities.  In order to fully assess the potential adverse effects of synthetic and natural EEDs, a more comprehensive understanding of their molecular, metabolic, and tissue level effects is required within the context of a whole organism.  This collaborative proposal will elucidate the pathways, networks and signaling cascades perturbed by EEDs using the complementary multidisciplinary expertise of its team members in the areas of toxicology, molecular biology, endocrinology, multinuclear NMR spectroscopy, data management and advanced data analysis.  The comparative effects of ethynyl estradiol (EE), genistein (GEN), and o, p'-dichlorodiphenyltrichloroethane (DDT) on metabolite levels will be assessed in urine, serum and liver extracts by multinuclear (i. e., 1H, 13C, 31P) NMR spectroscopy, and complemented with histopathology examination and gene expression data from ongoing microarray studies in both mouse and rat models.  All data will be stored and archived in dbZach, a MIAME-compliant toxicogenomic supportive database that facilitates data analysis, the integration of disparate data sets, the exchange of data between investigators, and the deposition of data into public repositories.  Advanced statistical approaches, modeling and data integration tools such as neural networks, data fusion, and Baysean inference will be used to fuse these disparate data sets in order to elucidate the conserved biological networks that are of importance in response to endogenous estrogens.  Moreover, EED perturbed pathways associated with elicited effects will be further defined.  Results from these studies will not only further define the physiologic and toxic mechanisms of action of estrogenic compounds but will also demonstrate the synergy of fusing complementary microarray, metabolomic and histopathology data into a comprehensive integrative computational model.  This approach will also demonstrate the ability to maximize knowledge extraction from all disparate data available within the proposed innovative data management system when used with the advanced information tools that will be developed.            n/a",Metabolomic Assessment of Estrogenic Endocrine Disruptor,7625039,R01ES013927,"['Adverse effects', 'Affect', 'Apical', 'Archives', 'Area', 'Biochemical Pathway', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Cell Proliferation', 'Chemicals', 'Classification', 'Clinical Chemistry', 'Cognitive', 'Complement', 'Computer Simulation', 'Data', 'Data Analyses', 'Data Base Management', 'Data Set', 'Databases', 'Deposition', 'Development', 'Disease Progression', 'Dose', 'Endocrine Disruptors', 'Endocrinology', 'Engineering', 'Environmental Pollution', 'Estradiol', 'Estrogens', 'Funding', 'Future', 'Gene Expression', 'Genistein', 'Health', 'Hepatic', 'Histopathology', 'Hormones', 'Knowledge Extraction', 'Lead', 'Link', 'Lipids', 'Liver Extract', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Metabolic', 'Metabolism', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Profiling', 'Monitor', 'Multinuclear NMR', 'Mus', 'NMR Spectroscopy', 'Organ Weight', 'Outcome', 'Pathway interactions', 'Pattern Recognition', 'Pharmacologic Substance', 'Physiological', 'Principal Investigator', 'Process', 'Rattus', 'Reporting', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Assessment', 'Rodent', 'Sampling', 'Screening procedure', 'Serum', 'Signal Transduction', 'System', 'Techniques', 'Time', 'Tissues', 'Toxic effect', 'Toxicogenomics', 'Toxicology', 'Urine', 'Whole Organism', 'aqueous', 'comparative', 'data exchange', 'data format', 'data integration', 'data management', 'dichlorodiphenyltrichloroethane', 'dietary supplements', 'estrogenic endocrine disruptor', 'experience', 'fitness', 'innovation', 'member', 'metabolic abnormality assessment', 'metabolomics', 'multidisciplinary', 'programs', 'repository', 'reproductive', 'research study', 'response', 'tool']",NIEHS,MICHIGAN STATE UNIVERSITY,R01,2009,536571,89938253,-0.012491575304870365
"Robust Classification Methods for Categorical Regression    DESCRIPTION (provided by applicant): Improving statistical methods to provide better classification performance and new analytical capabilities for categorical regression would be invaluable to the medical and health care research communities. Categorical regression models (e.g., binary logistic, multinomial logistic) are used extensively to identify patterns of alcohol-related symptoms, screen for disorders, and assess policies. In addition, such models are used extensively in other areas of research such as mental illness, cancer, traumatic injuries, and AIDS-related pathologies. However, many such models are developed with inadequate support to fully analyze and exploit the intrinsically probabilistic nature of their results. This is of critical importance as health researchers, clinicians, and administrators are often faced with classification decisions using categorical regression models to identify unacceptable risks, adequate outcomes, and acceptable guidelines for screening, diagnoses, treatment, and quality of care. Commercially available statistical software does not offer sophisticated methods for robust estimation of posterior probabilities in the presence of model misspecification, missing covariates, and nonignorable missing data generating processes. Such robust missing data handling methods provide natural mechanisms for dealing with verification bias and modeling correlated, longitudinal, or survey data with complex sampling designs. Moreover, commercially available statistical software does not provide automated methods for using estimated posterior probabilities to make optimal classification decisions with respect to different optimality criteria. In particular, automated features such as optimizing multiple decision criteria (allocation rules) that trade off specificity against sensitivity, decision threshold confidence intervals, statistical tests for evaluating correct specification of posterior probabilities, statistical tests for comparing competing classifier thresholds, and methods for multi-outcome classification and inference are not readily available. Phase II research will extend Phase I findings for binary logistic regression to develop and implement automated robust classification methods for multinomial logistic regression modeling, which also applies to the larger class of nonlinear categorical regression models that output posterior probabilities. The Phase II software prototype will provide: 1) new user-selectable robust decision threshold estimators, 2) robust confidence intervals on decision threshold estimators, 3) new classifier threshold comparison tests, 4) new outcome probability specification tests, 5) efficient missing data handling methods in the presence of nonignorable nonresponse data, and 6) second-order analytic and simulation-based Bayesian methods for improved small sample and rare event outcome probability estimation. These new methodologies will be integrated into a prototype user-friendly software package, evaluated with extensive simulation studies, and then applied to real world classification problems encountered in: alcohol, mental illness (depression, bipolar, schizophrenia), cancer (prostate), trauma (emergency room), and infectious disease (AIDS) through collaborations with domain experts in those respective fields. In summary, Phase II research will establish the essential technical foundation for Phase III commercialization with the objective of providing a suite of new classification analysis methods as an advanced statistical tool that improves epidemiologic, clinical, and public health research.                 n/a",Robust Classification Methods for Categorical Regression,7686932,R44CA139607,"['Accident and Emergency department', 'Achievement', 'Address', 'Administrator', 'Agreement', 'Alcohols', 'Algorithms', 'Area', 'Bayesian Method', 'Behavior', 'Bipolar Depression', 'Classification', 'Clinical', 'Clinical Investigator', 'Collaborations', 'Communicable Diseases', 'Communities', 'Complex', 'Computer software', 'Confidence Intervals', 'Data', 'Data Analyses', 'Data Set', 'Decision Analysis', 'Decision Making', 'Development', 'Diagnosis', 'Disease', 'Empirical Research', 'Engineering', 'Epidemiologic Studies', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Foundations', 'Goals', 'Guidelines', 'Health', 'Health Services Research', 'Healthcare', 'Industry', 'Information Resources Management', 'Injury', 'Jordan', 'Journals', 'Knowledge', 'Literature', 'Logistic Regressions', 'Logistics', 'Malignant Neoplasms', 'Malignant neoplasm of prostate', 'Medical', 'Mental Health', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Outcome', 'Output', 'Pathology', 'Pattern', 'Peer Review', 'Performance', 'Phase', 'Phase I Clinical Trials', 'Phase II Clinical Trials', 'Policies', 'Preparation', 'Probability', 'Process', 'Publishing', 'Quality of Care', 'Relative (related person)', 'Research', 'Research Personnel', 'Risk', 'Robin bird', 'Sampling', 'Schizophrenia', 'Screening procedure', 'Sensitivity and Specificity', 'Simulate', 'Software Tools', 'Specific qualifier value', 'Specificity', 'Statistical Methods', 'Surveys', 'Symptoms', 'Technology', 'Testing', 'Trauma', 'anticancer research', 'base', 'commercialization', 'computerized data processing', 'density', 'design', 'graphical user interface', 'improved', 'innovation', 'prototype', 'public health research', 'simulation', 'software development', 'theories', 'tool', 'user friendly software']",NCI,MARTINGALE RESEARCH CORPORATION,R44,2009,957937,0,0.006766502232110227
"Development of a Research-Ready Pregnancy and Newborn Biobank in California    DESCRIPTION (provided by applicant): Development of a Research-Ready Pregnancy and Newborn Biobank in California Population-based biobanks are a critical resource for identifying disease mechanisms and developing screening tests for biomarkers associated with certain disorders. The California Department of Public Health has been banking newborn specimens statewide since 1982 (N~14 million) and maternal prenatal specimens for a portion of the state since 2000 (N~1 million), creating one of the largest, if not the largest single biological specimen banks with linked data in the world. With the fast pace of new knowledge in genetics and laboratory methods, the demand for specimens and data from researchers around the world now far surpasses the Department's ability to fill them. The goal of this infrastructure development project is to create an efficient, high throughput, low cost newborn screening and prenatal/maternal screening specimen biobank and linked data base that could be used by large numbers of researchers around the world for a wide range of studies through the following aims: (1) establishment of highly efficient protocols and procurement and integration of automated systems for pulling and processing specimens; (2) development of an integrated specimen tracking system into the Department's existing web-based Screening Information System; (3) development of a computerized system to track application requests for specimens and data; and (4) development of a linked screening program-vital records database that is organized into a life course, client based system. These aims will be accomplished through expansion of the Department's award-winning Screening Integration System to include web-based tracking of specimens and research requests, and use of an innovative machine-learning record matching application for high-performance linkages. After the 2 year grant period is completed, the California Research Ready Biospecimen Bank will be able to provide researchers with valuable biological specimens in a timely, cost-effective manner, thereby enabling a dramatic expansion of epidemiological research nationwide. The continuity of the system will be ensured by codifying human subjects-sensitive policies and procedures into Departmental regulations and by charging researchers modest fees for specimens, data and other research services.      PUBLIC HEALTH RELEVANCE: Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.           Program Narrative Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.",Development of a Research-Ready Pregnancy and Newborn Biobank in California,7853378,RC2HD065514,"['Area', 'Award', 'Biological', 'Biological Markers', 'Biological Specimen Banks', 'California', 'Case-Control Studies', 'Cessation of life', 'Charge', 'Client', 'Data', 'Data Files', 'Databases', 'Development', 'Disease', 'Ensure', 'Epidemiology', 'Family Study', 'Fees', 'Fetal Death', 'Funding', 'Future', 'Genetic', 'Goals', 'Government', 'Grant', 'Information Systems', 'Infusion procedures', 'Knowledge', 'Laboratories', 'Laws', 'Life Cycle Stages', 'Link', 'Live Birth', 'Machine Learning', 'Methods', 'Multiple Pregnancy', 'Neonatal Screening', 'Newborn Infant', 'Online Systems', 'Performance', 'Policies', 'Population', 'Pregnancy', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Public Health', 'Records', 'Regulation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Screening procedure', 'Services', 'Specimen', 'Specimen Handling', 'Study Subject', 'System', 'Systems Integration', 'Testing', 'Time', 'Woman', 'base', 'biobank', 'cohort', 'computerized', 'cost', 'human subject', 'infrastructure development', 'innovation', 'population based', 'prenatal', 'programs', 'public health relevance']",NICHD,SEQUOIA FOUNDATION,RC2,2009,2003191,0,-0.016572470360205052
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.          n/a",Scalable Learning with Ensemble Techniques and Parallel Computing,7433144,R44GM083965,"['Adoption', 'Algorithms', 'Architecture', 'Arts', 'Biological Sciences', 'Biomedical Research', 'Cations', 'Class', 'Classification', 'Communication', 'Communities', 'Companions', 'Complex', 'Computer software', 'Computers', 'Consult', 'Data', 'Data Set', 'Databases', 'Detection', 'Diagnosis', 'Disease', 'Effectiveness', 'Emerging Technologies', 'Ensure', 'Fostering', 'Foundations', 'Future', 'Generations', 'Goals', 'Graph', 'Grouping', 'Imagery', 'Knowledge', 'Language', 'Learning', 'Libraries', 'Machine Learning', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Numbers', 'Performance', 'Personal Satisfaction', 'Phase', 'Prevention', 'Problem Solving', 'Program Development', 'Public Health', 'Randomized', 'Range', 'Research', 'Research Infrastructure', 'Research Personnel', 'Running', 'Scheme', 'Simulate', 'Software Design', 'Software Tools', 'Speed', 'Structure', 'Techniques', 'Technology', 'Testing', 'Today', 'Training', 'Voting', 'Work', 'base', 'computerized tools', 'data mining', 'design', 'forest', 'improved', 'innovation', 'next generation', 'parallel computing', 'programs', 'prototype', 'research and development', 'response', 'software development', 'sound', 'statistics', 'theories', 'tool']",NIGMS,INSIGHTFUL CORPORATION,R44,2008,25548,0,0.024523114682297353
"Computational Models of Infectious Disease Threats DESCRIPTION (provided by applicant):  Microbial threats, including bioterrorism and naturally emerging infectious diseases, pose a serious challenge to national security in the United States and to health worldwide.  This proposal describes the creation of a center for computational modeling of infectious diseases at the Johns Hopkins Bloomberg School of Public Health, with the collaboration of key experts at the Brookings Institution, the National Aeronautic and Space Administration, the University of Maryland, and Imperial College (London).  The overarching aim of this project is to integrate the most advanced and powerful techniques of epidemiological data analysis with those of computer simulation (agent-based modeling) to produce a unified computational epidemiology that is scientifically sound, highly visual and user-friendly, and responsive to biosecurity and public health policy requirements.  Data analysis will be guided by the insight that epidemic patterns over space and time can be approached as nearly decomposable systems, in which frequency components of the incidence signal can be isolated and studied.  Wavelet transforms, and empiric mode decomposition using Hilbert-Huang Transforms, will be used to sift nonlinear, nonstationary epidemiological data, allowing frequency band patterns to be defined.  Isolated frequency modes will then be associated with external forcing (weather, social contact patterns) and internal dynamics (Kermack-McKendrick predator-prey models).  Results of the epidemiological data decomposition analysis, along with the knowledge of infectious disease experts, will instruct the creation and development of agent-based models.  Such models feature populations of mobile individuals in artificial societies that interact locally with other individuals.  Features of the basic model include variable social network structures, individual susceptibility and immunity, incubation periods, transmission rates, contact rates, and other selectable parameters.  After the agent-based model is calibrated to generate epidemic patterns consistent with real world epidemiology, preventive strategies including vaccination, contact tracing, isolation, quarantine, and other public health measures will be systematically introduced and their impact evaluated.  Methods will be developed for assessing the utility of individual models, and for making decisions based on combined results from more than one model.  Infectious diseases to be studied initially include smallpox, SARS, dengue, West Nile, and unknown but hypothetically plausible agents.  As part of a Cooperative Agreement, the Center will work with other research groups, a bioinformatics core group, and the NIGMS to develop data sets, software and methods, agent-based models, and visualization tools.  In an infectious disease epidemic emergency the Center will redirect its activities to serve the nation's security, as guided by the NIGMS. n/a",Computational Models of Infectious Disease Threats,7688793,U01GM070708,"['AIDS therapy', 'AIDS/HIV problem', 'Academy', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Airborne Particulate Matter', 'Algorithms', 'American', 'Americas', 'Animal Experimentation', 'Appendix', 'Archives', 'Area', 'Arthropod Vectors', 'Award', 'Bacteria', 'Beds', 'Bioinformatics', 'Biological', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Books', 'Borrelia', 'Climate', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Condition', 'Contact Tracing', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Decision Theory', 'Demography', 'Dengue', 'Dengue Hemorrhagic Fever', 'Detection', 'Development', 'Dialysis procedure', 'Disease', 'Docking', 'Doctor of Medicine', 'Doctor of Philosophy', 'Earthquakes', 'Ecology', 'Economics', 'Educational process of instructing', 'Ehrlichia', 'Emergency Situation', 'Emerging Communicable Diseases', 'Encephalitis', 'Engineering', 'Environmental Engineering technology', 'Environmental Health', 'Epidemic', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'Event', 'Evolution', 'Facility Construction Funding Category', 'Faculty', 'Foot-and-Mouth Disease', 'Frequencies', 'Game Theory', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Geography', 'Glass', 'Goals', 'HIV', 'Hantavirus', 'Head', 'Health', 'Health Policy', 'Healthcare', 'Hepatitis E', 'Human', 'Human Resources', 'Hygiene', 'Imagery', 'Immunity', 'Immunology', 'Incidence', 'Individual', 'Infectious Agent', 'Infectious Disease Epidemiology', 'Influenza', 'Informatics', 'Information Services', 'Institute of Medicine (U.S.)', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internal Medicine', 'International', 'Internet', 'Intervention', 'Joints', 'Journals', 'Knowledge', 'Laboratories', 'Laboratory Research', 'Laboratory Study', 'Lead', 'Legal patent', 'Leptospira', 'Libraries', 'Location', 'London', 'Lung', 'Machine Learning', 'Maintenance', 'Malaria', 'Maryland', 'Master&apos', 's Degree', 'Mathematical Biology', 'Mathematics', 'Measles', 'Measures', 'Mechanics', 'Methods', 'Microbiology', 'Military Personnel', 'Modeling', 'Modified Smallpox', 'Molecular', 'National Institute of General Medical Sciences', 'National Security', 'New York', 'Nonlinear Dynamics', 'Nonparametric Statistics', 'Observational Study', 'Oceanography', 'Outcome', 'Paper', 'Pattern', 'Physical Dialysis', 'Play', 'Policies', 'Policy Maker', 'Population', 'Positioning Attribute', 'Predisposition', 'Pregnancy Outcome', 'Prevention strategy', 'Preventive', 'Principal Investigator', 'Prion Diseases', 'Procedures', 'Process', 'Provider', 'Proxy', 'Public Health', 'Public Health Schools', 'Public Policy', 'Publications', 'Publishing', 'Purpose', 'Quarantine', 'Rate', 'Recording of previous events', 'Reference Standards', 'Relative (related person)', 'Research', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Rickettsia', 'Risk Assessment', 'Rodent', 'Role', 'Route', 'Schedule', 'Schools', 'Science', 'Scientist', 'Screening procedure', 'Security', 'Series', 'Severe Acute Respiratory Syndrome', 'Signal Transduction', 'Simulate', 'Smallpox', 'Social Network', 'Social Sciences', 'Societies', 'Software Tools', 'Space Flight', 'Statistical Computing', 'Statistical Models', 'Structure', 'Students', 'System', 'Systems Analysis', 'Testing', 'Theoretical model', 'Time', 'Time Series Analysis', 'Training', 'Tropical Medicine', 'U-Series Cooperative Agreements', 'Uncertainty', 'United States', 'United States National Academy of Sciences', 'United States National Aeronautics and Space Administration', 'Universities', 'Vaccination', 'Variant', 'Vector-transmitted infectious disease', 'Violence', 'Viral', 'Viral Hemorrhagic Fevers', 'Virus', 'Virus Diseases', 'Visual', 'Weather', 'West Nile virus', 'Work', 'base', 'biosecurity', 'c new', 'college', 'computer science', 'concept', 'design', 'disease natural history', 'disease transmission', 'disorder prevention', 'disorder risk', 'editorial', 'experience', 'improved', 'indexing', 'infectious disease model', 'insight', 'interest', 'mathematical model', 'member', 'microbial', 'models and simulation', 'network models', 'pathogen', 'peer', 'predictive modeling', 'prevent', 'professor', 'programs', 'remote sensing', 'respiratory', 'simulation', 'skills', 'social', 'social organization', 'sound', 'theories', 'tool', 'transmission process', 'user-friendly', 'vaccination strategy']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2008,58299,570146095,0.011193141900597737
"Clinical Cytometry Analysis Software with Automated Gating    DESCRIPTION (provided by applicant): The proposed Clinical Cytometry Analysis Software Project described in this grant application is designed to create a new, more efficient and effective way of analyzing cells for the presence of cancer, HIV/AIDS and other disease, using a fully automated software system. Using modern data mining techniques (pattern recognition, feature recognition, image analysis) we will design software which will analyze data (the cell samples from patients) at a much faster rate and with fewer false positives and negatives than the manual method now in use. Objectives: Assemble and validate algorithms in software that can automatically classify regions of interest in flow cytometry data. We will demonstrate that the particular populations required by our use cases can be validly, rigorously and repeatably identified automatically. Develop and validate graphical and statistical results that satisfy FDA requirements for medical device software, simplify regulatory compliance by the clinical user, and automatically deliver analysis results to diagnostic expert systems and/or LIMS systems. Satisfy the  translational medicine  goals outlined in the NIH Roadmap. This software will bring the clinician streamlined testing currently only available in research labs. Methods: Four use cases have been selected, one employing synthetic data and three clinical data; Leukemia/Lymphoma test, Analysis of longitudinal Graft vs. Host Disease (GvHD) in bone marrow transplant specimens for predictive markers and HIV/AIDS - Gag-specific T cell cytokine response profile assay. For each we have access to a substantial body of existing data, analyzed by experts. Beginning with the autogating routines in our own FlowJo software, we will test and expand the application of Magnetic gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural and Support Vector Machines (SVM). Using a sampling of human operators to establish a control range, we will test each of these five techniques against the four use cases in cooperation with our collaborators. Events in the manually classified samples are given a weighted score based on the frequency with which they are included by all the operators. A single operator's score or the gating algorithm's score is compared with the cumulative score of the expert group and a match rating is computed. Additional validation techniques include combinatory validation on internal measures with respect to Pareto optimality, and Predictive Power/Stability self consistency checks using resampled or perturbed data measured with external indices such as the adjusted Rand index and the Variation of Information index.  PUBLIC HEALTH RELEVANCE: By eliminating the operator's time, we estimate that the cost of clinical flow cytometry analysis can be reduced to half the current figure while delivering the results much faster. By eliminating the subjectivity and human error of manually created regions and reducing the range of variability of the so created, there would result fewer false positives and false negatives, improving the clinical outcome for those patients needing therapy but undetected by current methods. An order of magnitude increase in speed means faster therapeutic intervention.  A less expensive test improves outcome by making the test accessible to more patients.          n/a",Clinical Cytometry Analysis Software with Automated Gating,7482923,R43RR024094,"['AIDS/HIV problem', 'Algorithms', 'Applications Grants', 'B-Lymphocytes', 'Biological Assay', 'Biological Neural Networks', 'Bone Marrow Transplantation', 'Cells', 'Characteristics', 'Class', 'Classification', 'Clinical', 'Clinical Data', 'Cluster Analysis', 'Complex', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Cytometry', 'Data', 'Data Analyses', 'Data Files', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Educational process of instructing', 'Environment', 'Evaluation', 'Event', 'Expert Systems', 'Facility Construction Funding Category', 'Flow Cytometry', 'Frequencies', 'Funding', 'Future', 'Gagging', 'Generations', 'Goals', 'Grant', 'Graph', 'Human', 'Image Analysis', 'Individual', 'Instruction', 'Knowledge', 'Legal patent', 'Life Cycle Stages', 'Machine Learning', 'Magnetism', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Medical', 'Medical Device', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Noise', 'Numbers', 'Outcome', 'Output', 'Patients', 'Pattern', 'Pattern Recognition', 'Performance', 'Phase', 'Population', 'Probability', 'Process', 'Public Health', 'Publishing', 'Range', 'Rate', 'Regulation', 'Reporting', 'Research', 'Sample Size', 'Sampling', 'Scientist', 'Score', 'Software Design', 'Software Engineering', 'Software Tools', 'Solutions', 'Specific qualifier value', 'Specimen', 'Speed', 'Standards of Weights and Measures', 'Structure', 'System', 'T-Lymphocyte', 'Target Populations', 'Techniques', 'Testing', 'Therapeutic Intervention', 'Time', 'Training', 'Tube', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Voting', 'Weight', 'base', 'clinical Diagnosis', 'commercialization', 'cost', 'cytokine', 'data mining', 'design', 'improved', 'indexing', 'innovation', 'interest', 'leukemia/lymphoma', 'novel', 'predictive modeling', 'relating to nervous system', 'research study', 'response', 'software systems', 'statistics', 'tool', 'translational medicine']",NCRR,"TREE STAR, INC.",R43,2008,100854,0,-0.020892195778634187
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.          n/a",Scalable Learning with Ensemble Techniques and Parallel Computing,7748401,R44GM083965,"['Learning', 'Techniques', 'parallel computing']",NIGMS,INSILICOS,R44,2008,143361,0,0.024523114682297353
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,7386333,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Allografting', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Classification', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'None or Not Applicable', 'Numbers', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Personal Satisfaction', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Purpose', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'base', 'cancer microarray', 'cancer type', 'design', 'desire', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2008,255036,340417756,-0.006401210586271298
"Protein Packing Defects as Functional Markers and Drug Targets Our preliminary structure-based investigations show that water exclusion from deficiently packed hydrogen bonds and other pre-formed electrostatic interactions constitutes a driving factor conferring high specificity to protein association. Thus, an evolutionary conserved feature, the under-dehydrated hydrogen bond, termed dehydron, appears to be a structural marker for interactivity. Dehydrons were experimentally and statistically shown to constitute sticky spots on the protein surface and to be abundant at protein-protein interfaces, especially at those that cannot be understood in terms of standard interactions. The dehydron distribution on the surface of soluble proteins constitutes a determinant of the propensity for association and aberrant aggregation. The identification of dehydrons has relied so far on detailed structural information, a limitation precluding a proteomic analysis. This proposal is geared at introducing a sequence- based predictive method to establish the biological relevance of dehydrons and their potential as markers for drugable targets. Thus, we intend to introduce a powerful unsupervised scanning technology to detect signals of interactivity and drugability at a genomic scale. This goal requires constructing a machine-learning discriminator trained on a structural database. The over-all aim is to develop a sequence-based multi-purpose tool to expand the universe of drugable targets, diagnose propensity for aberrant aggregation and make interactomic inferences. The efficacy of our predictor will be tested on five grounds: a) Assaying for amino-acid variability and determining whether residues predicted solely from sequence to be engaged in dehydrons are actually conserved, b) Using a redundancy-free curated PDB sample as training set, we shall determine the accuracy and precision of the sequence-based predictor using a nonhomologous PDB complement set and annotated SwissProt entries as testing sets, c) Contrasting our results with an alternative dehydron predictor based on a reliable sequence-based predictor of native disorder (PONDR¿). This dehydron predictor is based on a correlation found between the extent of hydrogen-bond packing and the score of structural disorder, d) Contrasting sequence-based diagnosis of amyloidogenic aggregation with SwissProt annotations and other annotated disease-related sequence repositories; e) Contrasting compiled drug-target quality assessments and structural data and screening profiles for protein-ligand associations with the predicted dehydron patterns. Thus, the novel design concept of ""drug inhibitor as a wrapper of functional packing defects"" will be explored and validated. n/a",Protein Packing Defects as Functional Markers and Drug Targets,7490397,R01GM072614,"['Address', 'Adopted', 'Amino Acids', 'Area', 'Automobile Driving', 'Base Sequence', 'Binding', 'Binding Sites', 'Biological', 'Biological Assay', 'Biology', 'Chemicals', 'Code', 'Collaborations', 'Complement', 'Complex', 'Condition', 'Data', 'Databases', 'Defect', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug Delivery Systems', 'Electrostatics', 'Evolution', 'Excision', 'Exclusion', 'Exhibits', 'Genome', 'Genomics', 'Goals', 'Gray unit of radiation dose', 'Homologous Gene', 'Hydrogen Bonding', 'Individual', 'Investigation', 'Libraries', 'Ligands', 'Light', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Membrane Proteins', 'Methods', 'Molecular', 'Nature', 'Numbers', 'Paper', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Physiological', 'Positioning Attribute', 'Principal Investigator', 'Proteins', 'Proteomics', 'Public Domains', 'Purpose', 'Qualifying', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Sampling', 'Scanning', 'Score', 'Screening procedure', 'Sequence Alignment', 'Signal Transduction', 'Site', 'Specificity', 'Spottings', 'Standards of Weights and Measures', 'Statistical Study', 'Structure', 'Surface', 'SwissProt', 'Technology', 'Testing', 'Training', 'Validation', 'Variant', 'Vertebral column', 'Water', 'base', 'computerized tools', 'concept', 'design', 'drug discovery', 'improved', 'inhibitor/antagonist', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'repository', 'size', 'stem', 'success', 'tool']",NIGMS,RICE UNIVERSITY,R01,2008,275880,19199034,0.0033393231211999055
"Building motif lexicons    DESCRIPTION (provided by applicant):  With the complete sequencing of numerous genomes and the annotation of proteomes, one of the next major challenges in biology is to understand the functions and integration of the encoded proteins (a NIH Roadmap area of emphasis). Deciphering protein function is a very time consuming, expensive process, as reflected by the disproportionately low percentage of proteins with well-established functions. One approach for extrapolating established functions to new proteins is to predict short motifs. Short motifs target proteins for post-translational modification, trafficking to cellular compartments, and binding to other proteins or molecules. Our cross-disciplinary team has built Minimotif Miner (MnM), a short motif database and platform- independent web-tool that identifies motif consensus sequences in protein queries and thus potential new protein functions (http://mnm.engr.uconn.edu/). MnM can also be used to develop new hypotheses of how specific mutations cause human disease and to identify putative targets for the development of therapeutic drugs, antibiotics, insecticides, and antiviral agents. Despite the utility of MnM and other motif resources, prediction of functional motifs still has two major limitations, which we address in this proposal. 1) To reduce the false-positive prediction of motifs, we have created a new language that allows us to consider the 3- dimensional structural conservation of motifs. For each motif, we will build specific motif definitions by combining experimental data with data from motif structures in the Protein Data Bank. We will also determine the sequence permutations that can form the observed motif structure by using molecular dynamic simulations. 2) To build a more comprehensive motif database we will use artificial intelligence to mine PubMed. The expert system will use automated literature screening, document summarization, and motif identification efficiency score to extract the majority of known motifs from PubMed. Addressing these limitations will vastly increase the utility of short motif prediction.              n/a",Building motif lexicons,7380093,R01GM079689,"['3-Dimensional', 'Adaptor Signaling Protein', 'Address', 'Amino Acids', 'Antibiotics', 'Antiviral Agents', 'Area', 'Artificial Intelligence', 'Binding', 'Biochemical', 'Biology', 'Code', 'Consensus', 'Consensus Sequence', 'Data', 'Databases', 'Development', 'Expert Systems', 'Genome', 'Growth Factor', 'Human', 'Insecticides', 'Internet', 'Language', 'Letters', 'Literature', 'Measures', 'Mining', 'Molecular Conformation', 'Mutation', 'Nomenclature', 'Numbers', 'Paper', 'Performance', 'Post-Translational Protein Processing', 'Process', 'Protein Binding', 'Proteins', 'Proteome', 'PubMed', 'Reading', 'Research Personnel', 'Resources', 'Score', 'Screening procedure', 'Series', 'Signal Transduction', 'Structure', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'base', 'data mining', 'human disease', 'indexing', 'molecular dynamics', 'platform-independent', 'programs', 'protein function', 'protein structure', 'src Homology Region 2 Domain', 'three dimensional structure', 'tool', 'trafficking']",NIGMS,UNIVERSITY OF CONNECTICUT SCH OF MED/DNT,R01,2008,280033,65553150,0.03282286667460869
"Improve predictions of structure and function by PredictProtein    DESCRIPTION (provided by applicant):       SUMMARY: Over 25,000 researchers in the US and over 50,000 in 120 other countries have exploited the PredictProtein (PP) Internet server to analyze proteins by homology-transfer and by eye novo predictions of protein structure and function. Here, we propose technical and scientific solutions that will improve the functionality of PP and its extension portal META-PP. Many technical changes will remain hidden to users and are required to increase the maintainability, scalability, and portability of these servers. New Graphical User Interfaces are one proposed solution that will visibly impact the service. The scientific solutions address two related tasks pertaining to the prediction of structure and function. The first is to predict the effect of mutations. We propose the development of novel machine learning-based methods to distinguish between mutations that affect structure, function, or have no apparent phenotype. Our final method will be applied to the screening of SNP data from our experimental colleagues at Columbia, as well as to the prediction of SNP effects in public databases. The second major task is the identification of natively unstructured regions and their functional classification. Proteins that do not adopt regular structures in isolation are increasingly becoming an important research area; they may provide a key to the evolution of complexity from prokaryotes to eukaryotes. We propose the development of a machine learning-based identification of features specific to this important class of molecules. We also plan to attack the problem from a very different angle by using predictions of interaction densities inside proteins. The resulting novel tools will allow a proteome-wide analysis of the role of these molecules. All methods will be made available through PP.      RELEVANCE: Information about protein structure adds an entire dimension to protein analysis and genome annotation. This addition is often essential to infer function even for natively unstructured proteins. The PredictProtein server is unique in its combination and exploitation of evolution, structure, and function; many thousands of theoretical, experimental, and clinical researches have benefited from this. The long-term goal of the research proposed here is to improve our ability to use the evolutionary record of amino acid substitutions, i.e. to ultimately understand the amino acid ""language"". The short-term goal is to address two tasks that are closely related to human diseases, namely the distinction between silent and important mutations and the mapping of unstructured proteins onto networks and diseases.          n/a",Improve predictions of structure and function by PredictProtein,7389563,R01LM007329,"['Address', 'Adopted', 'Affect', 'Amino Acid Substitution', 'Amino Acids', 'Area', 'Biology', 'Budgets', 'Class', 'Classification', 'Clinical Research', 'Code', 'Communities', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Devices', 'Dimensions', 'Disease', 'Eukaryota', 'Eukaryotic Cell', 'Evolution', 'Eye', 'Funding', 'Genome', 'Genomics', 'Goals', 'Grant', 'Induced Mutation', 'Internet', 'Investigation', 'Language', 'Linux', 'Machine Learning', 'Maintenance', 'Maps', 'Medical', 'Membrane', 'Methods', 'Mutation', 'Nucleotides', 'Numbers', 'Output', 'Phenotype', 'Plug-in', 'Point Mutation', 'Progress Reports', 'Prokaryotic Cells', 'Property', 'Protein Analysis', 'Proteins', 'Proteome', 'Range', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Screening procedure', 'Sequence Analysis', 'Services', 'Solutions', 'Standards of Weights and Measures', 'Structure', 'Testing', 'Training', 'base', 'data modeling', 'density', 'design', 'graphical user interface', 'human disease', 'improved', 'novel', 'portability', 'protein function', 'protein protein interaction', 'protein structure', 'protein structure function', 'protein structure prediction', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2008,320418,558628098,0.028968897504906423
"Robust computational framework for predictive ADME-Tox modeling    DESCRIPTION (provided by applicant):    This proposal seeks to establish a universally applicable and robust predictive ADME-Tox modeling framework based on rigorous Quantitative Structure Activity/Property Relationships (QSAR/QSPR) modeling. The framework has been refined in the course of many years of our research in the areas of QSPR methodology development and application to experimental datasets that led to novel analytical approaches, descriptors, model validation schemes, overall QSPR workflow design, and multiple end-point studies. This proposal focuses on the design of optimized QSPR protocols for the development of reliable predictors of critically important ADME-Tox properties. The ADME properties will include, but not limited to, water solubility, membrane permeability, P450 metabolism inhibition and induction, metabolic stability, human intestinal absorption, bioavailability, transporters and PK data; a variety of toxicological end-points vital to human health will be explored; they are available from recent initiatives on development and standardization of toxicity data, such as the US FDA, NIEHS, and EPA DSS-Tox and other database projects. The ultimate goal of this project is sharing both modeling software and specialized predictors with the research community via a web-based Predictive ADME-Tox Portal. The project objectives will be achieved via concurrent development of QSPR methodology (Specific Aim 1), building highly predictive, robust QSPR models of known ADME-Tox properties (Specific Aim 2), and the deployment of both modeling software and individual predictors via a specialized web-portal (Specific Aim 3). To achieve the goals of this project focusing on the development and delivery of specialized tools and rigorous predictors, we have assembled a research team of mostly senior investigators with complimentary skills and track records of accomplishment in the areas of computational drug discovery, experimental toxicology, statistical modeling, and software development and integration; two of the team members have had recent industrial experience before transitioning to academia. To the best of our knowledge, the results of this proposal will lead to the first publicly available in silico ADME-Tox modeling framework and predictors that can be used by the research community to analyze any set of chemicals (i.e., virtual and real compound sets). The framework will have a significant impact on compound prioritization, chemical library design, and candidate selection for preclinical and clinical development.            n/a",Robust computational framework for predictive ADME-Tox modeling,7433931,R21GM076059,"['Academia', 'Acute', 'Address', 'Area', 'Biological Availability', 'Cardiotoxicity', 'Cell Membrane Permeability', 'Chemicals', 'Chronic', 'Clinical', 'Collaborations', 'Communities', 'Computer Simulation', 'Computer software', 'Computers', 'Consensus', 'Cytochrome P450', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Drug Kinetics', 'End Point', 'Ensure', 'Environment', 'Goals', 'Health', 'Hepatotoxicity', 'Human', 'Individual', 'Internet', 'Intestinal Absorption', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Letters', 'Lung', 'Machine Learning', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Organ', 'Pharmacologic Substance', 'Postdoctoral Fellow', 'Property', 'Protocols documentation', 'Quantitative Structure-Activity Relationship', 'Records', 'Recruitment Activity', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Scheme', 'Scientist', 'Screening procedure', 'Secure', 'Source', 'Specialist', 'Standardization', 'Standards of Weights and Measures', 'Statistical Models', 'Statistically Significant', 'Structure', 'Students', 'Techniques', 'Technology', 'Testing', 'Toxic effect', 'Toxicology', 'Training', 'United States Environmental Protection Agency', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Validation', 'base', 'carcinogenicity', 'career', 'cluster computing', 'combinatorial', 'computer framework', 'data mining', 'design', 'drug discovery', 'experience', 'genotoxicity', 'innovation', 'knowledge of results', 'member', 'method development', 'neurotoxicity', 'novel', 'open source', 'pre-clinical', 'programs', 'protocol development', 'reproductive', 'skills', 'small molecule libraries', 'software development', 'tool', 'virtual', 'water solubility']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R21,2008,322087,511185245,-0.006913752135882223
"Computational Models of Infectious Disease Threats DESCRIPTION (provided by applicant):  Microbial threats, including bioterrorism and naturally emerging infectious diseases, pose a serious challenge to national security in the United States and to health worldwide.  This proposal describes the creation of a center for computational modeling of infectious diseases at the Johns Hopkins Bloomberg School of Public Health, with the collaboration of key experts at the Brookings Institution, the National Aeronautic and Space Administration, the University of Maryland, and Imperial College (London).  The overarching aim of this project is to integrate the most advanced and powerful techniques of epidemiological data analysis with those of computer simulation (agent-based modeling) to produce a unified computational epidemiology that is scientifically sound, highly visual and user-friendly, and responsive to biosecurity and public health policy requirements.  Data analysis will be guided by the insight that epidemic patterns over space and time can be approached as nearly decomposable systems, in which frequency components of the incidence signal can be isolated and studied.  Wavelet transforms, and empiric mode decomposition using Hilbert-Huang Transforms, will be used to sift nonlinear, nonstationary epidemiological data, allowing frequency band patterns to be defined.  Isolated frequency modes will then be associated with external forcing (weather, social contact patterns) and internal dynamics (Kermack-McKendrick predator-prey models).  Results of the epidemiological data decomposition analysis, along with the knowledge of infectious disease experts, will instruct the creation and development of agent-based models.  Such models feature populations of mobile individuals in artificial societies that interact locally with other individuals.  Features of the basic model include variable social network structures, individual susceptibility and immunity, incubation periods, transmission rates, contact rates, and other selectable parameters.  After the agent-based model is calibrated to generate epidemic patterns consistent with real world epidemiology, preventive strategies including vaccination, contact tracing, isolation, quarantine, and other public health measures will be systematically introduced and their impact evaluated.  Methods will be developed for assessing the utility of individual models, and for making decisions based on combined results from more than one model.  Infectious diseases to be studied initially include smallpox, SARS, dengue, West Nile, and unknown but hypothetically plausible agents.  As part of a Cooperative Agreement, the Center will work with other research groups, a bioinformatics core group, and the NIGMS to develop data sets, software and methods, agent-based models, and visualization tools.  In an infectious disease epidemic emergency the Center will redirect its activities to serve the nation's security, as guided by the NIGMS. n/a",Computational Models of Infectious Disease Threats,7458835,U01GM070708,"['AIDS therapy', 'AIDS/HIV problem', 'Academy', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Airborne Particulate Matter', 'Algorithms', 'American', 'Americas', 'Animal Experimentation', 'Appendix', 'Archives', 'Area', 'Arthropod Vectors', 'Award', 'Bacteria', 'Beds', 'Bioinformatics', 'Biological', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Books', 'Borrelia', 'Climate', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Condition', 'Contact Tracing', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Decision Theory', 'Demography', 'Dengue', 'Dengue Hemorrhagic Fever', 'Detection', 'Development', 'Dialysis procedure', 'Disease', 'Docking', 'Doctor of Medicine', 'Doctor of Philosophy', 'Earthquakes', 'Ecology', 'Economics', 'Educational process of instructing', 'Ehrlichia', 'Emergency Situation', 'Emerging Communicable Diseases', 'Encephalitis', 'Engineering', 'Environmental Engineering technology', 'Environmental Health', 'Epidemic', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'Event', 'Evolution', 'Facility Construction Funding Category', 'Faculty', 'Foot-and-Mouth Disease', 'Frequencies', 'Game Theory', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Geography', 'Glass', 'Goals', 'HIV', 'Hantavirus', 'Head', 'Health', 'Health Policy', 'Healthcare', 'Hepatitis E', 'Human', 'Human Resources', 'Hygiene', 'Imagery', 'Immunity', 'Immunology', 'Incidence', 'Individual', 'Infectious Agent', 'Infectious Disease Epidemiology', 'Influenza', 'Informatics', 'Information Services', 'Institute of Medicine (U.S.)', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internal Medicine', 'International', 'Internet', 'Intervention', 'Joints', 'Journals', 'Knowledge', 'Laboratories', 'Laboratory Research', 'Laboratory Study', 'Lead', 'Legal patent', 'Leptospira', 'Libraries', 'Location', 'London', 'Lung', 'Machine Learning', 'Maintenance', 'Malaria', 'Maryland', 'Master&apos', 's Degree', 'Mathematical Biology', 'Mathematics', 'Measles', 'Measures', 'Mechanics', 'Methods', 'Microbiology', 'Military Personnel', 'Modeling', 'Modified Smallpox', 'Molecular', 'National Institute of General Medical Sciences', 'National Security', 'New York', 'Nonlinear Dynamics', 'Nonparametric Statistics', 'Observational Study', 'Oceanography', 'Outcome', 'Paper', 'Pattern', 'Physical Dialysis', 'Play', 'Policies', 'Policy Maker', 'Population', 'Positioning Attribute', 'Predisposition', 'Pregnancy Outcome', 'Prevention strategy', 'Preventive', 'Principal Investigator', 'Prion Diseases', 'Procedures', 'Process', 'Provider', 'Proxy', 'Public Health', 'Public Health Schools', 'Public Policy', 'Publications', 'Publishing', 'Purpose', 'Quarantine', 'Rate', 'Recording of previous events', 'Reference Standards', 'Relative (related person)', 'Research', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Rickettsia', 'Risk Assessment', 'Rodent', 'Role', 'Route', 'Schedule', 'Schools', 'Science', 'Scientist', 'Screening procedure', 'Security', 'Series', 'Severe Acute Respiratory Syndrome', 'Signal Transduction', 'Simulate', 'Smallpox', 'Social Network', 'Social Sciences', 'Societies', 'Software Tools', 'Space Flight', 'Statistical Computing', 'Statistical Models', 'Structure', 'Students', 'System', 'Systems Analysis', 'Testing', 'Theoretical model', 'Time', 'Time Series Analysis', 'Training', 'Tropical Medicine', 'U-Series Cooperative Agreements', 'Uncertainty', 'United States', 'United States National Academy of Sciences', 'United States National Aeronautics and Space Administration', 'Universities', 'Vaccination', 'Variant', 'Vector-transmitted infectious disease', 'Violence', 'Viral', 'Viral Hemorrhagic Fevers', 'Virus', 'Virus Diseases', 'Visual', 'Weather', 'West Nile virus', 'Work', 'base', 'biosecurity', 'c new', 'college', 'computer science', 'concept', 'design', 'disease natural history', 'disease transmission', 'disorder prevention', 'disorder risk', 'editorial', 'experience', 'improved', 'indexing', 'infectious disease model', 'insight', 'interest', 'mathematical model', 'member', 'microbial', 'models and simulation', 'network models', 'pathogen', 'peer', 'predictive modeling', 'prevent', 'professor', 'programs', 'remote sensing', 'respiratory', 'simulation', 'skills', 'social', 'social organization', 'sound', 'theories', 'tool', 'transmission process', 'user-friendly', 'vaccination strategy']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2008,503603,570146095,0.011193141900597737
"Metabolomic Assessment of Estrogenic Endocrine Disruptor    DESCRIPTION (provided by applicant)     Estrogenic endocrine disruptors (EEDs) are a group of structurally diverse compounds that include pharmaceuticals, dietary supplements, industrial chemicals and environmental contaminants.  They can elicit a number of adverse health effects such as hormone dependent cancers, reproductive tract abnormalities, compromised reproductive fitness, and impaired cognitive abilities.  In order to fully assess the potential adverse effects of synthetic and natural EEDs, a more comprehensive understanding of their molecular, metabolic, and tissue level effects is required within the context of a whole organism.  This collaborative proposal will elucidate the pathways, networks and signaling cascades perturbed by EEDs using the complementary multidisciplinary expertise of its team members in the areas of toxicology, molecular biology, endocrinology, multinuclear NMR spectroscopy, data management and advanced data analysis.  The comparative effects of ethynyl estradiol (EE), genistein (GEN), and o, p'-dichlorodiphenyltrichloroethane (DDT) on metabolite levels will be assessed in urine, serum and liver extracts by multinuclear (i. e., 1H, 13C, 31P) NMR spectroscopy, and complemented with histopathology examination and gene expression data from ongoing microarray studies in both mouse and rat models.  All data will be stored and archived in dbZach, a MIAME-compliant toxicogenomic supportive database that facilitates data analysis, the integration of disparate data sets, the exchange of data between investigators, and the deposition of data into public repositories.  Advanced statistical approaches, modeling and data integration tools such as neural networks, data fusion, and Baysean inference will be used to fuse these disparate data sets in order to elucidate the conserved biological networks that are of importance in response to endogenous estrogens.  Moreover, EED perturbed pathways associated with elicited effects will be further defined.  Results from these studies will not only further define the physiologic and toxic mechanisms of action of estrogenic compounds but will also demonstrate the synergy of fusing complementary microarray, metabolomic and histopathology data into a comprehensive integrative computational model.  This approach will also demonstrate the ability to maximize knowledge extraction from all disparate data available within the proposed innovative data management system when used with the advanced information tools that will be developed.            n/a",Metabolomic Assessment of Estrogenic Endocrine Disruptor,7440169,R01ES013927,"['Adverse effects', 'Affect', 'Apical', 'Archives', 'Area', 'Biochemical Pathway', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Cell Proliferation', 'Chemicals', 'Class', 'Classification', 'Clinical Chemistry', 'Cognitive', 'Complement', 'Computer Simulation', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Deposition', 'Development', 'Disease Progression', 'Dose', 'Endocrine Disruptors', 'Endocrinology', 'Engineering', 'Environmental Pollution', 'Estradiol', 'Estrogens', 'Funding', 'Future', 'Gene Expression', 'Genistein', 'Health', 'Hepatic', 'Histopathology', 'Hormones', 'Knowledge Extraction', 'Lead', 'Link', 'Lipids', 'Liver Extract', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Metabolic', 'Metabolism', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Profiling', 'Monitor', 'Multinuclear NMR', 'Mus', 'NMR Spectroscopy', 'Numbers', 'Organ Weight', 'Outcome', 'Pathway interactions', 'Pattern Recognition', 'Pharmacologic Substance', 'Physiological', 'Principal Investigator', 'Process', 'Rattus', 'Reporting', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Assessment', 'Rodent', 'Sampling', 'Screening procedure', 'Serum', 'Signal Transduction', 'Spectrum Analysis', 'System', 'Techniques', 'Time', 'Tissues', 'Toxic effect', 'Toxicogenomics', 'Toxicology', 'Urine', 'Whole Organism', 'aqueous', 'comparative', 'data integration', 'data management', 'dichlorodiphenyltrichloroethane', 'dietary supplements', 'estrogenic endocrine disruptor', 'experience', 'fitness', 'innovation', 'member', 'metabolic abnormality assessment', 'metabolomics', 'multidisciplinary', 'programs', 'repository', 'reproductive', 'research study', 'response', 'tool']",NIEHS,MICHIGAN STATE UNIVERSITY,R01,2008,535031,89938253,-0.012491575304870365
"Proteomic Phosphopeptide Chip Technology for Protein Profiling    DESCRIPTION (provided by applicant):   This R21/R33 grant application is in response to NIH/NCI RFA-CA-07-005 for ""Advanced Proteomic Platforms and Computational Sciences for the NCI Clinical Proteomic Technologies Initiative"". Cancers are malignant growths caused by misregulated and uncontrolled cell division; these abnormal cellular activities are typically accompanied by unusual protein expression profiles1. Existing proteomics technologies cannot meet the demand of reliable, sensitive, accurate determinations of these disease-related molecular profiles; to achieve reproducible quality, high throughput, and affordable proteomic analyses, many challenging technological hurdles must first be addressed. We propose developing a proteomic phosphopeptide (PPEP) microchip technology platform that profiles proteins carrying phosphopeptide binding domains (PPBDs); using profiles generated in these experiments, along with predicative computational modeling based on both experimental data and a comprehensive PPEP and PPBD interaction database, we will demonstrate specific and quantitative measurements related to protein functions for proteins of significant biological importance. The strength of our proposed work lies in the integration of an already established array technology and a highly promising bioinformatics platform. The methods developed will enable many researchers to rapidly and vigorously develop peptide arrays for quantitative measurement of the proteins in the biological systems of their own interest or to use standard domain-optimized peptide arrays to systematically profile biological samples of basic research or clinical importance. Over the long term, the methods we develop will be used to establish domain-recognition systems for other types of domain-carrying proteins measurements. These capabilities of reliable measurement of proteins as a function of disease states are essential for cancer research, diagnosis and treatment.             n/a",Proteomic Phosphopeptide Chip Technology for Protein Profiling,7622959,R33CA126209,"['Address', 'Antibodies', 'Applications Grants', 'Basic Science', 'Binding', 'Binding Proteins', 'Biochemical Reaction', 'Bioinformatics', 'Biological', 'Cell division', 'Clinical', 'Computational Science', 'Computer Simulation', 'Computer software', 'DNA', 'Data', 'Data Analyses', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease regression', 'Ensure', 'Epitopes', 'Experimental Designs', 'Facility Construction Funding Category', 'Growth', 'Information Resources', 'Link', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Methods', 'Microfluidic Microchips', 'Microfluidics', 'Modeling', 'Molecular Profiling', 'Numbers', 'PTPN11 gene', 'Peptide Mapping', 'Peptide Signal Sequences', 'Peptides', 'Phase', 'Phosphopeptides', 'Phosphorylation Site', 'Phosphotransferases', 'Process', 'Protein Binding', 'Protein Kinase', 'Proteins', 'Proteomics', 'Publishing', 'Research Personnel', 'Resource Informatics', 'Sampling', 'Signal Transduction', 'Software Tools', 'Staging', 'Standards of Weights and Measures', 'System', 'Technology', 'Testing', 'Time', 'Work', 'Writing', 'anticancer research', 'base', 'database design', 'density', 'design', 'interest', 'microchip', 'novel', 'programs', 'protein aminoacid sequence', 'protein expression', 'protein function', 'research study', 'response', 'software development', 'src Homology Region 2 Domain', 'technology development', 'tool', 'web-accessible']",NCI,UNIVERSITY OF HOUSTON,R33,2008,536748,31980265,0.02201046504312122
"Robust Classification Methods for Categorical Regression    DESCRIPTION (provided by applicant): Improving statistical methods to provide better classification performance and new analytical capabilities for categorical regression would be invaluable to the medical and health care research communities. Categorical regression models (e.g., binary logistic, multinomial logistic) are used extensively to identify patterns of alcohol-related symptoms, screen for disorders, and assess policies. In addition, such models are used extensively in other areas of research such as mental illness, cancer, traumatic injuries, and AIDS-related pathologies. However, many such models are developed with inadequate support to fully analyze and exploit the intrinsically probabilistic nature of their results. This is of critical importance as health researchers, clinicians, and administrators are often faced with classification decisions using categorical regression models to identify unacceptable risks, adequate outcomes, and acceptable guidelines for screening, diagnoses, treatment, and quality of care. Commercially available statistical software does not offer sophisticated methods for robust estimation of posterior probabilities in the presence of model misspecification, missing covariates, and nonignorable missing data generating processes. Such robust missing data handling methods provide natural mechanisms for dealing with verification bias and modeling correlated, longitudinal, or survey data with complex sampling designs. Moreover, commercially available statistical software does not provide automated methods for using estimated posterior probabilities to make optimal classification decisions with respect to different optimality criteria. In particular, automated features such as optimizing multiple decision criteria (allocation rules) that trade off specificity against sensitivity, decision threshold confidence intervals, statistical tests for evaluating correct specification of posterior probabilities, statistical tests for comparing competing classifier thresholds, and methods for multi-outcome classification and inference are not readily available. Phase II research will extend Phase I findings for binary logistic regression to develop and implement automated robust classification methods for multinomial logistic regression modeling, which also applies to the larger class of nonlinear categorical regression models that output posterior probabilities. The Phase II software prototype will provide: 1) new user-selectable robust decision threshold estimators, 2) robust confidence intervals on decision threshold estimators, 3) new classifier threshold comparison tests, 4) new outcome probability specification tests, 5) efficient missing data handling methods in the presence of nonignorable nonresponse data, and 6) second-order analytic and simulation-based Bayesian methods for improved small sample and rare event outcome probability estimation. These new methodologies will be integrated into a prototype user-friendly software package, evaluated with extensive simulation studies, and then applied to real world classification problems encountered in: alcohol, mental illness (depression, bipolar, schizophrenia), cancer (prostate), trauma (emergency room), and infectious disease (AIDS) through collaborations with domain experts in those respective fields. In summary, Phase II research will establish the essential technical foundation for Phase III commercialization with the objective of providing a suite of new classification analysis methods as an advanced statistical tool that improves epidemiologic, clinical, and public health research.                 n/a",Robust Classification Methods for Categorical Regression,7395177,R44CA139607,"['Accident and Emergency department', 'Achievement', 'Address', 'Administrator', 'Agreement', 'Alcohols', 'Algorithms', 'Area', 'Bayesian Method', 'Behavior', 'Bipolar Depression', 'Class', 'Classification', 'Clinical', 'Clinical Investigator', 'Collaborations', 'Communicable Diseases', 'Communities', 'Complex', 'Computer software', 'Confidence Intervals', 'Data', 'Data Analyses', 'Data Set', 'Decision Analysis', 'Decision Making', 'Development', 'Diagnosis', 'Disease', 'Disease regression', 'Empirical Research', 'Engineering', 'Epidemiologic Studies', 'Epidemiologist', 'Evaluation', 'Event', 'Foundations', 'Goals', 'Guidelines', 'Health', 'Health Services Research', 'Healthcare', 'Industry', 'Information Resources Management', 'Injury', 'Jordan', 'Journals', 'Knowledge', 'Literature', 'Logistic Regressions', 'Logistics', 'Malignant Neoplasms', 'Malignant neoplasm of prostate', 'Medical', 'Mental Health', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Outcome', 'Output', 'Pathology', 'Pattern', 'Peer Review', 'Performance', 'Phase', 'Phase I Clinical Trials', 'Phase II Clinical Trials', 'Policies', 'Preparation', 'Probability', 'Process', 'Public Health', 'Publishing', 'Quality of Care', 'Relative (related person)', 'Research', 'Research Personnel', 'Risk', 'Robin bird', 'Sampling', 'Schizophrenia', 'Screening procedure', 'Sensitivity and Specificity', 'Simulate', 'Software Tools', 'Specific qualifier value', 'Specificity', 'Standards of Weights and Measures', 'Statistical Methods', 'Surveys', 'Symptoms', 'Technology', 'Testing', 'Trauma', 'anticancer research', 'base', 'commercialization', 'computerized data processing', 'density', 'design', 'graphical user interface', 'improved', 'innovation', 'prototype', 'simulation', 'software development', 'theories', 'tool', 'user friendly software']",NCI,MARTINGALE RESEARCH CORPORATION,R44,2008,857168,0,0.006766502232110227
"Developing computerized tools for cryosurgery planning    DESCRIPTION (provided by applicant):    Cryosurgery has been known as an invasive surgical technique since 1961, when Cooper and Lee invented the first cryoprobe. In the 1990s, new developments in Joule-Thomson cooling (the cooling effect associated with a sudden relief of a pressurized gas) led to a dramatic decrease in the size of cryoprobes and an increase in the number of cryoprobes that could be used simultaneously. A dozen or more cryoprobes operating simultaneously in a single prostate cryosurgery is already common practice. If localized effectively, one of the primary benefits of using a large number of miniaturized cryoprobes is superior control over the freezing process.   Currently, the process of selecting the correct placement of the cryoprobes for a specific procedure is an art held by the cryosurgeon, based on the surgeon's own experience and rules of thumb. Cryoprobes are typically operated in a trial-and-error fashion, until the entire target volume is thought to be frozen. Currently, there are no means to determine the optimal locations for the cryoprobes. Suboptimal cryoprobe localization may leave regions in the target volume unfrozen, may lead to cryoinjury of healthy surrounding tissues, may require an unnecessarily large number of cryoprobes, may increase the duration of the surgical procedure, and may increase the likelihood of post cryosurgery complications, all of which affect the quality and cost of the medical treatment. Computerized planning tools would help to alleviate these difficulties.   The ""cryoheater,"" a new device for cryosurgery control has recently been presented by the research team. The cryoheater is a temperature controlled electrical heater. In broad terms, cryoheaters can dramatically increase the ability to control the shape and size of the frozen region, however, to achieve the full benefits of cryoheaters, computerized planning tools for cryoheater localization are necessary.   Our goal is to develop computerized planning tools for cryosurgery that are suitable for all available cooling techniques. The proposed research includes: (1) Development of an efficient numerical scheme for bioheat transfer simulations of cyroprocedures, (2) Development of an efficient optimization technique based on a force-field analogy. (3) Development of knowledge-based optimization techniques. (4) Experimental verification of the planning tool.       Besides planning, another important application of the proposed tool is the training of cryosurgeons. The proposed tool will provide cryosurgeons with the ability to visualize the 3D volumetric nature of the freezing process.   Likewise, it will allow the surgeon to explore the performance of various configurations of cryoprobes and cryoheaters, and observe the defects that would result from each. Such visualization capabilities will provide surgeons with insights into the physics of cryosurgery that are difficult to obtain from physical experiments or surgical practice.         n/a",Developing computerized tools for cryosurgery planning,7210691,R01EB003563,"['Affect', 'Arts', 'Biological', 'Catheters', 'Computational Technique', 'Condition', 'Cool-X-A', 'Cryosurgery', 'Defect', 'Depth', 'Development', 'Devices', 'Europe', 'Feasibility Studies', 'Freezing', 'Frequencies', 'Furuncles', 'Gases', 'Goals', 'Heating', 'Imagery', 'Imaging Device', 'Invasive', 'Lasers', 'Lead', 'Learning', 'Left', 'Liquid substance', 'Localized', 'Location', 'Machine Learning', 'Medical', 'Methods', 'Modems', 'Nature', 'Nitrogen', 'Numbers', 'Operating Rooms', 'Operative Surgical Procedures', 'Patients', 'Performance', 'Physics', 'Placement', 'Procedures', 'Process', 'Prostate', 'Publishing', 'Purpose', 'Radio', 'Reporting', 'Research', 'Research Proposals', 'Scheme', 'Shapes', 'Simulate', 'Solutions', 'Source', 'Surgeon', 'Techniques', 'Temperature', 'Thermal Ablation Therapy', 'Thinking', 'Thumb structure', 'Time', 'Tissues', 'Training', 'Ultrasonography', 'Urethra', 'base', 'clinical application', 'computerized', 'computerized tools', 'cost', 'experience', 'insight', 'knowledge base', 'miniaturize', 'research study', 'simulation', 'size', 'thermal seeds', 'three-dimensional modeling', 'tool']",NIBIB,CARNEGIE-MELLON UNIVERSITY,R01,2007,87443,30434536,-0.013921089279631768
"Least Angle Regression    DESCRIPTION (provided by applicant): This SBIR project aims to produce superior methods and software for classification and regression when there are many potential predictor variables to choose from. The methods should (1) produce stable results, where small changes in the data do not produce major changes in the variables selected or in model predictions; (2) produce accurate predictions; (3) facilitate scientific interpretation, by selecting a smaller subset of predictors which provide the best predictions; (4) allow continuous and categorical variables; and (5) support linear regression, logistic regression (predicting a binary outcome), survival analysis, and other types of regression. This project is based on least angle regression, which unifies and provides a fast implementation for a number of modern regression techniques. Least angle regression has great potential, but currently available software is limited in scope and robustness. The outcome of this project should be software which is more robust and widely applicable. This software would apply broadly, including to medical diagnosis, detecting cancer, feature selection in microarrays, and modeling patient characteristics like blood pressure. Phase I work demonstrates feasibility by extending least angle work in three key directions-categorical predictors, logistic regression, and a numerically-accurate implementation. Phase II goals include extensions to other types of explanatory variables (e.g. polynomial or spline functions, and interactions between variables), to survival and other additional regression models, and to handle missing data and massive data sets. This proposed software will enable medical researchers to obtain high prediction accuracy, and obtain stable and interpretable results, in high-dimensional situations. Predicting outcomes based on covariates, determining which covariates most affect outcomes, and adjusting treatment effects estimates for covariates, are among the most important problems in biostatistics. Prediction and feature selection are particularly difficult when there are more possible features than samples; gene microarrays and protein mass spectrometry are extreme examples of this, producing thousands to millions of measurements per sample. LARS excels at feature selection; the proposed software should enable medical researchers to obtain stable and interpretable models with better prediction accuracy in high-dimensional situations.             n/a",Least Angle Regression,7293630,R44GM074313,"['Affect', 'Algorithms', 'Biometry', 'Blood Pressure', 'Case Study', 'Cations', 'Characteristics', 'Classification', 'Computer software', 'Consult', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Disease regression', 'Future', 'Genes', 'Goals', 'Healthcare', 'Lasso', 'Libraries', 'Linear Models', 'Logistic Regressions', 'Malignant Neoplasms', 'Manuals', 'Mass Spectrum Analysis', 'Measurement', 'Medical', 'Methods', 'Microarray Analysis', 'Mind', 'Modeling', 'Non-linear Models', 'Numbers', 'Outcome', 'Output', 'Patients', 'Personal Satisfaction', 'Phase', 'Procedures', 'Process', 'Protein Microchips', 'Proteome', 'Proteomics', 'Research Personnel', 'Sampling', 'Small Business Funding Mechanisms', 'Small Business Innovation Research Grant', 'Survival Analysis', 'Techniques', 'Technology', 'Testing', 'Training', 'Validation', 'Work', 'base', 'design', 'falls', 'graphical user interface', 'improved', 'interest', 'prototype', 'statistics', 'tool', 'treatment effect', 'ward']",NIGMS,INSIGHTFUL CORPORATION,R44,2007,158481,0,0.02153033577933077
"Nanopore study of single antibody-antigen interactions DESCRIPTION:    Nanopore-based single-molecule detection has recently become established as a new tool in single molecule biophysics. Evidence is presented that single antibodies can be observed with a nanopore detector, which presents a wide range of possibilities for immunological research. The hypothesis to be tested is that nanopore-based detection can be used to study single molecule dynamics of antibody-antigen interaction and analyze conformational changes that occur in antibody upon binding to antigen. This application aims to develop the utility of the nanopore-based approach through improvements in both the detection device and the software used to extract information from the channel current signal. At the same time, these studies will allow the Candidate to gain expertise in immunology and the biophysical study of protein structure and function. To study the single molecule dynamics of antibody-antigen interaction, the following three specific aims are proposed:   1. Extend nanopore based detection to nanopore/antibody based detection.   2. Implement machine learning software for automated nanopore/antibody signal analysis and experimental feedback.   3. Use well-characterized, genetically engineered, antibodies to test the utility of the nanopore device to analyze motion in the antibody molecule.   These studies will expand the utility of nanopore devices to study single molecule protein interactions.   Information gained will lead to a better understanding of the molecular dynamics associated with antigen binding by antibody and the subsequent initiation of effector functions. Since most biological nanopore variants derive from pore-forming toxins, nanopore device enhancements eventually may lead to new methods for antibody and antimicrobial-peptide immunological screening. Antibody-based nanopore devices may also serve as highly sensitive immunosensors. n/a",Nanopore study of single antibody-antigen interactions,7286095,K22LM008794,"['Algorithms', 'Altretamine', 'Antibodies', 'Antigenic Specificity', 'Antigens', 'Architecture', 'Base Pairing', 'Binding', 'Biological', 'Biophysics', 'Biosensing Techniques', 'Buffers', 'Class', 'Classification', 'Computer software', 'Condition', 'DNA', 'Data', 'Detection', 'Development', 'Devices', 'Dissociation', 'Failure', 'Feedback', 'Fingerprint', 'Haptens', 'Hemolysin', 'Immunology', 'Informatics', 'Kinetics', 'Knowledge', 'Lead', 'Length', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Motion', 'Noise', 'Pattern Recognition', 'Physiological', 'Play', 'Pliability', 'Preparation', 'Principal Investigator', 'Probability', 'Process', 'Proteins', 'Protocols documentation', 'Range', 'Rate', 'Research', 'Research Personnel', 'Resistance', 'Resolution', 'Role', 'Sampling', 'Screening procedure', 'Semiconductors', 'Signal Transduction', 'Staging', 'Statistical Methods', 'Testing', 'Time', 'Time Study', 'Toxin', 'Variant', 'antibody engineering', 'antigen antibody binding', 'antigen binding', 'antimicrobial peptide', 'base', 'cheminformatics', 'computerized data processing', 'design', 'detector', 'expectation', 'experimental analysis', 'markov model', 'molecular dynamics', 'nanopore', 'programs', 'protein structure function', 'prototype', 'single molecule', 'stem', 'tool', 'vector', 'web interface']",NLM,CHILDREN'S HOSPITAL (NEW ORLEANS),K22,2007,162000,157330,0.01616482147748771
"Least Angle Regression    DESCRIPTION (provided by applicant): This SBIR project aims to produce superior methods and software for classification and regression when there are many potential predictor variables to choose from. The methods should (1) produce stable results, where small changes in the data do not produce major changes in the variables selected or in model predictions; (2) produce accurate predictions; (3) facilitate scientific interpretation, by selecting a smaller subset of predictors which provide the best predictions; (4) allow continuous and categorical variables; and (5) support linear regression, logistic regression (predicting a binary outcome), survival analysis, and other types of regression. This project is based on least angle regression, which unifies and provides a fast implementation for a number of modern regression techniques. Least angle regression has great potential, but currently available software is limited in scope and robustness. The outcome of this project should be software which is more robust and widely applicable. This software would apply broadly, including to medical diagnosis, detecting cancer, feature selection in microarrays, and modeling patient characteristics like blood pressure. Phase I work demonstrates feasibility by extending least angle work in three key directions-categorical predictors, logistic regression, and a numerically-accurate implementation. Phase II goals include extensions to other types of explanatory variables (e.g. polynomial or spline functions, and interactions between variables), to survival and other additional regression models, and to handle missing data and massive data sets. This proposed software will enable medical researchers to obtain high prediction accuracy, and obtain stable and interpretable results, in high-dimensional situations. Predicting outcomes based on covariates, determining which covariates most affect outcomes, and adjusting treatment effects estimates for covariates, are among the most important problems in biostatistics. Prediction and feature selection are particularly difficult when there are more possible features than samples; gene microarrays and protein mass spectrometry are extreme examples of this, producing thousands to millions of measurements per sample. LARS excels at feature selection; the proposed software should enable medical researchers to obtain stable and interpretable models with better prediction accuracy in high-dimensional situations.             n/a",Least Angle Regression,7748342,R44GM074313,"['Affect', 'Algorithms', 'Biometry', 'Blood Pressure', 'Case Study', 'Characteristics', 'Classification', 'Computer software', 'Consult', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Disease regression', 'Future', 'Genes', 'Goals', 'Healthcare', 'Lasso', 'Libraries', 'Linear Models', 'Logistic Regressions', 'Malignant Neoplasms', 'Manuals', 'Mass Spectrum Analysis', 'Measurement', 'Medical', 'Methods', 'Microarray Analysis', 'Mind', 'Modeling', 'Non-linear Models', 'Numbers', 'Outcome', 'Output', 'Patients', 'Personal Satisfaction', 'Phase', 'Procedures', 'Process', 'Protein Microchips', 'Proteome', 'Proteomics', 'Research Personnel', 'Sampling', 'Small Business Funding Mechanisms', 'Small Business Innovation Research Grant', 'Survival Analysis', 'Techniques', 'Technology', 'Testing', 'Training', 'Validation', 'Work', 'base', 'design', 'falls', 'graphical user interface', 'improved', 'interest', 'prototype', 'statistics', 'tool', 'treatment effect', 'ward']",NIGMS,INSILICOS,R44,2007,168203,0,0.02153033577933077
"LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction    DESCRIPTION (provided by applicant): The high cost ($0.8 - $1.7 billion) and long time frames (about 13 years) required to introduce new drugs to the market contributes substantially to spiraling health care costs and diseases persisting without effective cures. A major factor is the high attrition rate of new compounds failing due to toxicity identified years into clinical trials. This particular circumstance cost the pharmaceutical industry approximately $8 billion in 2003. In silico tools generally offer the promise of identifying toxicity issues much more rapidly than clinical methods, however, they are not sufficiently accurate for pharmaceutical companies to confidently make definitive early screening and related investment decisions. LiverTox is a highly advanced, self-learning liver toxicity prediction tool that represents a quantum leap over current in silico methods. It offers a highly innovative use of multiple analytical approaches to accurately predict the toxicity of candidate Pharmaceuticals in the liver. A differentiating capability is its self-learning computational neural networks (CNNs) and wavelets. They rapidly assimilate massive volumes of information from LiverTox's extensive, dynamic, and thoroughly reviewed databases. Initially, LiverTox will generate predictions derived from five independent CNN-based submodules; one trained in advanced computational chemistry methods to make quantitative structure activity relationship (QSAR) analyses; a second trained with microarray data; a third trained with Massively Parallel Signature Sequencing and Gene Expression (MPSS/GE) data; and fourth and fifth submodules trained with proteomics and metabolomics/metabonomics data, respectively. Challenging LiverTox with new chemical formulations triggers the five independent submodules to each make toxicity endpoint predictions drawing upon its knowledge base and its similarity analysis/fuzzy logic/statistical training. This tool's flexible, highly advanced system architecture and advanced learning capabilities using data obtained from diverse techniques enable it to rapidly digest new data, build upon new data acquisition techniques, and use prior lessons learned to achieve overall toxicity predictions with greater than 95% accuracy. LiverTox's ability to rapidly and accurately predict the toxicity of drug candidates will allow pharmaceutical companies to move from discovery to curing disease faster, at greatly reduced cost, and with less reliance on animal-based tests.         n/a",LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction,7214118,R42ES013321,"['Accounting', 'Animals', 'Architecture', 'Biological Assay', 'Biological Neural Networks', 'Chemicals', 'Clinical', 'Clinical Trials', 'Computer Simulation', 'Computer software', 'Contracts', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Disease', 'Drug Formulations', 'Drug Industry', 'Drug toxicity', 'End Point', 'Expert Systems', 'Funding', 'Future', 'Fuzzy Logic', 'Gene Expression', 'Guidelines', 'Health Care Costs', 'Hepatotoxicity', 'Investments', 'Learning', 'Liver', 'Marketing', 'Methods', 'Network-based', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Process', 'Proteomics', 'Protocols documentation', 'Quantitative Structure-Activity Relationship', 'Rate', 'Relative (related person)', 'Reliance', 'Research', 'Research Personnel', 'Screening procedure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Toxic effect', 'Toxicogenomics', 'Toxicology', 'Training', 'Validation', 'base', 'computational chemistry', 'cost', 'data acquisition', 'design', 'highly advanced system', 'improved', 'innovation', 'knowledge base', 'metabolomics', 'quantum', 'serial analysis of gene expression', 'subtraction hybridization', 'tool']",NIEHS,"YAHSGS, LLC",R42,2007,257269,0,-0.012354154738543182
MACE - Michigan Alliance for Cheminformatic Exploration No abstract available n/a,MACE - Michigan Alliance for Cheminformatic Exploration,7472717,P20HG003890,"['Address', 'Algorithms', 'Applications Grants', 'Area', 'Belief', 'Bioinformatics', 'Biotechnology', 'Categories', 'Cell Nucleus', 'Chemicals', 'Chemistry', 'Class', 'Classification', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Making', 'Descriptor', 'Development', 'Educational workshop', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Generations', 'Generic Drugs', 'Grant', 'Hybrids', 'Industry', 'Institution', 'Interdisciplinary Study', 'Knowledge', 'Laboratories', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Michigan', 'Mining', 'Modeling', 'Molecular', 'Molecular Bank', 'Molecular Medicine', 'Molecular Structure', 'Online Systems', 'Pattern Recognition', 'Pharmacotherapy', 'Pilot Projects', 'Preparation', 'Process', 'Property', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Screening procedure', 'Seeds', 'Site', 'Specialist', 'Structure', 'Students', 'Sum', 'TNFRSF5 gene', 'Techniques', 'Testing', 'Travel', 'Validation', 'Vision', 'Work', 'base', 'chemical property', 'cheminformatics', 'computer center', 'computer science', 'concept', 'data mining', 'design', 'in vitro Assay', 'interdisciplinary approach', 'knowledge base', 'model development', 'organizational structure', 'predictive modeling', 'symposium', 'tool', 'training project']",NHGRI,UNIVERSITY OF MICHIGAN,P20,2007,271370,0,0.019846374239813516
"Protein Packing Defects as Functional Markers and Drug Targets DESCRIPTION (provided by applicant):  Our preliminary structure-based investigations show that water exclusion from deficiently packed hydrogen bonds and other pre-formed electrostatic interactions constitutes a driving factor conferring high specificity to protein association. Thus, an evolutionary conserved feature, the under-dehydrated hydrogen bond, termed dehydron, appears to be a structural marker for interactivity. Dehydrons were experimentally and statistically shown to constitute sticky spots on the protein surface and to be abundant at protein-protein interfaces, especially at those that cannot be understood in terms of standard interactions. The dehydron distribution on the surface of soluble proteins constitutes a determinant of the propensity for association and aberrant aggregation. The identification of dehydrons has relied so far on detailed structural information, a limitation precluding a proteomic analysis. This proposal is geared at introducing a sequence-based predictive method to establish the biological relevance of dehydrons and their potential as markers for drugable targets. Thus, we intend to introduce a powerful unsupervised scanning technology to detect signals of interactivity and drugability at a genomic scale. This goal requires constructing a machine-learning discriminator trained on a structural database. The over-all aim is to develop a sequence-based multi-purpose tool to expand the universe of drugable targets, diagnose propensity for aberrant aggregation and make interactomic inferences. The efficacy of our predictor will be tested on five grounds: a) Assaying for amino-acid variability and determining whether residues predicted solely from sequence to be engaged in dehydrons are actually conserved, b) Using a redundancy-free curated PDB sample as training set, we shall determine the accuracy and precision of the sequence-based predictor using a nonhomologous PDB complement set and annotated SwissProt entries as testing sets, c) Contrasting our results with an alternative dehydron predictor based on a reliable sequence-based predictor of native disorder (PONDR(r)). This dehydron predictor is based on a correlation found between the extent of hydrogen-bond packing and the score of structural disorder, d) Contrasting sequence-based diagnosis of amyloidogenic aggregation with SwissProt annotations and other annotated disease-related sequence repositories; e) Contrasting compiled drug-target quality assessments and structural data and screening profiles for protein-ligand associations with the predicted dehydron patterns. Thus, the novel design concept of ""drug inhibitor as a wrapper of functional packing defects"" will be explored and validated. n/a",Protein Packing Defects as Functional Markers and Drug Targets,7282361,R01GM072614,"['Address', 'Adopted', 'Amino Acids', 'Area', 'Automobile Driving', 'Base Sequence', 'Binding', 'Binding Sites', 'Biological', 'Biological Assay', 'Biology', 'Chemicals', 'Code', 'Collaborations', 'Complement', 'Complex', 'Condition', 'Data', 'Databases', 'Defect', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug Delivery Systems', 'Electrostatics', 'Evolution', 'Excision', 'Exclusion', 'Exhibits', 'Genome', 'Genomics', 'Goals', 'Gray unit of radiation dose', 'Homologous Gene', 'Hydrogen Bonding', 'Individual', 'Investigation', 'Libraries', 'Ligands', 'Light', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Membrane Proteins', 'Methods', 'Molecular', 'Nature', 'Numbers', 'Paper', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Physiological', 'Positioning Attribute', 'Principal Investigator', 'Proteins', 'Proteomics', 'Public Domains', 'Purpose', 'Qualifying', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Sampling', 'Scanning', 'Score', 'Screening procedure', 'Sequence Alignment', 'Signal Transduction', 'Site', 'Specificity', 'Spottings', 'Standards of Weights and Measures', 'Statistical Study', 'Structure', 'Surface', 'SwissProt', 'Technology', 'Testing', 'Training', 'Validation', 'Variant', 'Vertebral column', 'Water', 'base', 'computerized tools', 'concept', 'design', 'drug discovery', 'improved', 'inhibitor/antagonist', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'repository', 'size', 'stem', 'success', 'tool']",NIGMS,RICE UNIVERSITY,R01,2007,284461,19199034,0.0038660154237451262
"Building motif lexicons    DESCRIPTION (provided by applicant):  With the complete sequencing of numerous genomes and the annotation of proteomes, one of the next major challenges in biology is to understand the functions and integration of the encoded proteins (a NIH Roadmap area of emphasis). Deciphering protein function is a very time consuming, expensive process, as reflected by the disproportionately low percentage of proteins with well-established functions. One approach for extrapolating established functions to new proteins is to predict short motifs. Short motifs target proteins for post-translational modification, trafficking to cellular compartments, and binding to other proteins or molecules. Our cross-disciplinary team has built Minimotif Miner (MnM), a short motif database and platform- independent web-tool that identifies motif consensus sequences in protein queries and thus potential new protein functions (http://mnm.engr.uconn.edu/). MnM can also be used to develop new hypotheses of how specific mutations cause human disease and to identify putative targets for the development of therapeutic drugs, antibiotics, insecticides, and antiviral agents. Despite the utility of MnM and other motif resources, prediction of functional motifs still has two major limitations, which we address in this proposal. 1) To reduce the false-positive prediction of motifs, we have created a new language that allows us to consider the 3- dimensional structural conservation of motifs. For each motif, we will build specific motif definitions by combining experimental data with data from motif structures in the Protein Data Bank. We will also determine the sequence permutations that can form the observed motif structure by using molecular dynamic simulations. 2) To build a more comprehensive motif database we will use artificial intelligence to mine PubMed. The expert system will use automated literature screening, document summarization, and motif identification efficiency score to extract the majority of known motifs from PubMed. Addressing these limitations will vastly increase the utility of short motif prediction.              n/a",Building motif lexicons,7268219,R01GM079689,"['3-Dimensional', 'Adaptor Signaling Protein', 'Address', 'Amino Acids', 'Antibiotics', 'Antiviral Agents', 'Area', 'Artificial Intelligence', 'Binding', 'Biochemical', 'Biology', 'Code', 'Consensus', 'Consensus Sequence', 'Data', 'Databases', 'Development', 'Expert Systems', 'Genome', 'Growth Factor', 'Human', 'Insecticides', 'Internet', 'Language', 'Letters', 'Literature', 'Measures', 'Mining', 'Molecular Conformation', 'Mutation', 'Nomenclature', 'Numbers', 'Paper', 'Performance', 'Post-Translational Protein Processing', 'Process', 'Protein Binding', 'Proteins', 'Proteome', 'PubMed', 'Publishing', 'Reading', 'Research Personnel', 'Resources', 'Score', 'Screening procedure', 'Series', 'Signal Transduction', 'Specificity', 'Structure', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'base', 'data mining', 'human disease', 'indexing', 'molecular dynamics', 'platform-independent', 'programs', 'protein function', 'protein structure', 'src Homology Region 2 Domain', 'three dimensional structure', 'tool', 'trafficking']",NIGMS,UNIVERSITY OF CONNECTICUT SCH OF MED/DNT,R01,2007,288025,65553150,0.03282286667460869
"Improve predictions of structure and function by PredictProtein    DESCRIPTION (provided by applicant):       SUMMARY: Over 25,000 researchers in the US and over 50,000 in 120 other countries have exploited the PredictProtein (PP) Internet server to analyze proteins by homology-transfer and by eye novo predictions of protein structure and function. Here, we propose technical and scientific solutions that will improve the functionality of PP and its extension portal META-PP. Many technical changes will remain hidden to users and are required to increase the maintainability, scalability, and portability of these servers. New Graphical User Interfaces are one proposed solution that will visibly impact the service. The scientific solutions address two related tasks pertaining to the prediction of structure and function. The first is to predict the effect of mutations. We propose the development of novel machine learning-based methods to distinguish between mutations that affect structure, function, or have no apparent phenotype. Our final method will be applied to the screening of SNP data from our experimental colleagues at Columbia, as well as to the prediction of SNP effects in public databases. The second major task is the identification of natively unstructured regions and their functional classification. Proteins that do not adopt regular structures in isolation are increasingly becoming an important research area; they may provide a key to the evolution of complexity from prokaryotes to eukaryotes. We propose the development of a machine learning-based identification of features specific to this important class of molecules. We also plan to attack the problem from a very different angle by using predictions of interaction densities inside proteins. The resulting novel tools will allow a proteome-wide analysis of the role of these molecules. All methods will be made available through PP.      RELEVANCE: Information about protein structure adds an entire dimension to protein analysis and genome annotation. This addition is often essential to infer function even for natively unstructured proteins. The PredictProtein server is unique in its combination and exploitation of evolution, structure, and function; many thousands of theoretical, experimental, and clinical researches have benefited from this. The long-term goal of the research proposed here is to improve our ability to use the evolutionary record of amino acid substitutions, i.e. to ultimately understand the amino acid ""language"". The short-term goal is to address two tasks that are closely related to human diseases, namely the distinction between silent and important mutations and the mapping of unstructured proteins onto networks and diseases.          n/a",Improve predictions of structure and function by PredictProtein,7207482,R01LM007329,"['Address', 'Adopted', 'Affect', 'Amino Acid Substitution', 'Amino Acids', 'Area', 'Biology', 'Budgets', 'Class', 'Classification', 'Clinical Research', 'Code', 'Communities', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Devices', 'Dimensions', 'Disease', 'Eukaryota', 'Eukaryotic Cell', 'Evolution', 'Eye', 'Funding', 'Genome', 'Genomics', 'Goals', 'Grant', 'Induced Mutation', 'Internet', 'Investigation', 'Language', 'Linux', 'Machine Learning', 'Maintenance', 'Maps', 'Medical', 'Membrane', 'Methods', 'Mutation', 'Nucleotides', 'Numbers', 'Output', 'Phenotype', 'Plug-in', 'Point Mutation', 'Progress Reports', 'Prokaryotic Cells', 'Property', 'Protein Analysis', 'Proteins', 'Proteome', 'Range', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Screening procedure', 'Sequence Analysis', 'Services', 'Solutions', 'Standards of Weights and Measures', 'Structure', 'Testing', 'Training', 'base', 'data modeling', 'density', 'design', 'graphical user interface', 'human disease', 'improved', 'novel', 'portability', 'protein function', 'protein protein interaction', 'protein structure', 'protein structure function', 'protein structure prediction', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2007,323192,558628098,0.028968897504906423
"Robust computational framework for predictive ADME-Tox modeling    DESCRIPTION (provided by applicant):    This proposal seeks to establish a universally applicable and robust predictive ADME-Tox modeling framework based on rigorous Quantitative Structure Activity/Property Relationships (QSAR/QSPR) modeling. The framework has been refined in the course of many years of our research in the areas of QSPR methodology development and application to experimental datasets that led to novel analytical approaches, descriptors, model validation schemes, overall QSPR workflow design, and multiple end-point studies. This proposal focuses on the design of optimized QSPR protocols for the development of reliable predictors of critically important ADME-Tox properties. The ADME properties will include, but not limited to, water solubility, membrane permeability, P450 metabolism inhibition and induction, metabolic stability, human intestinal absorption, bioavailability, transporters and PK data; a variety of toxicological end-points vital to human health will be explored; they are available from recent initiatives on development and standardization of toxicity data, such as the US FDA, NIEHS, and EPA DSS-Tox and other database projects. The ultimate goal of this project is sharing both modeling software and specialized predictors with the research community via a web-based Predictive ADME-Tox Portal. The project objectives will be achieved via concurrent development of QSPR methodology (Specific Aim 1), building highly predictive, robust QSPR models of known ADME-Tox properties (Specific Aim 2), and the deployment of both modeling software and individual predictors via a specialized web-portal (Specific Aim 3). To achieve the goals of this project focusing on the development and delivery of specialized tools and rigorous predictors, we have assembled a research team of mostly senior investigators with complimentary skills and track records of accomplishment in the areas of computational drug discovery, experimental toxicology, statistical modeling, and software development and integration; two of the team members have had recent industrial experience before transitioning to academia. To the best of our knowledge, the results of this proposal will lead to the first publicly available in silico ADME-Tox modeling framework and predictors that can be used by the research community to analyze any set of chemicals (i.e., virtual and real compound sets). The framework will have a significant impact on compound prioritization, chemical library design, and candidate selection for preclinical and clinical development.            n/a",Robust computational framework for predictive ADME-Tox modeling,7244058,R21GM076059,"['Academia', 'Acute', 'Address', 'Area', 'Biological Availability', 'Cardiotoxicity', 'Cell Membrane Permeability', 'Chemicals', 'Chronic', 'Clinical', 'Collaborations', 'Communities', 'Computer Simulation', 'Computer software', 'Computers', 'Consensus', 'Cytochrome P450', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Drug Kinetics', 'End Point', 'Ensure', 'Environment', 'Goals', 'Health', 'Hepatotoxicity', 'Human', 'Individual', 'Internet', 'Intestinal Absorption', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Letters', 'Lung', 'Machine Learning', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Organ', 'Pharmacologic Substance', 'Postdoctoral Fellow', 'Property', 'Protocols documentation', 'Quantitative Structure-Activity Relationship', 'Records', 'Recruitment Activity', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Scheme', 'Scientist', 'Screening procedure', 'Secure', 'Source', 'Specialist', 'Standardization', 'Standards of Weights and Measures', 'Statistical Models', 'Statistically Significant', 'Structure', 'Students', 'Techniques', 'Technology', 'Testing', 'Toxic effect', 'Toxicology', 'Training', 'United States Environmental Protection Agency', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Validation', 'base', 'carcinogenicity', 'career', 'cluster computing', 'combinatorial', 'computer framework', 'data mining', 'design', 'drug discovery', 'experience', 'genotoxicity', 'innovation', 'knowledge of results', 'member', 'method development', 'neurotoxicity', 'novel', 'open source', 'pre-clinical', 'programs', 'protocol development', 'reproductive', 'skills', 'small molecule libraries', 'software development', 'tool', 'virtual', 'water solubility']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R21,2007,328325,511185245,-0.006913752135882223
Comparative and Web-Enabled Virtual Screening No abstract available n/a,Comparative and Web-Enabled Virtual Screening,7472716,P20HG003900,"['Address', 'Algorithms', 'Applications Grants', 'Area', 'Belief', 'Bioinformatics', 'Biotechnology', 'Categories', 'Cell Nucleus', 'Chemicals', 'Chemistry', 'Class', 'Classification', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Making', 'Descriptor', 'Development', 'Educational workshop', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Generations', 'Generic Drugs', 'Grant', 'Hybrids', 'Industry', 'Institution', 'Interdisciplinary Study', 'Knowledge', 'Laboratories', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Molecular Bank', 'Molecular Medicine', 'Molecular Structure', 'Online Systems', 'Pattern Recognition', 'Pharmacotherapy', 'Pilot Projects', 'Preparation', 'Process', 'Property', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Screening procedure', 'Seeds', 'Site', 'Specialist', 'Structure', 'Students', 'Sum', 'TNFRSF5 gene', 'Techniques', 'Testing', 'Travel', 'Validation', 'Vision', 'Work', 'base', 'chemical property', 'cheminformatics', 'comparative', 'computer center', 'computer science', 'concept', 'data mining', 'design', 'in vitro Assay', 'interdisciplinary approach', 'knowledge base', 'model development', 'organizational structure', 'predictive modeling', 'symposium', 'tool', 'training project', 'virtual', 'web-enabled']",NHGRI,NORTH CAROLINA STATE UNIVERSITY RALEIGH,P20,2007,363833,32532200,-0.020604959105537966
The RPI Exploratory Center for Cheminformatics (RMI) No abstract available n/a,The RPI Exploratory Center for Cheminformatics (RMI),7472067,P20HG003899,"['Address', 'Algorithms', 'Applications Grants', 'Area', 'Belief', 'Bioinformatics', 'Biotechnology', 'Categories', 'Cell Nucleus', 'Chemicals', 'Chemistry', 'Class', 'Classification', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Making', 'Descriptor', 'Development', 'Educational workshop', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Generations', 'Generic Drugs', 'Grant', 'Hybrids', 'Industry', 'Institution', 'Interdisciplinary Study', 'Knowledge', 'Laboratories', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Molecular Bank', 'Molecular Medicine', 'Molecular Structure', 'Online Systems', 'Pattern Recognition', 'Pharmacotherapy', 'Pilot Projects', 'Preparation', 'Process', 'Property', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Screening procedure', 'Seeds', 'Site', 'Specialist', 'Structure', 'Students', 'Sum', 'TNFRSF5 gene', 'Techniques', 'Testing', 'Travel', 'Validation', 'Vision', 'Work', 'base', 'chemical property', 'cheminformatics', 'computer center', 'computer science', 'concept', 'data mining', 'design', 'in vitro Assay', 'interdisciplinary approach', 'knowledge base', 'model development', 'organizational structure', 'predictive modeling', 'symposium', 'tool', 'training project']",NHGRI,RENSSELAER POLYTECHNIC INSTITUTE,P20,2007,364010,12471676,0.01966881467265351
Carolina Exploratory Center for Cheminformatics Research No abstract available n/a,Carolina Exploratory Center for Cheminformatics Research,7472715,P20HG003898,"['Address', 'Algorithms', 'Applications Grants', 'Area', 'Belief', 'Bioinformatics', 'Biotechnology', 'Categories', 'Cell Nucleus', 'Chemicals', 'Chemistry', 'Class', 'Classification', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Making', 'Descriptor', 'Development', 'Educational workshop', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Generations', 'Generic Drugs', 'Grant', 'Hybrids', 'Industry', 'Institution', 'Interdisciplinary Study', 'Knowledge', 'Laboratories', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Molecular Bank', 'Molecular Medicine', 'Molecular Structure', 'Online Systems', 'Pattern Recognition', 'Pharmacotherapy', 'Pilot Projects', 'Preparation', 'Process', 'Property', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Screening procedure', 'Seeds', 'Site', 'Specialist', 'Structure', 'Students', 'Sum', 'TNFRSF5 gene', 'Techniques', 'Testing', 'Travel', 'Validation', 'Vision', 'Work', 'base', 'chemical property', 'cheminformatics', 'computer center', 'computer science', 'concept', 'data mining', 'design', 'in vitro Assay', 'interdisciplinary approach', 'knowledge base', 'model development', 'organizational structure', 'predictive modeling', 'symposium', 'tool', 'training project']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,P20,2007,373960,511185245,0.01966881467265351
"FLUORESCENT SPECKLE MICROSCOPY    DESCRIPTION (provided by applicant): Fluorescent speckle microscopy (FSM) is a technique we initially developed for measuring the movements and sites of polymerization/depolymerization of individual microtubules (MTs) and arrays of actin filaments in motile tissue culture cells and the poleward flux of MTs within spindle fibers during mitosis. Assembly of these polymers from a pool containing a low percentage of fluorescently labeled subunits (about 1% or less) produces a random distribution of fluorophores along the polymer lattice that produces ""fluorescent speckle"" fiduciary marks varying from zero to several fluorophores (5-8) within the diffraction limited resolution of the microscope. The major focus of this application is on the further development of the FSM method for the analysis of MT function in spindle mechanics. In particular, how MT and kinetochore proteins function in spindle assembly, chromosome alignment and accurate chromosome segregation. This requires the development of new FSM microscope technology for the rapid recording of multi-wavelength and 3-D time-lapse images of MT fluorescent speckles relative to fluorescent marks or speckles at kinetochores, poles, MT associated proteins (MAPs), motor proteins and MT ends. A major next step for FSM to become a powerful analytical tool for these systems is the development of new Computer Vision methods for obtaining quantitative information about polymer movement and turnover in 2-D and 3-D at high resolution relative to the other molecular fluorescent markers in the spindle. To study protein function, we are particularly interested in optimizing FSM for genetic model organisms including budding yeast, for the biochemically accessible Xenopus egg extracts and for siRNA with mammalian tissue cells. Experience gained in the course of these studies will be used to direct and refine hardware and software development.           n/a",FLUORESCENT SPECKLE MICROSCOPY,7163551,R01GM060678,"['3-Dimensional', 'Ablation', 'Address', 'Algorithms', 'Animal Model', 'Biochemical', 'Biological', 'Biological Assay', 'Cells', 'Centromere', 'Chromatin', 'Chromosome Segregation', 'Chromosomes', 'Computer Vision Systems', 'Computer software', 'Coupling', 'Data', 'Dependence', 'Detection', 'Development', 'Fiber', 'Fluorescence', 'Genetic', 'Genetic Screening', 'Goals', 'Image', 'Imagery', 'In Situ', 'In Vitro', 'Individual', 'Kinetics', 'Kinetochores', 'Label', 'Lasers', 'Maps', 'Measurement', 'Measures', 'Mechanics', 'Methods', 'Microfilaments', 'Microscope', 'Microscopy', 'Microtubule Proteins', 'Microtubule-Associated Proteins', 'Microtubules', 'Mitosis', 'Modeling', 'Molecular', 'Motor', 'Movement', 'Organism', 'Pattern', 'Plus End of the Microtubule', 'Polymers', 'Process', 'Proteins', 'Relative (related person)', 'Resistance', 'Resolution', 'Role', 'Running', 'Saccharomycetales', 'Site', 'Small Interfering RNA', 'Staging', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Tubulin', 'Writing', 'Xenopus', 'analytical tool', 'cell type', 'charge coupled device camera', 'depolymerization', 'egg', 'experience', 'fluorophore', 'in vitro Assay', 'interest', 'numb protein', 'polymerization', 'programs', 'protein function', 'quantum', 'software development', 'tissue/cell culture']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2007,475153,511185245,-0.008277460599741033
"Metabolomic Assessment of Estrogenic Endocrine Disruptor    DESCRIPTION (provided by applicant)     Estrogenic endocrine disruptors (EEDs) are a group of structurally diverse compounds that include pharmaceuticals, dietary supplements, industrial chemicals and environmental contaminants.  They can elicit a number of adverse health effects such as hormone dependent cancers, reproductive tract abnormalities, compromised reproductive fitness, and impaired cognitive abilities.  In order to fully assess the potential adverse effects of synthetic and natural EEDs, a more comprehensive understanding of their molecular, metabolic, and tissue level effects is required within the context of a whole organism.  This collaborative proposal will elucidate the pathways, networks and signaling cascades perturbed by EEDs using the complementary multidisciplinary expertise of its team members in the areas of toxicology, molecular biology, endocrinology, multinuclear NMR spectroscopy, data management and advanced data analysis.  The comparative effects of ethynyl estradiol (EE), genistein (GEN), and o, p'-dichlorodiphenyltrichloroethane (DDT) on metabolite levels will be assessed in urine, serum and liver extracts by multinuclear (i. e., 1H, 13C, 31P) NMR spectroscopy, and complemented with histopathology examination and gene expression data from ongoing microarray studies in both mouse and rat models.  All data will be stored and archived in dbZach, a MIAME-compliant toxicogenomic supportive database that facilitates data analysis, the integration of disparate data sets, the exchange of data between investigators, and the deposition of data into public repositories.  Advanced statistical approaches, modeling and data integration tools such as neural networks, data fusion, and Baysean inference will be used to fuse these disparate data sets in order to elucidate the conserved biological networks that are of importance in response to endogenous estrogens.  Moreover, EED perturbed pathways associated with elicited effects will be further defined.  Results from these studies will not only further define the physiologic and toxic mechanisms of action of estrogenic compounds but will also demonstrate the synergy of fusing complementary microarray, metabolomic and histopathology data into a comprehensive integrative computational model.  This approach will also demonstrate the ability to maximize knowledge extraction from all disparate data available within the proposed innovative data management system when used with the advanced information tools that will be developed.            n/a",Metabolomic Assessment of Estrogenic Endocrine Disruptor,7240459,R01ES013927,"['Adverse effects', 'Affect', 'Apical', 'Archives', 'Area', 'Biochemical Pathway', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Cell Proliferation', 'Chemicals', 'Class', 'Classification', 'Clinical Chemistry', 'Cognitive', 'Complement', 'Computer Simulation', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Deposition', 'Development', 'Disease Progression', 'Dose', 'Endocrine Disruptors', 'Endocrinology', 'Engineering', 'Environmental Pollution', 'Estradiol', 'Estrogens', 'Funding', 'Future', 'Gene Expression', 'Genistein', 'Health', 'Hepatic', 'Histopathology', 'Hormones', 'Knowledge Extraction', 'Lead', 'Link', 'Lipids', 'Liver Extract', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Metabolic', 'Metabolism', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Profiling', 'Monitor', 'Multinuclear NMR', 'Mus', 'NMR Spectroscopy', 'Numbers', 'Organ Weight', 'Outcome', 'Pathway interactions', 'Pattern Recognition', 'Pharmacologic Substance', 'Physiological', 'Principal Investigator', 'Process', 'Rattus', 'Reporting', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Assessment', 'Rodent', 'Sampling', 'Screening procedure', 'Serum', 'Signal Transduction', 'Spectrum Analysis', 'System', 'Techniques', 'Time', 'Tissues', 'Toxic effect', 'Toxicogenomics', 'Toxicology', 'Urine', 'Whole Organism', 'aqueous', 'comparative', 'data integration', 'data management', 'dichlorodiphenyltrichloroethane', 'dietary supplements', 'estrogenic endocrine disruptor', 'experience', 'fitness', 'innovation', 'member', 'metabolic abnormality assessment', 'metabolomics', 'multidisciplinary', 'programs', 'repository', 'reproductive', 'research study', 'response', 'tool']",NIEHS,MICHIGAN STATE UNIVERSITY,R01,2007,543226,89938253,-0.012491575304870365
"Computational Models of Infectious Disease Threats DESCRIPTION (provided by applicant):  Microbial threats, including bioterrorism and naturally emerging infectious diseases, pose a serious challenge to national security in the United States and to health worldwide.  This proposal describes the creation of a center for computational modeling of infectious diseases at the Johns Hopkins Bloomberg School of Public Health, with the collaboration of key experts at the Brookings Institution, the National Aeronautic and Space Administration, the University of Maryland, and Imperial College (London).  The overarching aim of this project is to integrate the most advanced and powerful techniques of epidemiological data analysis with those of computer simulation (agent-based modeling) to produce a unified computational epidemiology that is scientifically sound, highly visual and user-friendly, and responsive to biosecurity and public health policy requirements.  Data analysis will be guided by the insight that epidemic patterns over space and time can be approached as nearly decomposable systems, in which frequency components of the incidence signal can be isolated and studied.  Wavelet transforms, and empiric mode decomposition using Hilbert-Huang Transforms, will be used to sift nonlinear, nonstationary epidemiological data, allowing frequency band patterns to be defined.  Isolated frequency modes will then be associated with external forcing (weather, social contact patterns) and internal dynamics (Kermack-McKendrick predator-prey models).  Results of the epidemiological data decomposition analysis, along with the knowledge of infectious disease experts, will instruct the creation and development of agent-based models.  Such models feature populations of mobile individuals in artificial societies that interact locally with other individuals.  Features of the basic model include variable social network structures, individual susceptibility and immunity, incubation periods, transmission rates, contact rates, and other selectable parameters.  After the agent-based model is calibrated to generate epidemic patterns consistent with real world epidemiology, preventive strategies including vaccination, contact tracing, isolation, quarantine, and other public health measures will be systematically introduced and their impact evaluated.  Methods will be developed for assessing the utility of individual models, and for making decisions based on combined results from more than one model.  Infectious diseases to be studied initially include smallpox, SARS, dengue, West Nile, and unknown but hypothetically plausible agents.  As part of a Cooperative Agreement, the Center will work with other research groups, a bioinformatics core group, and the NIGMS to develop data sets, software and methods, agent-based models, and visualization tools.  In an infectious disease epidemic emergency the Center will redirect its activities to serve the nation's security, as guided by the NIGMS. n/a",Computational Models of Infectious Disease Threats,7284239,U01GM070708,"['AIDS therapy', 'AIDS/HIV problem', 'Academy', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Airborne Particulate Matter', 'Algorithms', 'American', 'Americas', 'Animal Experimentation', 'Appendix', 'Archives', 'Area', 'Arthropod Vectors', 'Award', 'Bacteria', 'Beds', 'Bioinformatics', 'Biological', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Books', 'Borrelia', 'Climate', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Condition', 'Contact Tracing', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Decision Theory', 'Demography', 'Dengue', 'Dengue Hemorrhagic Fever', 'Detection', 'Development', 'Dialysis procedure', 'Disease', 'Docking', 'Doctor of Medicine', 'Doctor of Philosophy', 'Earthquakes', 'Ecology', 'Economics', 'Educational process of instructing', 'Ehrlichia', 'Emergency Situation', 'Emerging Communicable Diseases', 'Encephalitis', 'Engineering', 'Environmental Engineering technology', 'Environmental Health', 'Epidemic', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'Event', 'Evolution', 'Facility Construction Funding Category', 'Faculty', 'Foot-and-Mouth Disease', 'Frequencies', 'Game Theory', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Geography', 'Glass', 'Goals', 'HIV', 'Hantavirus', 'Head', 'Health', 'Health Policy', 'Healthcare', 'Hepatitis E', 'Human', 'Human Resources', 'Hygiene', 'Imagery', 'Immunity', 'Immunology', 'Incidence', 'Individual', 'Infectious Agent', 'Infectious Disease Epidemiology', 'Influenza', 'Informatics', 'Information Services', 'Institute of Medicine (U.S.)', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internal Medicine', 'International', 'Internet', 'Intervention', 'Joints', 'Journals', 'Knowledge', 'Laboratories', 'Laboratory Research', 'Laboratory Study', 'Lead', 'Legal patent', 'Leptospira', 'Libraries', 'Location', 'London', 'Lung', 'Machine Learning', 'Maintenance', 'Malaria', 'Maryland', 'Master&apos', 's Degree', 'Mathematical Biology', 'Mathematics', 'Measles', 'Measures', 'Mechanics', 'Methods', 'Microbiology', 'Military Personnel', 'Modeling', 'Modified Smallpox', 'Molecular', 'National Institute of General Medical Sciences', 'National Security', 'New York', 'Nonlinear Dynamics', 'Nonparametric Statistics', 'Observational Study', 'Oceanography', 'Outcome', 'Paper', 'Pattern', 'Physical Dialysis', 'Play', 'Policies', 'Policy Maker', 'Population', 'Positioning Attribute', 'Predisposition', 'Pregnancy Outcome', 'Prevention strategy', 'Preventive', 'Principal Investigator', 'Prion Diseases', 'Procedures', 'Process', 'Provider', 'Proxy', 'Public Health', 'Public Health Schools', 'Public Policy', 'Publications', 'Publishing', 'Purpose', 'Quarantine', 'Rate', 'Recording of previous events', 'Reference Standards', 'Relative (related person)', 'Research', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Rickettsia', 'Risk Assessment', 'Rodent', 'Role', 'Route', 'Schedule', 'Schools', 'Science', 'Scientist', 'Screening procedure', 'Security', 'Series', 'Severe Acute Respiratory Syndrome', 'Signal Transduction', 'Simulate', 'Smallpox', 'Social Network', 'Social Sciences', 'Societies', 'Software Tools', 'Space Flight', 'Statistical Computing', 'Statistical Models', 'Structure', 'Students', 'System', 'Systems Analysis', 'Testing', 'Theoretical model', 'Time', 'Time Series Analysis', 'Training', 'Tropical Medicine', 'U-Series Cooperative Agreements', 'Uncertainty', 'United States', 'United States National Academy of Sciences', 'United States National Aeronautics and Space Administration', 'Universities', 'Vaccination', 'Variant', 'Vector-transmitted infectious disease', 'Violence', 'Viral', 'Viral Hemorrhagic Fevers', 'Virus', 'Virus Diseases', 'Visual', 'Weather', 'West Nile virus', 'Work', 'base', 'biosecurity', 'c new', 'college', 'computer science', 'concept', 'design', 'disease natural history', 'disease transmission', 'disorder prevention', 'disorder risk', 'editorial', 'experience', 'improved', 'indexing', 'infectious disease model', 'insight', 'interest', 'mathematical model', 'member', 'microbial', 'models and simulation', 'network models', 'pathogen', 'peer', 'predictive modeling', 'prevent', 'professor', 'programs', 'remote sensing', 'respiratory', 'simulation', 'skills', 'social', 'social organization', 'sound', 'theories', 'tool', 'transmission process', 'user-friendly', 'vaccination strategy']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2007,588968,570146095,0.011193141900597737
"Nanopore study of single antibody-antigen interactions DESCRIPTION:    Nanopore-based single-molecule detection has recently become established as a new tool in single molecule biophysics. Evidence is presented that single antibodies can be observed with a nanopore detector, which presents a wide range of possibilities for immunological research. The hypothesis to be tested is that nanopore-based detection can be used to study single molecule dynamics of antibody-antigen interaction and analyze conformational changes that occur in antibody upon binding to antigen. This application aims to develop the utility of the nanopore-based approach through improvements in both the detection device and the software used to extract information from the channel current signal. At the same time, these studies will allow the Candidate to gain expertise in immunology and the biophysical study of protein structure and function. To study the single molecule dynamics of antibody-antigen interaction, the following three specific aims are proposed:   1. Extend nanopore based detection to nanopore/antibody based detection.   2. Implement machine learning software for automated nanopore/antibody signal analysis and experimental feedback.   3. Use well-characterized, genetically engineered, antibodies to test the utility of the nanopore device to analyze motion in the antibody molecule.   These studies will expand the utility of nanopore devices to study single molecule protein interactions.   Information gained will lead to a better understanding of the molecular dynamics associated with antigen binding by antibody and the subsequent initiation of effector functions. Since most biological nanopore variants derive from pore-forming toxins, nanopore device enhancements eventually may lead to new methods for antibody and antimicrobial-peptide immunological screening. Antibody-based nanopore devices may also serve as highly sensitive immunosensors. n/a",Nanopore study of single antibody-antigen interactions,7119996,K22LM008794,"['antigen antibody reaction', 'bioinformatics', 'biomedical automation', 'computer program /software', 'computer system design /evaluation', 'intermolecular interaction', 'molecular dynamics', 'nanotechnology', 'pore forming protein', 'protein quantitation /detection']",NLM,CHILDREN'S HOSPITAL (NEW ORLEANS),K22,2006,162000,157330,0.01616482147748771
"Protein Packing Defects as Functional Markers and Drug Targets DESCRIPTION (provided by applicant):  Our preliminary structure-based investigations show that water exclusion from deficiently packed hydrogen bonds and other pre-formed electrostatic interactions constitutes a driving factor conferring high specificity to protein association. Thus, an evolutionary conserved feature, the under-dehydrated hydrogen bond, termed dehydron, appears to be a structural marker for interactivity. Dehydrons were experimentally and statistically shown to constitute sticky spots on the protein surface and to be abundant at protein-protein interfaces, especially at those that cannot be understood in terms of standard interactions. The dehydron distribution on the surface of soluble proteins constitutes a determinant of the propensity for association and aberrant aggregation. The identification of dehydrons has relied so far on detailed structural information, a limitation precluding a proteomic analysis. This proposal is geared at introducing a sequence-based predictive method to establish the biological relevance of dehydrons and their potential as markers for drugable targets. Thus, we intend to introduce a powerful unsupervised scanning technology to detect signals of interactivity and drugability at a genomic scale. This goal requires constructing a machine-learning discriminator trained on a structural database. The over-all aim is to develop a sequence-based multi-purpose tool to expand the universe of drugable targets, diagnose propensity for aberrant aggregation and make interactomic inferences. The efficacy of our predictor will be tested on five grounds: a) Assaying for amino-acid variability and determining whether residues predicted solely from sequence to be engaged in dehydrons are actually conserved, b) Using a redundancy-free curated PDB sample as training set, we shall determine the accuracy and precision of the sequence-based predictor using a nonhomologous PDB complement set and annotated SwissProt entries as testing sets, c) Contrasting our results with an alternative dehydron predictor based on a reliable sequence-based predictor of native disorder (PONDR(r)). This dehydron predictor is based on a correlation found between the extent of hydrogen-bond packing and the score of structural disorder, d) Contrasting sequence-based diagnosis of amyloidogenic aggregation with SwissProt annotations and other annotated disease-related sequence repositories; e) Contrasting compiled drug-target quality assessments and structural data and screening profiles for protein-ligand associations with the predicted dehydron patterns. Thus, the novel design concept of ""drug inhibitor as a wrapper of functional packing defects"" will be explored and validated. n/a",Protein Packing Defects as Functional Markers and Drug Targets,7118204,R01GM072614,"['aminoacid', 'amyloid proteins', 'bioinformatics', 'biomarker', 'chemical genetics', 'computational biology', 'computer program /software', 'hydrogen bond', 'information systems', 'method development', 'pharmacogenetics', 'protein sequence', 'protein structure function', 'proteomics']",NIGMS,RICE UNIVERSITY,R01,2006,294217,19199034,0.0038660154237451262
"Vortex Tubed Thermocycler with Intelligent Software    DESCRIPTION (provided by applicant): A novel system is proposed for the rapid identification of DNA. The system comprises of a unique thermocycler platform built around the extraordinary vortex tube, detection optics, intelligent software to provide users with information on most ideal operating conditions, and virtual insight into the PCR process as it progresses. An intelligent user interface will allow DNA amplification/detection on an unprecedented timescale (less than 10 minutes). A multi-disciplinary team that consists of a chemical engineer, a mechanical engineer, and two biochemists has been assembled for this project. The efficiency of the vortex tube will be optimized by the use of computational fluid dynamics. Heat transfer between the gas phase and the cuvets will be improved through computational fluid dynamic calculations. The intelligent software consists of a detailed mathematical model that uses similar starting conditions as the initial cuvet composition to model the amplification progress and it performs a virtual PCR in parallel with the actual process. The virtual PCR will become a quantitative tool point-of-care diagnosis of a wide variety of heritable and infectious diseases. The virtues of the intelligent vortex tube PCRJet are: speed, versatility, reliability, portability and low cost.         n/a",Vortex Tubed Thermocycler with Intelligent Software,7243612,R33RR020219,"['DNA', 'artificial intelligence', 'bacterial DNA', 'bioengineering /biomedical engineering', 'bioinformatics', 'biomedical equipment development', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'diagnosis design /evaluation', 'diagnostic tests', 'mathematical model', 'neoplasm /cancer genetics', 'nucleic acid amplification techniques', 'nucleic acid purification', 'nucleic acid quantitation /detection', 'optics', 'polymerase chain reaction', 'portable biomedical equipment', 'virus DNA']",NCRR,UNIVERSITY OF NEBRASKA LINCOLN,R33,2006,350636,22951068,0.016863889334236055
"Least Angle Regression    DESCRIPTION (provided by applicant): This SBIR project aims to produce superior methods and software for classification and regression when there are many potential predictor variables to choose from. The methods should (1) produce stable results, where small changes in the data do not produce major changes in the variables selected or in model predictions; (2) produce accurate predictions; (3) facilitate scientific interpretation, by selecting a smaller subset of predictors which provide the best predictions; (4) allow continuous and categorical variables; and (5) support linear regression, logistic regression (predicting a binary outcome), survival analysis, and other types of regression. This project is based on least angle regression, which unifies and provides a fast implementation for a number of modern regression techniques. Least angle regression has great potential, but currently available software is limited in scope and robustness. The outcome of this project should be software which is more robust and widely applicable. This software would apply broadly, including to medical diagnosis, detecting cancer, feature selection in microarrays, and modeling patient characteristics like blood pressure. Phase I work demonstrates feasibility by extending least angle work in three key directions-categorical predictors, logistic regression, and a numerically-accurate implementation. Phase II goals include extensions to other types of explanatory variables (e.g. polynomial or spline functions, and interactions between variables), to survival and other additional regression models, and to handle missing data and massive data sets. This proposed software will enable medical researchers to obtain high prediction accuracy, and obtain stable and interpretable results, in high-dimensional situations. Predicting outcomes based on covariates, determining which covariates most affect outcomes, and adjusting treatment effects estimates for covariates, are among the most important problems in biostatistics. Prediction and feature selection are particularly difficult when there are more possible features than samples; gene microarrays and protein mass spectrometry are extreme examples of this, producing thousands to millions of measurements per sample. LARS excels at feature selection; the proposed software should enable medical researchers to obtain stable and interpretable models with better prediction accuracy in high-dimensional situations.             n/a",Least Angle Regression,7219604,R44GM074313,"['computer human interaction', 'computer program /software', 'data management', 'handbook', 'mathematical model', 'mathematics', 'method development', 'microarray technology', 'model design /development', 'statistics /biometry']",NIGMS,INSIGHTFUL CORPORATION,R44,2006,374846,0,0.02153033577933077
"LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction    DESCRIPTION (provided by applicant): The high cost ($0.8 - $1.7 billion) and long time frames (about 13 years) required to introduce new drugs to the market contributes substantially to spiraling health care costs and diseases persisting without effective cures. A major factor is the high attrition rate of new compounds failing due to toxicity identified years into clinical trials. This particular circumstance cost the pharmaceutical industry approximately $8 billion in 2003. In silico tools generally offer the promise of identifying toxicity issues much more rapidly than clinical methods, however, they are not sufficiently accurate for pharmaceutical companies to confidently make definitive early screening and related investment decisions. LiverTox is a highly advanced, self-learning liver toxicity prediction tool that represents a quantum leap over current in silico methods. It offers a highly innovative use of multiple analytical approaches to accurately predict the toxicity of candidate Pharmaceuticals in the liver. A differentiating capability is its self-learning computational neural networks (CNNs) and wavelets. They rapidly assimilate massive volumes of information from LiverTox's extensive, dynamic, and thoroughly reviewed databases. Initially, LiverTox will generate predictions derived from five independent CNN-based submodules; one trained in advanced computational chemistry methods to make quantitative structure activity relationship (QSAR) analyses; a second trained with microarray data; a third trained with Massively Parallel Signature Sequencing and Gene Expression (MPSS/GE) data; and fourth and fifth submodules trained with proteomics and metabolomics/metabonomics data, respectively. Challenging LiverTox with new chemical formulations triggers the five independent submodules to each make toxicity endpoint predictions drawing upon its knowledge base and its similarity analysis/fuzzy logic/statistical training. This tool's flexible, highly advanced system architecture and advanced learning capabilities using data obtained from diverse techniques enable it to rapidly digest new data, build upon new data acquisition techniques, and use prior lessons learned to achieve overall toxicity predictions with greater than 95% accuracy. LiverTox's ability to rapidly and accurately predict the toxicity of drug candidates will allow pharmaceutical companies to move from discovery to curing disease faster, at greatly reduced cost, and with less reliance on animal-based tests.         n/a",LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction,7125135,R42ES013321,"['artificial intelligence', 'chemical structure function', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'drug discovery /isolation', 'drug screening /evaluation', 'functional /structural genomics', 'hepatotoxin', 'microarray technology', 'toxicant screening']",NIEHS,"YAHSGS, LLC",R42,2006,387181,0,-0.012354154738543182
"Urine protein markers of disease in lupus nephritis DESCRIPTION (provided by applicant):     Lupus nephritis (LN) affects up to 65% of systemic lupus erythematosus (SLE) patients and results in renal failure in up to 42% of patients after five years. Renal failure rates can vary up to eight fold over five years in aggressive LN despite use of the same treatment protocol. A more reliable means of determining prognosis is therefore required. The long-term objective of this proposal is to identify noninvasive urine protein markers of disease that are predictive of renal compromise. Previous studies in this area have resulted in the identification of a limited number of candidate proteins that correlate with disease type and glomerular inflammatory activity. To develop models of prognostic indicators in a heterogeneous disorder such as LN, the candidate protein approach has limited power. The Specific Aims of this proposal are to determine: 1) differentially expressed proteins in the urine of LN subjects (revealed by proteomic techniques and artificial neural network (ANN) technology) as surrogate markers of the WHO class, activity, and chronicity of glomerular lesions on renal biopsy and 2) the feasibility of a larger study designed to examine these urine protein markers as predictors of renal outcome in LN. To address these aims, urine from 1) LN subjects undergoing renal biopsy and 2) subjects with either LN or SLE without LN will be analyzed using proteomic techniques. All subjects will be followed prospectively for renal outcomes. Urine proteins will be concentrated and resolved by 2D-gel electrophoresis. Spots will be identified by location and removed for identification by matrix assisted laser desorption/ionization mass spectrometry and peptide mass fingerprinting. Data from these procedures will be analyzed using ANN modeling to determine surrogate urine protein markers of WHO class, activity, and chronicity of LN renal biopsies. ANN will be used to determine surrogate markers that predict compromise in renal function due to LN. These markers will be compared to renal biopsy and traditional laboratory and clinical indices of disease as predictors of renal outcome. Within the two years of this study, surrogate urine protein markers of biopsy findings should be identified, and the feasibility of a long-term outcome study should be determined. The ultimate results of this study could revolutionize the manner in which treatment for LN is determined. Due to the power of proteomic and ANN techniques, novel mediators of disease activity and damage should be identified and thus contribute to the development of more targeted therapies for LN. n/a",Urine protein markers of disease in lupus nephritis,6951552,R21AR051719,"['artificial intelligence', 'biomarker', 'clinical research', 'diagnosis design /evaluation', 'human subject', 'kidney disorder diagnosis', 'lupus nephritis', 'mass spectrometry', 'matrix assisted laser desorption ionization', 'noninvasive diagnosis', 'patient oriented research', 'prognosis', 'protein purification', 'proteomics', 'two dimensional gel electrophoresis', 'urinalysis']",NIAMS,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R21,2005,63000,136810522,0.0064700848062087144
"Least Angle Regression DESCRIPTION (provided by applicant): This SBIR project aims to produce superior methods and software for classification and regression when there are many potential predictor variables to choose from. The methods should (1) produce stable results, where small changes in the data do to produce major changes in the variables selected or in model predictions, (2) produce accurate predictions, (3) facilitate scientific interpretation, by selecting a smaller subset of predictors which provide the best predictions, (4) allow continuous and categorical variables, and (5) support linear regression, logistic regression (predicting a binary outcome), survival analysis, and other types of regression. This project is based on least angle regression, which unifies and provides a fast implementation for a number of modern regression techniques. Least angle regression has great potential, but the state of the art is limited to linear regression with continuous or binary variables, and uses numerically-unstable calculations. The outcome of this project should be software which is more robust and widely applicable. This software would apply broadly, including to medical diagnosis, detecting cancer, feature selection in microarrays, and modeling patient characteristics like blood pressure.  Phase I work will demonstrate feasibility by extending least angle work in three key directions-categorical predictors, logistic regression, and a numerically-accurate implementation. Phase II will extend the work to other types of explanatory variables (e.g. polynomial or spline functions, and interactions between variables), and to survival and other additional regression models. This proposed software will enable medical researchers to obtain high prediction accuracy, and obtain stable and interpretable results, in high-dimensional situations. n/a",Least Angle Regression,6933500,R43GM074313,"['clinical research', 'computational biology', 'computer graphics /printing', 'computer human interaction', 'computer program /software', 'data collection methodology /evaluation', 'data quality /integrity', 'human data', 'mathematical model', 'model design /development', 'statistics /biometry', 'technology /technique development']",NIGMS,INSIGHTFUL CORPORATION,R43,2005,99685,0,0.010885800027175355
"Multi-microphone long probe for OAE acquisition DESCRIPTION (provided by applicant): The development of an innovative multi-microphone probe and acquisition system for recording otoacoustic emissions {OAEs} with advanced noise cancellation algorithms, increased frequency and intensity ranges and pressurization capabilities is proposed. Two advanced noise cancellation algorithm will be implemented: 1) a multi-reference adaptive noise cancellation (ANC) network and 2) two-dimensional filtering. These algorithms will utilize the independent measurements provided by the multiple microphones in order to reduce noise contaminants. Each microphone or microphone groupings will be connected to individual analog-to-digital (A/D) converters in order to allow for the implementation of the digital signal processing algorithms. The pressurization capabilities of the probe will allow implementation of tympanometry and the acquisition of OAEs while compensating for pressure imbalances between the outer and middle ear. Results from a prototype single microphone long probe are presented demonstrating that the design concept is valid and provides good quality OAE recordings while reducing the undesirable effects of the metal response. The proposed probe will also improve upon the limited dynamic and frequency range of current OAE probes. The probe is expected to be able to provide stimulus levels of up to 90 dB HL and a frequency response of up to 24 kHz. During Phase I, various probes will be constructed and tested under different noise conditions in adult and infant subjects. During Phase II, the pressurization capabilities of the new probe will be further developed and examined. The optimal probe designed will be implemented along with the optimal noise cancellation algorithm and tested in a comprehensive clinical study incorporating the pressurization capabilities of the probe. n/a",Multi-microphone long probe for OAE acquisition,6933674,R43DC007543,"['adult human (21+)', 'artificial intelligence', 'bioengineering /biomedical engineering', 'biomedical equipment development', 'clinical biomedical equipment', 'clinical research', 'computer program /software', 'ear disorder diagnosis', 'infant human (0-1 year)', 'mathematics', 'noise', 'otoacoustic emission', 'sound frequency']",NIDCD,INTELLIGENT HEARING SYSTEMS,R43,2005,100000,618360,-0.002846617485471015
"Nanopore study of single antibody-antigen interactions DESCRIPTION:    Nanopore-based single-molecule detection has recently become established as a new tool in single molecule biophysics. Evidence is presented that single antibodies can be observed with a nanopore detector, which presents a wide range of possibilities for immunological research. The hypothesis to be tested is that nanopore-based detection can be used to study single molecule dynamics of antibody-antigen interaction and analyze conformational changes that occur in antibody upon binding to antigen. This application aims to develop the utility of the nanopore-based approach through improvements in both the detection device and the software used to extract information from the channel current signal. At the same time, these studies will allow the Candidate to gain expertise in immunology and the biophysical study of protein structure and function. To study the single molecule dynamics of antibody-antigen interaction, the following three specific aims are proposed:   1. Extend nanopore based detection to nanopore/antibody based detection.   2. Implement machine learning software for automated nanopore/antibody signal analysis and experimental feedback.   3. Use well-characterized, genetically engineered, antibodies to test the utility of the nanopore device to analyze motion in the antibody molecule.   These studies will expand the utility of nanopore devices to study single molecule protein interactions.   Information gained will lead to a better understanding of the molecular dynamics associated with antigen binding by antibody and the subsequent initiation of effector functions. Since most biological nanopore variants derive from pore-forming toxins, nanopore device enhancements eventually may lead to new methods for antibody and antimicrobial-peptide immunological screening. Antibody-based nanopore devices may also serve as highly sensitive immunosensors. n/a",Nanopore study of single antibody-antigen interactions,6959048,K22LM008794,"['antigen antibody reaction', 'bioinformatics', 'biomedical automation', 'computer program /software', 'computer system design /evaluation', 'intermolecular interaction', 'molecular dynamics', 'nanotechnology', 'pore forming protein', 'protein quantitation /detection']",NLM,CHILDREN'S HOSPITAL (NEW ORLEANS),K22,2005,162000,157330,0.01616482147748771
"Vortex Tubed Thermocycler with Intelligent Software    DESCRIPTION (provided by applicant): A novel system is proposed for the rapid identification of DNA. The system comprises of a unique thermocycler platform built around the extraordinary vortex tube, detection optics, intelligent software to provide users with information on most ideal operating conditions, and virtual insight into the PCR process as it progresses. An intelligent user interface will allow DNA amplification/detection on an unprecedented timescale (less than 10 minutes). A multi-disciplinary team that consists of a chemical engineer, a mechanical engineer, and two biochemists has been assembled for this project. The efficiency of the vortex tube will be optimized by the use of computational fluid dynamics. Heat transfer between the gas phase and the cuvets will be improved through computational fluid dynamic calculations. The intelligent software consists of a detailed mathematical model that uses similar starting conditions as the initial cuvet composition to model the amplification progress and it performs a virtual PCR in parallel with the actual process. The virtual PCR will become a quantitative tool point-of-care diagnosis of a wide variety of heritable and infectious diseases. The virtues of the intelligent vortex tube PCRJet are: speed, versatility, reliability, portability and low cost.         n/a",Vortex Tubed Thermocycler with Intelligent Software,6914863,R21RR020219,"['DNA', 'artificial intelligence', 'bacterial DNA', 'bioengineering /biomedical engineering', 'bioinformatics', 'biomedical equipment development', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'diagnosis design /evaluation', 'diagnostic tests', 'mathematical model', 'neoplasm /cancer genetics', 'nucleic acid amplification techniques', 'nucleic acid purification', 'nucleic acid quantitation /detection', 'optics', 'polymerase chain reaction', 'portable biomedical equipment', 'virus DNA']",NCRR,UNIVERSITY OF NEBRASKA LINCOLN,R21,2005,167918,22951068,0.016863889334236055
"LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction    DESCRIPTION (provided by applicant): The high cost ($0.8 - $1.7 billion) and long time frames (about 13 years) required to introduce new drugs to the market contributes substantially to spiraling health care costs and diseases persisting without effective cures. A major factor is the high attrition rate of new compounds failing due to toxicity identified years into clinical trials. This particular circumstance cost the pharmaceutical industry approximately $8 billion in 2003. In silico tools generally offer the promise of identifying toxicity issues much more rapidly than clinical methods, however, they are not sufficiently accurate for pharmaceutical companies to confidently make definitive early screening and related investment decisions. LiverTox is a highly advanced, self-learning liver toxicity prediction tool that represents a quantum leap over current in silico methods. It offers a highly innovative use of multiple analytical approaches to accurately predict the toxicity of candidate Pharmaceuticals in the liver. A differentiating capability is its self-learning computational neural networks (CNNs) and wavelets. They rapidly assimilate massive volumes of information from LiverTox's extensive, dynamic, and thoroughly reviewed databases. Initially, LiverTox will generate predictions derived from five independent CNN-based submodules; one trained in advanced computational chemistry methods to make quantitative structure activity relationship (QSAR) analyses; a second trained with microarray data; a third trained with Massively Parallel Signature Sequencing and Gene Expression (MPSS/GE) data; and fourth and fifth submodules trained with proteomics and metabolomics/metabonomics data, respectively. Challenging LiverTox with new chemical formulations triggers the five independent submodules to each make toxicity endpoint predictions drawing upon its knowledge base and its similarity analysis/fuzzy logic/statistical training. This tool's flexible, highly advanced system architecture and advanced learning capabilities using data obtained from diverse techniques enable it to rapidly digest new data, build upon new data acquisition techniques, and use prior lessons learned to achieve overall toxicity predictions with greater than 95% accuracy. LiverTox's ability to rapidly and accurately predict the toxicity of drug candidates will allow pharmaceutical companies to move from discovery to curing disease faster, at greatly reduced cost, and with less reliance on animal-based tests.         n/a",LiverTox: Advanced QSAR and Toxicogenomic Software for Hepatoxicity Prediction,7052491,R42ES013321,"['artificial intelligence', 'chemical structure function', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'drug discovery /isolation', 'drug screening /evaluation', 'functional /structural genomics', 'hepatotoxin', 'microarray technology', 'toxicant screening']",NIEHS,"YAHSGS, LLC",R42,2005,180862,0,-0.012354154738543182
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6850134,R01AR042739,"['artificial intelligence', 'bone density', 'bone fracture', 'clinical research', 'computer assisted diagnosis', 'densitometry', 'diagnosis design /evaluation', 'disease /disorder proneness /risk', 'hip', 'human subject', 'information systems', 'limbs', 'mathematical model', 'noninvasive diagnosis', 'osteoporosis', 'photon absorptiometry', 'radiography', 'spine', 'women&apos', 's health']",NIAMS,UNIVERSITY OF CHICAGO,R01,2005,297104,246330700,0.03762237619226509
"Protein Packing Defects as Functional Markers and Drug Targets DESCRIPTION (provided by applicant):  Our preliminary structure-based investigations show that water exclusion from deficiently packed hydrogen bonds and other pre-formed electrostatic interactions constitutes a driving factor conferring high specificity to protein association. Thus, an evolutionary conserved feature, the under-dehydrated hydrogen bond, termed dehydron, appears to be a structural marker for interactivity. Dehydrons were experimentally and statistically shown to constitute sticky spots on the protein surface and to be abundant at protein-protein interfaces, especially at those that cannot be understood in terms of standard interactions. The dehydron distribution on the surface of soluble proteins constitutes a determinant of the propensity for association and aberrant aggregation. The identification of dehydrons has relied so far on detailed structural information, a limitation precluding a proteomic analysis. This proposal is geared at introducing a sequence-based predictive method to establish the biological relevance of dehydrons and their potential as markers for drugable targets. Thus, we intend to introduce a powerful unsupervised scanning technology to detect signals of interactivity and drugability at a genomic scale. This goal requires constructing a machine-learning discriminator trained on a structural database. The over-all aim is to develop a sequence-based multi-purpose tool to expand the universe of drugable targets, diagnose propensity for aberrant aggregation and make interactomic inferences. The efficacy of our predictor will be tested on five grounds: a) Assaying for amino-acid variability and determining whether residues predicted solely from sequence to be engaged in dehydrons are actually conserved, b) Using a redundancy-free curated PDB sample as training set, we shall determine the accuracy and precision of the sequence-based predictor using a nonhomologous PDB complement set and annotated SwissProt entries as testing sets, c) Contrasting our results with an alternative dehydron predictor based on a reliable sequence-based predictor of native disorder (PONDR(r)). This dehydron predictor is based on a correlation found between the extent of hydrogen-bond packing and the score of structural disorder, d) Contrasting sequence-based diagnosis of amyloidogenic aggregation with SwissProt annotations and other annotated disease-related sequence repositories; e) Contrasting compiled drug-target quality assessments and structural data and screening profiles for protein-ligand associations with the predicted dehydron patterns. Thus, the novel design concept of ""drug inhibitor as a wrapper of functional packing defects"" will be explored and validated. n/a",Protein Packing Defects as Functional Markers and Drug Targets,6965896,R01GM072614,"['aminoacid', 'amyloid proteins', 'bioinformatics', 'biomarker', 'chemical genetics', 'computational biology', 'computer program /software', 'hydrogen bond', 'information systems', 'method development', 'pharmacogenetics', 'protein sequence', 'protein structure function', 'proteomics']",NIGMS,RICE UNIVERSITY,R01,2005,309178,19199034,0.0038660154237451262
"Urine protein markers of disease in lupus nephritis DESCRIPTION (provided by applicant):     Lupus nephritis (LN) affects up to 65% of systemic lupus erythematosus (SLE) patients and results in renal failure in up to 42% of patients after five years. Renal failure rates can vary up to eight fold over five years in aggressive LN despite use of the same treatment protocol. A more reliable means of determining prognosis is therefore required. The long-term objective of this proposal is to identify noninvasive urine protein markers of disease that are predictive of renal compromise. Previous studies in this area have resulted in the identification of a limited number of candidate proteins that correlate with disease type and glomerular inflammatory activity. To develop models of prognostic indicators in a heterogeneous disorder such as LN, the candidate protein approach has limited power. The Specific Aims of this proposal are to determine: 1) differentially expressed proteins in the urine of LN subjects (revealed by proteomic techniques and artificial neural network (ANN) technology) as surrogate markers of the WHO class, activity, and chronicity of glomerular lesions on renal biopsy and 2) the feasibility of a larger study designed to examine these urine protein markers as predictors of renal outcome in LN. To address these aims, urine from 1) LN subjects undergoing renal biopsy and 2) subjects with either LN or SLE without LN will be analyzed using proteomic techniques. All subjects will be followed prospectively for renal outcomes. Urine proteins will be concentrated and resolved by 2D-gel electrophoresis. Spots will be identified by location and removed for identification by matrix assisted laser desorption/ionization mass spectrometry and peptide mass fingerprinting. Data from these procedures will be analyzed using ANN modeling to determine surrogate urine protein markers of WHO class, activity, and chronicity of LN renal biopsies. ANN will be used to determine surrogate markers that predict compromise in renal function due to LN. These markers will be compared to renal biopsy and traditional laboratory and clinical indices of disease as predictors of renal outcome. Within the two years of this study, surrogate urine protein markers of biopsy findings should be identified, and the feasibility of a long-term outcome study should be determined. The ultimate results of this study could revolutionize the manner in which treatment for LN is determined. Due to the power of proteomic and ANN techniques, novel mediators of disease activity and damage should be identified and thus contribute to the development of more targeted therapies for LN. n/a",Urine protein markers of disease in lupus nephritis,6838455,R21AR051719,"['artificial intelligence', 'biomarker', 'clinical research', 'diagnosis design /evaluation', 'human subject', 'kidney disorder diagnosis', 'lupus nephritis', 'mass spectrometry', 'matrix assisted laser desorption ionization', 'noninvasive diagnosis', 'patient oriented research', 'prognosis', 'protein purification', 'proteomics', 'two dimensional gel electrophoresis', 'urinalysis']",NIAMS,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R21,2004,63000,136810522,0.0064700848062087144
"Tree Ensemble Regression and Classification Methods    DESCRIPTION (provided by applicant):    This SBIR aims to produce next generation classification and regression software based upon ensembles of decision trees: bagging, random forests, and boosting. The prediction accuracy of these methods has caused much excitement in the machine learning community, and both challenges and complements the data modeling culture prevalent among biostatisticians. Recent research extends the methodology to likelihood based methods used in biostatistics, leading to models for survival data and generalized forest models. Generalized forest models extend regression forests in the same way that generalized linear models extend linear models.      This software would apply broadly, including to medical diagnosis, prognostic modeling, and detecting cancer; and for modeling patient characteristics like blood pressure, discrete responses in clinical trials, and count data.      Phase I work will prototype software for survival data, and investigate the performance of ensemble methods on simulated and real data. For survival applications, we will assess out-of-bag estimates of performance, and investigate measures of variable importance and graphics that help clinicians understand the results. Experience writing prototypes and using them on data will lead to a preliminary software design that serves as the foundation of Phase II work.      Phase II will expand upon this work to create commercial software. We will research and implement algorithms for a wider range of applications including generalized forest models, classification, and least squares regression. We will also implement robust loss criteria that enable good performance on noisy data, and make adaptations to handle large data sets.      This proposed software will enable medical researchers to obtain high prediction accuracy, and complement traditional tools like discriminant analysis, linear and logistic regression models, and the Cox model.         n/a",Tree Ensemble Regression and Classification Methods,6832086,R43CA105724,"['clinical research', 'computer assisted medical decision making', 'computer graphics /printing', 'computer human interaction', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'human data', 'mathematical model', 'method development', 'model design /development', 'neoplasm /cancer classification /staging', 'neoplasm /cancer diagnosis', 'neoplasm /cancer remission /regression', 'prognosis', 'statistics /biometry']",NCI,INSIGHTFUL CORPORATION,R43,2004,99937,0,0.011400315312135176
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6849505,R01AR042739,"['artificial intelligence', 'bone density', 'bone fracture', 'clinical research', 'computer assisted diagnosis', 'densitometry', 'diagnosis design /evaluation', 'disease /disorder proneness /risk', 'hip', 'human subject', 'information systems', 'limbs', 'mathematical model', 'noninvasive diagnosis', 'osteoporosis', 'photon absorptiometry', 'radiography', 'spine', 'women&apos', 's health']",NIAMS,UNIVERSITY OF CHICAGO,R01,2004,105415,246330700,0.03762237619226509
"A Novel Probabilistic Engine for Virtual Screening DESCRIPTION (provided by applicant): The goal of this work is to provide a novel probabilistic computational engine for docking-based virtual screening. The engine is based on probabilistic model of Markov Random Fieds (MRF). MRF's have proven successful in other fields such as Computer Vision, and can be seen as a 3D analog of the successful 1D application of Hidden Markov Models to bioinformatics. The docking of a rigid ligand or ligand fragment into a protein active site is modeled as a weighted graphical match of an abstracted description of the ligand to an abstracted description of the active site. These abstracted descriptions are graphs, whose nodes are chemical entities (hydrogen bond acceptors/donors, hydrophobic spheres and etc.) and whose edges are associated distance constraints. The weighted graph-matching problem is expressed as an MRF, whose solution minimizes its associated free energy function. A fast, convergent message-passing scheme called Belief Propagation is used to solve the MRF. The result is a probability distribution that describes all possible placements of the ligand into the active site. Individual low-energy placements of the molecule are obtained by marginalizing this probability distribution. The method provides a fast and mathematically complete examination of possible fits of the ligand into the protein active site, and our prototype MRF application demonstrates excellent timing and completeness properties. The method also provides an attractive data structure enabling a variety of applications. The data structure intrinsically admits an enriched description of the active site. This description can incorporate an extended set of chemical substructures for matching at its nodes. It also can incorporate sets of probabilistic beliefs, expressed as probabilistic prior distributions. These can be used to bias matches according to known actives. Our goals in Phase I are to further develop our prototype into a robust MRF-based docking engine to positioning rigid molecules and molecular fragments into protein active sites. Our goals in Phase II will be to implement applications based on the MRF docking engine: (i) inclusion flexible ligand docking, (ii) incorporation of flexible side chains into docking, (iii) de-novo ligand design, and (iv) docking into multiple aligned proteins. We will seek corporate partners interested in collaborating on applying the technologies to specific problems in drug discovery in Phase I1. The technology developed will be sold as commercial software in Phase III. n/a",A Novel Probabilistic Engine for Virtual Screening,6786885,R43GM071055,"['binding sites', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'ligands', 'statistics /biometry']",NIGMS,"BIOCOMPUTING GROUP, INC.",R43,2004,153420,0,-0.01207849753490973
"Vortex Tubed Thermocycler with Intelligent Software    DESCRIPTION (provided by applicant): A novel system is proposed for the rapid identification of DNA. The system comprises of a unique thermocycler platform built around the extraordinary vortex tube, detection optics, intelligent software to provide users with information on most ideal operating conditions, and virtual insight into the PCR process as it progresses. An intelligent user interface will allow DNA amplification/detection on an unprecedented timescale (less than 10 minutes). A multi-disciplinary team that consists of a chemical engineer, a mechanical engineer, and two biochemists has been assembled for this project. The efficiency of the vortex tube will be optimized by the use of computational fluid dynamics. Heat transfer between the gas phase and the cuvets will be improved through computational fluid dynamic calculations. The intelligent software consists of a detailed mathematical model that uses similar starting conditions as the initial cuvet composition to model the amplification progress and it performs a virtual PCR in parallel with the actual process. The virtual PCR will become a quantitative tool point-of-care diagnosis of a wide variety of heritable and infectious diseases. The virtues of the intelligent vortex tube PCRJet are: speed, versatility, reliability, portability and low cost.         n/a",Vortex Tubed Thermocycler with Intelligent Software,6810083,R21RR020219,"['DNA', 'artificial intelligence', 'bacterial DNA', 'bioengineering /biomedical engineering', 'bioinformatics', 'biomedical equipment development', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'diagnosis design /evaluation', 'diagnostic tests', 'mathematical model', 'neoplasm /cancer genetics', 'nucleic acid amplification techniques', 'nucleic acid purification', 'nucleic acid quantitation /detection', 'optics', 'polymerase chain reaction', 'portable biomedical equipment', 'virus DNA']",NCRR,UNIVERSITY OF NEBRASKA LINCOLN,R21,2004,178840,22951068,0.016863889334236055
"Use of Microarray Test Data for Toxicogenomic Prediction    DESCRIPTION (provided by applicant):    This project bridges the understanding between physical and chemical principles and genomic/proteomic response by integrating three independent parallel toxicity prediction tools. Each uses computational neural networks (CNNs) and wavelets to rapidly and accurately make pharmaceutical/chemical toxicity predictions. A CNN-based Quantitative Structure-Activity Relationship (QSAR) module makes toxicological predictions based only on structure-activity analyses; a second CNN/wavelet module makes independent toxicogenomic predictions using microarray data; and a third CNN/wavelet module makes toxicogenomic predictions using Massively Parallel Signature Sequencing (MPSS) data. This multi-intelligent, three-module approach provides crosschecks to reduce false positives and false negatives while substantially increasing confidence in predictions relative to current computer-based toxicity prediction techniques. The resulting product could potentially become a primary tool used by (a) human health researchers, b) pharmaceutical companies for screening drugs early during development, c) companies designing/developing new chemicals and chemically treated materials, and (d) government organizations (e.g., military) for mission-related chemical deployments. Public benefits include reduced health and environmental risks (e.g., 4 out of 5 chemicals in use today have inadequate testing); reduced reliance on animal testing; and reduced time and cost required to bring new pharmaceuticals and chemicals into beneficial medical and commercial use.            n/a",Use of Microarray Test Data for Toxicogenomic Prediction,6743871,R41ES013321,"['computational neuroscience', 'computer data analysis', 'evaluation /testing', 'method development', 'microarray technology', 'polymerase chain reaction', 'toxicant screening', 'toxicology']",NIEHS,"YAHSGS, LLC",R41,2004,211770,0,0.00019545448387485573
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6701378,R01AR042739,"['artificial intelligence', 'bone density', 'bone fracture', 'clinical research', 'computer assisted diagnosis', 'densitometry', 'diagnosis design /evaluation', 'disease /disorder proneness /risk', 'hip', 'human subject', 'information systems', 'limbs', 'mathematical model', 'noninvasive diagnosis', 'osteoporosis', 'photon absorptiometry', 'radiography', 'spine', 'women&apos', 's health']",NIAMS,UNIVERSITY OF CHICAGO,R01,2004,297104,246330700,0.03762237619226509
"Development of Ultrasonic Appratus for Dental Diagnosis DESCRIPTION: An ultrasonic diagnostic apparatus has been proposed for Dental applications in determining tooth pathologies such as demineralization/caries, hidden fractures, and formation of abscesses. The equipment adopts a piezoelectric and laser optic hybrid transduction system for interrogation of teeth. Ultrasonic responses of the tooth structure will be analyzed by a pattern recognition expert system (artificial intelligence) to determine the diagnosis of the tooth inspected. The proposed research will eventually help to reduce the use of harmful X-ray radiation in Dental clinics and contribute to artificial intelligence based diagnosis. The proposed concept has been successfully demonstrated in the previous Phase I study. In this Phase II study, instrumentation for clinical data collection using a combination of conventional piezoelectric and new laser-based ultrasonic technologies will be developed and optimized; an artificial intelligence based diagnostic function will be developed using clinical data and implemented using embedded computing; numerical simulations will be used to enhance diagnostic function development; and finally, initial clinical trials will be conducted to demonstrate the performance of the prototype equipment. The ultrasonic apparatus for Dental diagnosis outlined in this application is a first application of AI-based NDE in Dentistry. The research concept may also extend to periodontal and craniofacial applications. n/a",Development of Ultrasonic Appratus for Dental Diagnosis,6777482,R44DE014270,"['artificial intelligence', 'biomedical equipment development', 'clinical research', 'clinical trials', 'dental disorder diagnosis', 'dental structure', 'dentistry', 'diagnosis design /evaluation', 'human subject', 'patient oriented research', 'tooth', 'tooth surface']",NIDCR,AAC INTERNATIONAL,R44,2004,367866,0,0.0016829682699406332
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6702676,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2003,205127,246330700,0.03762237619226509
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6628097,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2003,297104,246330700,0.03762237619226509
"Development of Ultrasonic Appratus for Dental Diagnosis DESCRIPTION: An ultrasonic diagnostic apparatus has been proposed for Dental applications in determining tooth pathologies such as demineralization/caries, hidden fractures, and formation of abscesses. The equipment adopts a piezoelectric and laser optic hybrid transduction system for interrogation of teeth. Ultrasonic responses of the tooth structure will be analyzed by a pattern recognition expert system (artificial intelligence) to determine the diagnosis of the tooth inspected. The proposed research will eventually help to reduce the use of harmful X-ray radiation in Dental clinics and contribute to artificial intelligence based diagnosis. The proposed concept has been successfully demonstrated in the previous Phase I study. In this Phase II study, instrumentation for clinical data collection using a combination of conventional piezoelectric and new laser-based ultrasonic technologies will be developed and optimized; an artificial intelligence based diagnostic function will be developed using clinical data and implemented using embedded computing; numerical simulations will be used to enhance diagnostic function development; and finally, initial clinical trials will be conducted to demonstrate the performance of the prototype equipment. The ultrasonic apparatus for Dental diagnosis outlined in this application is a first application of AI-based NDE in Dentistry. The research concept may also extend to periodontal and craniofacial applications. n/a",Development of Ultrasonic Appratus for Dental Diagnosis,6691772,R44DE014270,"['artificial intelligence', ' biomedical equipment development', ' clinical research', ' clinical trials', ' dental disorder diagnosis', ' dental structure', ' dentistry', ' diagnosis design /evaluation', ' human subject', ' patient oriented research', ' tooth', ' tooth surface']",NIDCR,AAC INTERNATIONAL,R44,2003,382129,0,0.0016829682699406332
"AUTOMATED MINIMAL RESIDUAL DISEASE QUANTIFICATION Quantification of Minimal Residual Disease (MRD) is a general concern in oncology since this parameter is likely to give valuable information to clinicians to customize chemotherapeutic treatments and to anticipate possible relapses. An automated system will be developed for the quantification of MRD by using real-time PCR to quantify cancer cells in a background of non pathologic DNA. The system will be derived from an existing high-throughput sample handling system (developed by Meldrum and team) named Acapella. Acapella has a pipelined serial architecture which makes it possible to develop an adaptive PCR control algorithm ensuring a level of sensitivity and accuracy for the quantification results specified by the clinician. In the R21 phase of the project, a critical component of the system is the real-time thermocycler. It will provide DNA quantification results of greater precision than what is currently possible with commercial instruments such as the ABI PRISM 7700 Sequence Detector by taking advantage of a high performance custom fluorescence analyzer and sophisticated data analysis methods. The fluorescence analyzer will have a signal to noise ratio and a dynamic range of at least one order of magnitude larger than the optics used on commercial systems. This unique feature will permit precise measurements of the amplification kinetics during at least five cycles in the exponential phase of the reaction. The amplification yield will he derived from these data using statistical estimators customized to meet the requirements of real-time PCR data analysis. The sample DNA content will be derived from the amplification yield and the calibrated fluorescence measurements of the reaction kinetics. This new approach will make it possible to run series of real-time PCRs with more flexibility than would be possible if the assay was based on standard reactions required to have the same amplification yield as the clinical samples. PCR conditions will be adapted online without concerns about possible differences of amplification rate. The assay control algorithm will adapt the reaction DNA content, the primer selection, and the number of PCRs to meet the clinician requirements for particular patient DNA. During the R21 phase of the project a prototype of the real-time thermocycler will be developed to demonstrate the optic performance and the possibility to estimate the PCR amplification rates with 5% accuracy. A conceptual design of the fully integrated process from patient blood sample to MRD quantification data will be completed to allow assessment of expected performance, cost, and risks associated with the development of the fully engineered system. The development of the various hardware and software components along with their integration into the automated system will take place during the R33 phase of the project. Performance of the system will he evaluated on real biological samples provided by the UW Department of Laboratory Medicine. Results returned by the system will he compared with results of t(14; 18) PCR performed in this department with an ABI PRISM 7700 for the diagnosis of patients suffering from follicular lymphomas.  n/a",AUTOMATED MINIMAL RESIDUAL DISEASE QUANTIFICATION,6633609,R33CA084691,"['artificial intelligence', ' biomedical automation', ' biomedical equipment development', ' chromosome translocation', ' computer assisted diagnosis', ' diagnosis design /evaluation', ' fluorescence', ' high throughput technology', ' human tissue', ' minimal residual disease', ' neoplasm /cancer diagnosis', "" nonHodgkin's lymphoma"", ' nucleic acid quantitation /detection', ' polymerase chain reaction']",NCI,UNIVERSITY OF WASHINGTON,R33,2003,618638,533302350,0.008626032574371433
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6558149,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2002,10000,246330700,0.03762237619226509
"Automated PCR Pathogen Detection and Quantification  DESCRIPTION (provided by applicant):  We will develop software for automated pathogen detection and quantification using data from PCR experiments. Automated pathogen detection using data from a PCR experiment requires software to determine whether DNA from the pathogen is present or absent in a sample. We will develop a pattern-matching algorithm to mathematically analyze PCR amplification data. We will optimize the algorithm against a data set of at least 5000 PCR reactions (including a significant set of data gathered during the anthrax attack) to determine its efficacy and limitations. We expect the pathogen detection algorithms to distinguish positives samples from negative samples in more than 98% of the samples, to find inconclusive results in less than 1% of the samples, and to incorrectly classify less than 1% of the samples. We will also develop software to perform automated melting curve analysis of samples that our detection algorithm has determined to be positive or inconclusive. The melting profile of the probes is a property of the assay, and it can be used for secondary confirmation of a pathogen by comparing the profile of the unknown samples to the profile of the assay's positive controls. We will develop algorithms to automatically determine whether the melting profile of the sample and controls match. With melting analysis confirmation, the failure rate of the final detection algorithm should be less than 0.5%.   Automated pathogen quantification requires software to determine the number of copies of a pathogen's DNA in a sample. We will develop discrete dynamical models of PCR for quantification. We will optimize these methods against a large data set of PCR reactions with dilution series. We will systematically determine the features of the models that provide information and the features that can be ignored. We will measure efficacy by comparing computed DNA copy numbers against the known concentrations (as specified by experimenters), and against each other. We will use the most effective model (or models) in the software we produce.   n/a",Automated PCR Pathogen Detection and Quantification,6555484,R43AI052944,"['artificial intelligence', ' bioterrorism /chemical warfare', ' communicable disease diagnosis', ' computer program /software', ' computer system design /evaluation', ' microorganism', ' nucleic acid denaturation', ' nucleic acid quantitation /detection', ' phase change', ' polymerase chain reaction']",NIAID,IDAHO TECHNOLOGY,R43,2002,100000,0,0.0015413783780569464
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6497411,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2002,297104,246330700,0.03762237619226509
"AUTOMATED MINIMAL RESIDUAL DISEASE QUANTIFICATION Quantification of Minimal Residual Disease (MRD) is a general concern in oncology since this parameter is likely to give valuable information to clinicians to customize chemotherapeutic treatments and to anticipate possible relapses. An automated system will be developed for the quantification of MRD by using real-time PCR to quantify cancer cells in a background of non pathologic DNA. The system will be derived from an existing high-throughput sample handling system (developed by Meldrum and team) named Acapella. Acapella has a pipelined serial architecture which makes it possible to develop an adaptive PCR control algorithm ensuring a level of sensitivity and accuracy for the quantification results specified by the clinician. In the R21 phase of the project, a critical component of the system is the real-time thermocycler. It will provide DNA quantification results of greater precision than what is currently possible with commercial instruments such as the ABI PRISM 7700 Sequence Detector by taking advantage of a high performance custom fluorescence analyzer and sophisticated data analysis methods. The fluorescence analyzer will have a signal to noise ratio and a dynamic range of at least one order of magnitude larger than the optics used on commercial systems. This unique feature will permit precise measurements of the amplification kinetics during at least five cycles in the exponential phase of the reaction. The amplification yield will he derived from these data using statistical estimators customized to meet the requirements of real-time PCR data analysis. The sample DNA content will be derived from the amplification yield and the calibrated fluorescence measurements of the reaction kinetics. This new approach will make it possible to run series of real-time PCRs with more flexibility than would be possible if the assay was based on standard reactions required to have the same amplification yield as the clinical samples. PCR conditions will be adapted online without concerns about possible differences of amplification rate. The assay control algorithm will adapt the reaction DNA content, the primer selection, and the number of PCRs to meet the clinician requirements for particular patient DNA. During the R21 phase of the project a prototype of the real-time thermocycler will be developed to demonstrate the optic performance and the possibility to estimate the PCR amplification rates with 5% accuracy. A conceptual design of the fully integrated process from patient blood sample to MRD quantification data will be completed to allow assessment of expected performance, cost, and risks associated with the development of the fully engineered system. The development of the various hardware and software components along with their integration into the automated system will take place during the R33 phase of the project. Performance of the system will he evaluated on real biological samples provided by the UW Department of Laboratory Medicine. Results returned by the system will he compared with results of t(14; 18) PCR performed in this department with an ABI PRISM 7700 for the diagnosis of patients suffering from follicular lymphomas.  n/a",AUTOMATED MINIMAL RESIDUAL DISEASE QUANTIFICATION,6514339,R33CA084691,"['artificial intelligence', ' biomedical automation', ' biomedical equipment development', ' chromosome translocation', ' computer assisted diagnosis', ' diagnosis design /evaluation', ' fluorescence', ' high throughput technology', ' human tissue', ' minimal residual disease', ' neoplasm /cancer diagnosis', "" nonHodgkin's lymphoma"", ' nucleic acid quantitation /detection', ' polymerase chain reaction']",NCI,UNIVERSITY OF WASHINGTON,R33,2002,582434,533302350,0.008626032574371433
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6487190,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2001,10000,246330700,0.03762237619226509
"Educational Tools for Neuroscience   DESCRIPTION (provided by applicant): SHAI proposes to bring two instructional        technologies together to compliment neuroscience lectures and distance               learning. Specifically we want to embed Computer Simulations of experiments and      the chemical, genetic, and physiological systems that underlie them within an        Intelligent Tutoring System. Simulations are excellent tools for revealing the       structure and dynamics of systems to students. They can also serve as a basis        of interactive experiments where students can ""discover"" the answers to              questions. Intelligent Tutoring Systems (ITS) are an emerging educational            technology based on artificial intelligence research. They play the role of          tutor, in that they guide students with appropriate information or                   demonstrations when they are having difficulty with a lesson. They also              adaptively plan the presentation of new lessons based on evaluations of a            student's past performance and knowledge level. The objective of this phase I        proposal is to develop a prototype of NeuroTutor, a simulation-based ITS to          provide students with individualized instruction in a simulation centered            environment. Steps to reaching this objective include designing a curriculum,        developing instructional, presentations and support, developing appropriate          methods for Student Modeling and Diagnosis, and implementing a limited               prototype.                                                                           PROPOSED COMMERCIAL APPLICATION:  This project has a sizeable commercialization potential.  Medical schools and university  neuroscience courses from a significant market.  Moreover the technologies to be   developed are transferable to other domains in the natural and social sciences, business  and medicine.  The technologies used are appropriate for use in distance learning programs,  and can be used by individuals to educate themselves.                                                                                     n/a",Educational Tools for Neuroscience,6403961,R43MH065842,"['computer assisted instruction', ' computer simulation', ' educational resource design /development', ' interactive multimedia', ' neurobiology', ' science education']",NIMH,"STOTTLER HENKE ASSOCIATES, INC.",R43,2001,100000,0,-0.0018255447040647407
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6333620,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2001,297104,246330700,0.03762237619226509
"AUTOMATED MINIMAL RESIDUAL DISEASE QUANTIFICATION Quantification of Minimal Residual Disease (MRD) is a general concern in oncology since this parameter is likely to give valuable information to clinicians to customize chemotherapeutic treatments and to anticipate possible relapses. An automated system will be developed for the quantification of MRD by using real-time PCR to quantify cancer cells in a background of non pathologic DNA. The system will be derived from an existing high-throughput sample handling system (developed by Meldrum and team) named Acapella. Acapella has a pipelined serial architecture which makes it possible to develop an adaptive PCR control algorithm ensuring a level of sensitivity and accuracy for the quantification results specified by the clinician. In the R21 phase of the project, a critical component of the system is the real-time thermocycler. It will provide DNA quantification results of greater precision than what is currently possible with commercial instruments such as the ABI PRISM 7700 Sequence Detector by taking advantage of a high performance custom fluorescence analyzer and sophisticated data analysis methods. The fluorescence analyzer will have a signal to noise ratio and a dynamic range of at least one order of magnitude larger than the optics used on commercial systems. This unique feature will permit precise measurements of the amplification kinetics during at least five cycles in the exponential phase of the reaction. The amplification yield will he derived from these data using statistical estimators customized to meet the requirements of real-time PCR data analysis. The sample DNA content will be derived from the amplification yield and the calibrated fluorescence measurements of the reaction kinetics. This new approach will make it possible to run series of real-time PCRs with more flexibility than would be possible if the assay was based on standard reactions required to have the same amplification yield as the clinical samples. PCR conditions will be adapted online without concerns about possible differences of amplification rate. The assay control algorithm will adapt the reaction DNA content, the primer selection, and the number of PCRs to meet the clinician requirements for particular patient DNA. During the R21 phase of the project a prototype of the real-time thermocycler will be developed to demonstrate the optic performance and the possibility to estimate the PCR amplification rates with 5% accuracy. A conceptual design of the fully integrated process from patient blood sample to MRD quantification data will be completed to allow assessment of expected performance, cost, and risks associated with the development of the fully engineered system. The development of the various hardware and software components along with their integration into the automated system will take place during the R33 phase of the project. Performance of the system will he evaluated on real biological samples provided by the UW Department of Laboratory Medicine. Results returned by the system will he compared with results of t(14; 18) PCR performed in this department with an ABI PRISM 7700 for the diagnosis of patients suffering from follicular lymphomas.  n/a",AUTOMATED MINIMAL RESIDUAL DISEASE QUANTIFICATION,6467733,R33CA084691,"['artificial intelligence', ' biomedical automation', ' biomedical equipment development', ' chromosome translocation', ' computer assisted diagnosis', ' diagnosis design /evaluation', ' fluorescence', ' high throughput technology', ' human tissue', ' minimal residual disease', ' neoplasm /cancer diagnosis', "" nonHodgkin's lymphoma"", ' nucleic acid quantitation /detection', ' polymerase chain reaction']",NCI,UNIVERSITY OF WASHINGTON,R33,2001,607570,533302350,0.008626032574371433
"AUTOMATED MINIMAL RESIDUAL DISEASE QUANTIFICATION Quantification of Minimal Residual Disease (MRD) is a general concern in oncology since this parameter is likely to give valuable information to clinicians to customize chemotherapeutic treatments and to anticipate possible relapses. An automated system will be developed for the quantification of MRD by using real-time PCR to quantify cancer cells in a background of non pathologic DNA. The system will be derived from an existing high-throughput sample handling system (developed by Meldrum and team) named Acapella. Acapella has a pipelined serial architecture which makes it possible to develop an adaptive PCR control algorithm ensuring a level of sensitivity and accuracy for the quantification results specified by the clinician. In the R21 phase of the project, a critical component of the system is the real-time thermocycler. It will provide DNA quantification results of greater precision than what is currently possible with commercial instruments such as the ABI PRISM 7700 Sequence Detector by taking advantage of a high performance custom fluorescence analyzer and sophisticated data analysis methods. The fluorescence analyzer will have a signal to noise ratio and a dynamic range of at least one order of magnitude larger than the optics used on commercial systems. This unique feature will permit precise measurements of the amplification kinetics during at least five cycles in the exponential phase of the reaction. The amplification yield will he derived from these data using statistical estimators customized to meet the requirements of real-time PCR data analysis. The sample DNA content will be derived from the amplification yield and the calibrated fluorescence measurements of the reaction kinetics. This new approach will make it possible to run series of real-time PCRs with more flexibility than would be possible if the assay was based on standard reactions required to have the same amplification yield as the clinical samples. PCR conditions will be adapted online without concerns about possible differences of amplification rate. The assay control algorithm will adapt the reaction DNA content, the primer selection, and the number of PCRs to meet the clinician requirements for particular patient DNA. During the R21 phase of the project a prototype of the real-time thermocycler will be developed to demonstrate the optic performance and the possibility to estimate the PCR amplification rates with 5% accuracy. A conceptual design of the fully integrated process from patient blood sample to MRD quantification data will be completed to allow assessment of expected performance, cost, and risks associated with the development of the fully engineered system. The development of the various hardware and software components along with their integration into the automated system will take place during the R33 phase of the project. Performance of the system will he evaluated on real biological samples provided by the UW Department of Laboratory Medicine. Results returned by the system will he compared with results of t(14; 18) PCR performed in this department with an ABI PRISM 7700 for the diagnosis of patients suffering from follicular lymphomas.  n/a",AUTOMATED MINIMAL RESIDUAL DISEASE QUANTIFICATION,6062376,R21CA084691,"['artificial intelligence', ' biomedical automation', ' biomedical equipment development', ' chromosome translocation', ' computer assisted diagnosis', ' diagnosis design /evaluation', ' fluorescence', ' high throughput technology', ' human tissue', ' minimal residual disease', ' neoplasm /cancer diagnosis', "" nonHodgkin's lymphoma"", ' nucleic acid quantitation /detection', ' polymerase chain reaction']",NCI,UNIVERSITY OF WASHINGTON,R21,2000,154460,533302350,0.008626032574371433
"CLASSIFICATION METHODS FOR DETECTING DISEASE LOCI DESCRIPTION (Adapted from the Investigator's Abstract): Bold steps must be       taken to advance our understanding of the genetic and associated co-             variates affecting the inheritance of complex diseases. To that end, this        proposal will develop improved quantitative methods to detect genetic            factors contributing to increased susceptibility to complex disorders and        implement these methods in software for distribution to the research             community.                                                                                                                                                        The methods will concentrate on the use of classification techniques             applied to allele sharing data and other risk factors which affect the           trait. Allele sharing methods for mapping genes will be extended to              include the classification methods known as latent class models, cluster         analysis, and artificial neural networks, as well as a novel use of              logistic regression Co-variates such as gender, parental diagnosis, or           other concomitant factors will be systematically studied through                 applications to both stimulated and existing data sets. An additional goal       is to determine the optimal distribution of relative pairs (e.g. siblings,       first cousins) for these methods. Of great importance to this proposal is        the development of well-documented, user-friendly software and                   documentation which will be distributed to the scientific community via          the Internet. Existing software developed by the PI will be extensively          expanded for latent class models. Existing cluster analysis software will        be modified and combined for ease of use.                                                                                                                         This proposal consists of theoretical exploration, computer simulation,          data analysis, and software development. First, solutions of theoretical         questions relating to classification techniques will be pursued; second,         adaptation of computer programs to implement the analytic methods, and           investigation into alternative research strategies will be accomplished.         The new strategies will be applied to stimulated data, and finally, to           existing data sets of pedigrees in which a complex trait has been                diagnosed. Findings from this research may contribute to the ability to          locate susceptibility loci in complex traits and to the clarification of         those etiological mechanisms responsible for susceptibility.                      n/a",CLASSIFICATION METHODS FOR DETECTING DISEASE LOCI,6168495,R01AA012239,"['alleles', ' analytical method', ' artificial intelligence', ' biomedical resource', ' computer program /software', ' computer simulation', ' data collection methodology /evaluation', ' disease /disorder classification', ' disease /disorder etiology', ' family genetics', ' gene environment interaction', ' gene expression', ' genetic disorder', ' genetic disorder diagnosis', ' genetic mapping', ' genetic markers', ' genetic susceptibility', ' human data', ' mathematical model', ' model design /development', ' quantitative trait loci', ' statistics /biometry']",NIAAA,WASHINGTON UNIVERSITY,R01,2000,180260,533594881,-0.015540806788500049
"NEW ROC METHODOLOGY TO ASSESS DIAGNOSTIC ACCURACY DESCRIPTION (Adapted from Applicant's Abstract):  Receiver Operating             Characteristic (ROC) analysis is recognized widely as the best way of            measuring and specifying the accuracies of diagnostic procedures, because it     is able to distinguish between actual differences in discrimination              capacity, on one hand, and apparent differences that are due only to             decision-threshold effects, on the other.  Key methodological needs remain       to be satisfied before ROC analysis can address all of the practically           important situations that arise in diagnostic applications, however.  This       project employs signal detection theory and computer simulation to address       several of those needs, by:  (1) refining and continuing distribution of         software developed previously by the applicants for fitting ROC curves and       for testing the statistical significance of differences between ROC curve        estimates; (2) developing and evaluating new algorithms for ROC                  curve-Fitting and statistical testing, based on their recently-developed         ""proper"" binormal model, that should provide more meaningful results in          experimental situations that involve small samples of cases; (3)                 investigating the usefulness of a form of ROC methodology that is based on       mixture distributions in order to rduce the need for diagnostic truth in ROC     experiments; (4) investigating the effect of case-saple difficulty on the        statistical power tests for differences between ROC curves, in order to          determine the optimal difficulty of cases that shouldbe studied on rank          diagnostic systems; and (5) developing methods for training artificial           neural networks (ANNs) to maximize diagnostic accuracy in terms of ROC           analysis and signal detection theory.                                             n/a",NEW ROC METHODOLOGY TO ASSESS DIAGNOSTIC ACCURACY,6181168,R01GM057622,"['artificial intelligence', ' computer assisted diagnosis', ' computer system design /evaluation', ' method development', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R01,2000,218176,246330700,-0.005714504475809179
